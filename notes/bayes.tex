\section{Beliefs and probabilities}
\only<presentation>{
  \begin{frame}
    \tableofcontents[ 
    currentsection, 
    hideothersubsections, 
    sectionstyle=show/shaded
    ] 
  \end{frame}
}


\only<article>{Probability can be used to describe purely chance events, as in for example quantum physics. However, it is mostly used to describe uncertain events, such as the outcome of a dice roll or a coin flip, which only appear random. In fact, one can take it even further than that, and use it to model subjective uncertainty about any arbitrary event. Although probabilities are not the only way in which we can quantify uncertainty, it is a simple enough model, and with a rich enough history in mathematics, statistics, computer science and engineering that it is the most useful.}
\begin{frame}
  \frametitle{Uncertainty}
  \only<presentation>{
    \begin{itemize}
    \item We cannot perfectly predict the future.
    \item We cannot know for sure what happened in the past.
    \item How can we quantify this uncertainty?
    \item Probabilities!
    \end{itemize}
  }
  \only<article>{
  In this book, we use probability to quantify uncertainty. Consider a collection of possible events or statements $\Sigma$, which includes a special event $\Omega$, which is always true, so that for any event $A$, we have $A \subset \Omega$. For every events $A$, $B$ we assign some probability $P(A)$ so that, $P(A) > P(B)$ only if we think that $A$ is more likely to happen than $B$.

  Now, because we want to be able to reason about every possible combination of events in $\Sigma$, we should be able to combine them in logical statements. If $A, B$ are sets, it is natural to think of ``A or B'' as $A \cup B$ and ``A and B'' as $A \cap B$. It is also natural to think of ``not A'' as $\Omega \setminus A$. If we consider all combinations of events possible, we arrive at what is called a $\sigma$-algebra:
}
  \only<article>{
    \begin{definition}[$\sigma$-algebra]
      \index{$\sigma$-algebra}
      A $\sigma$-algebra $\Sigma$ on a set $\Omega$ has the following properties:
      \begin{enumerate}[(a)]
      \item $\Omega \in \Sigma$.
      \item If $A \in \Sigma$ then $A \subset \Omega$.
      \item If $A, B \in \Sigma$ then $A \cup B \in \Sigma$.
      \item If $A \in \Sigma$ then $\Omega \setminus A \in \Sigma$.
      \end{enumerate}
    \end{definition}
    For a more complete introduction to basic probability concepts, take a look at \citet{intro-prob:ber-tsi},\citet[Appendix B]{dimitrakakis-ortner:dmuurl-book}, or \citet{degroot:optimalstatisticaldecisions}
  }
  \only<article>{
    While we said that we assign higher probability to events we think are more likely, we would like our probability to obey certain rules. 
  }
  \begin{block}{Axioms of probability}
    \only<article>{Let $\Omega$ be the certain event, and $\Sigma$ is an appropriate $\sigma$-algebra on $\Omega$.}
    A probability measure $P$ on $(\Omega, \Sigma)$ has the following properties:
    \begin{enumerate}
    \item<2-> The probability of the certain event is $P(\Omega) = 1$
    \item<3->The probability of the impossible event is
      $P(\emptyset) = 0$
    \item<4->The probability of any event $A \in \Sigma$ is $0 \leq P(A) \leq 1$.
    \item<5-> If $A, B$ are disjoint, i.e. $A \cap B = \emptyset$, meaning
      that they cannot happen at the same time, then
      \[
      P(A \cup B) = P(A) + P(B)
      \]
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}
  \only<article>{ Sometimes we would like to calculate the probability
    of some event $A$ happening given that we know that some other
    event $B$ has happened. For this we need to first define the idea
    of conditional probability.  }
  \begin{definition}[Conditional probability]
    The probability of $A$ happening if we know that $B$ has happened
    is defined to be:
    \[
    P(A \mid B) \defn \frac{P(A \cap B) }{P(B)}.
    \]
  \end{definition}
  \only<1>{
    Conditional probabilities obey the same rules as probabilities. }
  \only<article>{
    Here, the probability measure of any event $A$ given $B$ is defined to be the probability of the intersection of of the events divided by the second event.
    We can rewrite this definition as follows, by using the definition for $P(B \mid A)$}
  \begin{block}{Bayes's theorem}
    For $P(A_1 \cup A_2)  = 1$, $A_1 \cap A_2 = \emptyset$,
    \[
    P(A_i \mid B)
    \uncover<2->{= \frac{P(B \mid A_i) P(A_i)}{P(B)}}
    \uncover<3->{= \frac{P(B \mid A_i) P(A_i)}{P(B \mid A_1) P(A_1) + P(B \mid A_2) P(A_2)}}
    \]
  \end{block}
  \uncover<4->{
    \begin{example}[probability of rain]
      What is the probability of rain given a forecast $x_1$ or $x_2$?
      \begin{columns}
        \begin{column}{0.33\textwidth}
          \begin{table}[H]
            \centering
            \begin{tabular}{c|c}
              $\outcome_1$: rain & $P(\outcome_1) = 80\%$ \\
              $\outcome_2$: dry & $P(\outcome_2) = 20\%$
            \end{tabular}
            \caption{Prior probability of rain tomorrow}
          \end{table}
        \end{column}
        
        \begin{column}{0.33\textwidth}
          \uncover<5->{
            \begin{table}[H]
              \centering
              \begin{tabular}{c|c}
                $x_1$: rain & $P(x_1 \mid \outcome_1) = 90\%$ \\
                $x_2$: dry & $P(x_2 \mid \outcome_2) = 50\%$
              \end{tabular}
              \caption{Probability the forecast is correct}
            \end{table}
          }
        \end{column}
        \uncover<6->{
          \begin{column}{0.33\textwidth}
            \begin{table}[H]
              \centering
              \begin{tabular}{c}
                $P(\outcome_1 \mid x_1) = 87.8\%$ \\
                $P(\outcome_1 \mid x_2) = 44.4\%$
              \end{tabular}
              \caption{Probability that it will rain given the forecast}
            \end{table}
          }
        \end{column}
      \end{columns}
    \end{example}
  }
\end{frame}


\begin{frame}
  \frametitle{Classification in terms of conditional probabilities}
  \only<1>{
    \only<presentation>{
      \begin{itemize}
      \item Features $x_t \in \CX$.
      \item Class label $y_t \in \CY$.
      \item Probability model $P_\model(x_t \mid y_t)$.
      \item Prior class probability $P_\model(y_t = c)$.
      \end{itemize}
    }
  }
  \only<article>{
    Conditional probability naturally appears in classification problems. Given a new example vector of data $x_t \in \CX$, we would like to calculate the probability of different classes $c \in \CY$ given the data, $P_\model(y_t = c \mid x_t)$.  
    If we somehow obtained the distribution of data $P_\model(x_t \mid y_t)$ for each possible class, as well as the prior class probability $P_\model(y_t = c)$, 
    from Bayes's theorem, we see that we can obtain the probability of the class:
  }
  \only<1>{
    \[
    P_\model(y_t = c \mid x_t) = \frac{P_\model(x_t \mid y_t = c) P_\model(y_t = c)}{\sum_{c' \in \CY} P_\model(x_t \mid y_t = c') P_\model(y_t = c')}
    \]
  }
  \only<article>{
    for any class $c$. This directly gives us a method for classifying new data, as long as we have a way to obtain $P_\model(x_t \mid y_t)$ and $P_\model(y_t)$.
  }
  \only<1>{
    \begin{figure}[H]
      \centering
      \begin{tikzpicture}
        \node[RV] at (0,0) (x) {$y_t$};
        \node[RV] at (0,2) (y) {$x_t$};
        \node[RV] at (1,1) (m) {$\model$};
        \draw[->] (x) to (y);
        \draw[->] (m) to (x); \draw[->] (m) to (y);
      \end{tikzpicture}
      \caption{A generative classification model. $\model$ identifies the model (paramter). $x_t$ are the features and $y_t$ the class label of the $t$-th example.}
    \end{figure}
  }
  \begin{figure}[H]
    \centering
    \begin{subfigure}{\fwidth}
      \includegraphics[width=\textwidth]{../figures/equal-variance}
      \caption{Equal prior and variance}
    \end{subfigure}
    \begin{subfigure}{\fwidth}
      \includegraphics[width=\textwidth]{../figures/unequal-variance}
      \caption{Unequal variance}
    \end{subfigure}
    \begin{subfigure}{\fwidth}
      \includegraphics[width=\textwidth]{../figures/unequal-prior}
      \caption{Unequal prior}
    \end{subfigure}
    \caption{The effect of changing variance and prior on the classification decision when we assume a normal distribution.}
    \label{fig:normal-generative}
  \end{figure}
  \begin{example}[Normal distribution]
    A simple example is when $x_t$ is normally distributed in a matter that depends on the class.  Figure~\ref{fig:normal-generative} shows the distribution of $x_t$ for two different classes, with means of $-1$ and $+1$ respectively, for three different case. In the first case, both classes have variance of 1, and we assume the same prior probability for both
    \[
    x_t \mid y_t = 0 \sim \Normal(-1,1),
    \qquad
    x_t \mid y_t = 1 \sim \Normal(1,1)
    \]
    \[
    x_t \mid y_t = 0 \sim \Normal(-1,1),
    \qquad
    x_t \mid y_t = 1 \sim \Normal(1,1)
    \]

  \end{example}
  
  \uncover<5>{
    \alert{But how can we get a probability model in the first place?}
  }
\end{frame}


\begin{frame}
  \frametitle{Subjective probability}
  \only<article>{While probabilities apply to truly random events, they are also useful for representing subjective uncertainty. In this course, we will use a special symbol for subjective probability, $\bel$.}
  \begin{block}{Subjective probability measure $\bel$}
    \begin{itemize}
    \item If we think event $A$ is more likely than $B$, then $\bel(A) > \bel(B)$.
    \item Usual rules of probability apply:
      \begin{enumerate}
      \item $\bel(A) \in [0,1]$.
      \item $\bel(\emptyset) = 0$.
      \item If $A \cap B = \emptyset$, then $\bel(A \cup B) = \bel(A) + \bel(B)$.
      \end{enumerate}
    \end{itemize}
  \end{block}
\end{frame}


\begin{frame}
  \frametitle{Bayesian inference illustration}
\only<article>{
Here is a simple example to illustrate the idea of Bayesian inference. Imagine you are a rat in the maze and you need to get to the cheese. In the example below, you believe that the maze lay out is only one of two possibilities $\model_1, \model_2$. Initially you might believe that the first model is more likely than the second, so you decide upon some function $\bel$ so that $\bel(\model_1) > \bel(\model_2)$, for example $\bel(\model_1) = 2/3$, $\bel(\model_2) = 1/3$. Now you start acting in the maze, collecting data $h$ consisting of your history of observation in this maze. The data gives a different amount of \emph{evidence} for each model of the world, which we can write as $P_\model(h)$. In the end, we want to combine this evidence with our initial belief to arrive at a conclusion. In the Bayesian setting, all of these quantities are probabilities, and the conclusion is simply the conditional probability of the model given the data.}
  \begin{columns}
    \begin{column}{0.7\textwidth}
      \begin{block}{Use a subjective belief $\bel(\model)$ on $\Model$}
        \begin{itemize}
        \item<1-> \alert{Prior} belief $\bel(\model)$ represents our initial uncertainty.
        \item<2-> We \alert{observe history} $h$.
        \item<3->Each possible $\model$ assigns a \alert{probability} $P_\model(h)$ to $h$.
        \item<4-> We can use this to \alert{update} our belief via Bayes' theorem to obtain the \alert{posterior} belief:
          \[
          \bel(\model \mid h) \propto P_\model(h) \bel(\model)
          \tag{conclusion = evidence $\times$ prior}
          \]
        \end{itemize}
      \end{block}
    \end{column}
    \begin{column}{0.3\textwidth}
      \centering
      \uncover<1->{\includegraphics[width=0.5\fwidth]{../figures/rl_worlds}
        \\
        prior
      }
      \\
      \uncover<2->{\includegraphics[width=0.5\fwidth]{../figures/rl_observations}
        \\
        evidence
      }
      \\
      \uncover<4->{\includegraphics[width=0.5\fwidth]{../figures/rl_worlds2}
        \\ 
        conclusion
      }
    \end{column}
  \end{columns}
\end{frame}




\subsection{Probability and Bayesian inference}
\only<article>{One of the most important methods in machine learning
  and statistics is that of Bayesian inference.  This is the most
  fundamental method of drawing conclusions from data and explicit
  prior assumptions. In Bayesian inference, prior assumptions are
  represented as a probabilities on a space of hypotheses. Each
  hypothesis is seen as a probabilistic model of all possible data
  that we can see.}

\only<article>{Frequently, we want to draw conclusions from data. However, the conclusions are never solely inferred from data, but also depend on prior assumptions about reality.}




\begin{frame}
  \frametitle{Some examples}

  \begin{example}
    John claims to be a medium. He throws a coin $n$ times and predicts its value always correctly. Should we believe that he is a medium?
    \begin{itemize}
    \item $\model_1$: John is a medium.
    \item $\model_0$: John is not a medium.
    \end{itemize}
  \end{example}
  The answer depends on what we \alert{expect} a medium to be able to do, and how likely we thought he'd be a medium in the first place.

  \only<article>{
    \begin{example}
      Traces of DNA are found at a murder scene. We perform a DNA test against a database of $10^4$ citizens registered to be living in the area. We know that the probability of a false positive (that is, the test finding a match by mistake) is $10^{-6}$. If there is a match in the database, does that mean that the citizen was at the scene of the crime?
    \end{example}
  }
\end{frame}





\begin{frame}
  \frametitle{Bayesian inference}
  \only<article>{
    Now let us apply this idea to our specific problem. We already have the probability of the observation for each model, but we just need to define a \emph{prior probability} for each model. Since this is usually completely subjective, we give it another symbol.
  }
  \only<article>{
    \begin{block}{Prior probability}
      The prior probability $\bel$ on a set of models $\Model$ specifies our subjective belief $\bel(\model)$ that each model is true.\footnote{More generally $\bel$ is a probability measure.}
    \end{block}
  }
  \only<article>{
    This allows us to calculate the probability of John being a medium, given the data:
    \[
    \bel(\model_1 \mid \bx) = \frac{\Pr(\bx \mid \model_1) \bel(\model_1)}{\Pr_\bel(\bx)},
    \]
    where
    \[
    \Pr_\bel(\bx) \defn \Pr(\bx \mid \model_1) \bel(\model_1) + \Pr(\bx \mid \model_0) \bel(\model_0).
    \]
    The only thing left to specify is $\bel(\model_1)$, the probability that John is a medium before seeing the data. This is our subjective prior belief that mediums exist and that John is one of them.
    More generally, we can think of Bayesian inference as follows: }
\only<article>{
  \begin{itemize}
  \item<1-> We start with a set of mutually exclusive models $\Model = \{\model_1, \ldots, \model_k\}$.
  \item<2->Each model $\model$ is represented by a specific probabilistic model for any possible data $x$, that is
    $P_\model(x) \equiv \Pr(x \mid \model)$.
  \item<3-> For each model, we have a prior probability $\bel(\model)$ that it is correct.
  \item<4-> After observing the data, we can calculate a posterior probability that the model is correct:
    \[
    \bel(\model \mid x) = \frac{\Pr(x \mid \model) \bel(\model)}{\sum_{\model' \in \Model} \Pr(x \mid \model') \bel(\model')}
    = \frac{P_\model(x) \bel(\model)}{\sum_{\model' \in \Model} P_{\model'} (x) \bel(\model')}.
    \]
  \end{itemize}
}
\only<presentation>{
  \begin{block}{Family of models $\Model = \{\model_1, \ldots, \model_k\}$}
    Defines a family of probabilities for \alert{any} data $x$:
    \[
     \{P_\model | \model \in \Model\}, \qquad P_\model(x) \equiv \Pr(x \mid \model).
    \]
  \end{block}
  \only<2->{
    \begin{block}{Prior belief $\bel$ over models $\Model$}
      $\bel(\model)$ is a \alert{distribution} how much we believe it is correct before seeing any data.
    \end{block}
  }
  \only<3->{
    \begin{block}{Posterior belief $\bel(\model \mid x)$ over models after seeing $x$}
      \[
      \bel(\model \mid x)
= \frac{P_\model(x) \bel(\model)}{\sum_{\model' \in \Model} P_{\model'} (x) \bel(\model')}
\only<4->{
  = \frac{\Pr(x \mid \model) \bel(\model)}{\sum_{\model' \in \Model} \Pr(x \mid \model') \bel(\model')}.
}
       \]
    \end{block}
  }
}
  \only<5->{
    \begin{exampleblock}{Interpretation}
      \begin{itemize}
      \item $\CM$: Set of all possible models that could describe the data.
      \item $P_\model(x)$: Probability of $x$ under model $\model$.
      \item Alternative notation $\Pr(x \mid \model)$: Probability of $x$ given that model $\model$ is correct.
      \item $\bel(\model)$: Our belief, before seeing the data, that $\model$ is correct.
      \item $\bel(\model \mid x)$: Our belief, aftering seeing the data, that $\model$ is correct.
      \end{itemize}
    \end{exampleblock}
    \only<article>{It must be emphasized that $P_\model(x) = \Pr(x \mid \model)$ as they are simply two different notations for the same thing. In words the first can be seen as the probability that model $\model$ assigns to data $x$, while the second as the probability of $x$ if $\model$ is the true model.}
  }
  \only<article>{
    Combining the prior belief with evidence is key in this procedure. Our posterior belief can then be used as a new prior belief when we get more evidence.}
\end{frame}
\begin{frame}
  \begin{exercise}[Continued example for medium]
    \only<article>{ Now let us apply this idea to our specific
      problem. We first make an independence assumption. In particular, we can assume that success and failure comes from a Bernoulli distribution with a parameter depending on the model.}
    \begin{align}
      P_{\model} (x) &= \prod_{t=1}^n P_{\model} (x_t).
                       \tag{independence property}
    \end{align}
    \only<article>{We first need to specify how well a medium could predict. Let's assume that a true medium would be able to predict perfectly, and that a non-medium would only predict randomly. This leads to the following models:}
    \uncover<2->{
    \begin{align}
      P_{\model_1}(x_t = 1) &= 1, &P_{\model_1}(x_t = 0) &= 0.
                                                           \tag{true medium model}
      \\
      P_{\model_0}(x_t = 1) &= 1/2, &P_{\model_0}(x_t = 0) &= 1/2.
                                                             \tag{non-medium model}
    \end{align}
  }
    \only<article>{
      The only thing left to specify is $\bel(\model_1)$, the probability
      that John is a medium before seeing the data. This is our
      subjective prior belief that mediums exist and that John is one of
      them.}
    \uncover<3->{
      \begin{align}
        \bel(\model_0) &= 1/2,   &  \bel(\model_1) &= 1/2.
                                                     \tag{prior belief}
      \end{align}
    }
    \only<article>{Combining the prior belief with evidence is key in this
      procedure. Our posterior belief can then be used as a new prior
      belief when we get more evidence.  }
    \uncover<4>{
      \begin{align}
        \bel(\model_1 \mid x) & = \frac{P_{\model_1}(x)
                                \bel(\model_1)}{\Pr_\bel(x)} \tag{posterior belief}
        \\
        \Pr_\bel(x) &\defn P_{\model_1}(x) \bel(\model_1) + P_{\model_0}(x) \bel(\model_0).
                      \tag{marginal distribution}
      \end{align}
    }
    If a classmate correctly predicts 4 coin tosses, what is your belief they are a medium? 
  \end{exercise}
\end{frame}


\begin{frame}
  \frametitle{Sequential update of beliefs}
  \only<article>{Assume you have $n$ meteorologists. At each day $t$, each meteorologist $i$ gives a probability $p_{t,\model_i}\defn P_{\model_i}(x_t = \textrm{rain})$ for rain. Consider the case of there being three meteorologists, and each one making the following prediction for the coming week. Start with a uniform prior $\bel(\model) = 1/3$ for each model.}
  {
    \begin{table}[h]
      \begin{tabular}{c|l|l|l|l|l|l|l}
        &M&T&W&T&F&S&S\\
        \hline
        CNN & 0.5 & 0.6 & 0.7 & 0.9 & 0.5 & 0.3 & 0.1\\
        SMHI & 0.3 & 0.7 & 0.8 & 0.9 & 0.5 & 0.2 & 0.1\\
        YR & 0.6 & 0.9 & 0.8 & 0.5 & 0.4 & 0.1 & 0.1\\
        \hline
        Rain? & Y & Y & Y & N & Y & N & N
      \end{tabular}
      \caption{Predictions by three different entities for the probability of rain on a particular day, along with whether or not it actually rained.}
      \label{tab:meteorologists}
    \end{table}
  }
  \begin{exercise}
    \begin{itemize}
    \item $n$ meteorological stations $\cset{\mdp_i}{i=1, \ldots,n}$
    \item The $i$-th station predicts rain $P_{\mdp_i}(x_t \mid x_1, \ldots, x_{t-1})$.
    \item Let $\bel_t(\mdp)$ be our belief at time $t$.
      Derive the next-step belief
      $\bel_{t+1}(\mdp) \defn  \bel_t(\mdp | x_{t})$ in terms of the current belief $\bel_t$.
    \item Write a python function that computes this posterior
    \end{itemize}
  \end{exercise}
  \uncover<2->{
    \[
    \bel_{t+1}(\mdp)
    \defn
    \bel_t(\mdp | x_{t})
    =
    \frac{P_\mdp(x_t \mid x_1, \ldots, x_{t-1}) \bel_t(\mdp)}
    {\sum_{\mdp'} P_{\mdp'}(x_t \mid x_1, \ldots, x_{t-1}) \bel_t(\mdp')}
    \]
  }
\end{frame}



\begin{frame}[label=beta-example]
  \frametitle{Bayesian inference for Bernoulli distributions}
  \only<1>{
    \begin{block}{Estimating a coin's bias}
      A fair coin comes heads $50\%$ of the time. 
      We want to test an unknown coin, which we think may not be completely fair. 
    \end{block}
  }
  \only<1,2>{
    \begin{figure}[h]
      \centering
      \includegraphics[width=\textwidth]{../figures/beta-prior}
      \caption{Prior belief $\bel$ about the coin bias $\theta$.}
    \end{figure}
  }
  \only<2>{
    For a sequence of throws $x_t \in \{0,1\}$,
    \[
    P_\theta(x) \propto \prod_t \theta^{x_t} (1 - \theta)^{1 - x_t}
    = \theta^{\textrm{\#Heads}} (1 - \theta)^{\textrm{\#Tails}}
    \]
  }
  \only<3>{
    \begin{figure}[h]
      \centering
      \includegraphics[width=\textwidth]{../figures/beta-likelihood}
      \caption{Prior belief $\bel$ about the coin bias $\theta$ and likelihood of $\theta$ for the data.}
    \end{figure}
    Say we throw the coin 100 times and obtain 70 heads. Then we plot the \alert{likelihood} $P_\theta(x)$ of different models.
  }
  \only<4>{
    \begin{figure}[h]
      \centering
      \includegraphics[width=\textwidth]{../figures/beta-posterior}
      \caption{Prior belief $\bel(\theta)$ about the coin bias $\theta$, likelihood of $\theta$ for the data, and posterior belief $\bel(\theta \mid x)$}
    \end{figure}
    From these, we calculate a \alert{posterior} distribution over the correct models. This represents our conclusion given our prior and the data.
  }
  \only<article>{If the prior distribution is described by the so-called Beta density
    \[
    f(\theta \mid \alpha, \beta) \propto \theta^{\alpha -1} (1 - \theta)^{\beta -1}
    \]
    where $\alpha, \beta$ describe the shape of the Beta distribution.
  }
\end{frame}

\subsubsection{Riemann and Lebesgue integrals}
\only<article>{
  Since $\bel$ is a probability measure over models $\Param$, it is always simple to write the posterior probability given some data $x$ in the following terms when $\Param$ is discrete (finite or countable):
  \[
  \bel(\param \mid x) = \frac{P_\param(x) \bel(\param)}{\sum_{\param'} P_{\param'(x) \bel(\param')}.}
  \]
  However, in many situations $\Param$ is uncountable, i.e. $\Param \subset \Reals^k$. Then, as both the prior $\bel$ and the posterior $\bel(\cdot \mid x)$ have to be probability measures on $\Param$, they are defined over subsets of $\Param$. This means that we can write the posterior in terms of Lebesgue integrals:
  \[
  \bel(B \mid x) = \frac{\int_B P_\param(x) \dd \bel(\param)}{\int_\Param P_\param(x) \dd \bel(\param)}.
  \]
  Alternatively, we can abuse notation and use $\bel(\param)$ to describe a density, so that the \emindex{posterior density} is written in terms of a Riemann integral:
  \[
  \bel(\param \mid x) \defn \frac{P_\param(x) \bel(\param) \dd \param}{\int_{\Param} P_{\param'}(x) \bel(\param') \param'}.
  \]
  However, the advantage of the Lebesgue integral notation is that it works the same no matter whether $\Param$ is discrete or continuous.
}

\subsection{Distributions and information theory}
It is very useful to define a divergence between probability measures. For simplicity, we give those first in discrete outcomes, before the general case.
\begin{equation}
  D(P\|Q) \defn \sum_{\omega \in \Omega} P(\omega) \ln \frac{P(\omega)}{Q(\omega)}
  \label{eq:kl-divergence}
\end{equation}
For the general case, where $P, Q$ are probability measures on $(\Omega, \Sigma)$, we can write this as $D(P\|Q) \defn \int_{\Omega} dP(\omega) \ln \frac{dP(\omega)}{dQ(\omega)}$.
Another common divergence is the total variation,
\begin{equation}
  \|P - Q\|_{TV} = \frac{1}{2}\|P - Q\|_1 \frac{1}{2} \sum_{\omega \in \Omega} |P(\omega) - Q(\omega)|.
\end{equation}
For the general case, this is defined as
\[
  \|P - Q\|_{TV} = \sup_{A \in \Sigma} |P(A) - Q(A)|.
\]
Those divergences are related by \emindex{Pinsker's inequality}
\begin{equation}
  \label{eq:pinsker}
  \|P - Q\|_{TV} \leq \sqrt{\frac{1}{2} D(P\|Q)}.
\end{equation}

Both of these are special cases of $f$-divergences, defined as:
\begin{equation}
  D_f(P\|Q) \defn \E_Q[f(dP/dQ)].
  \label{eq:f-divergence}
\end{equation}
It is easy to see that the KL-divergence is obtained by
$f(t) = t \ln t$ and the total variation by
$f(t) = \frac{1}{2}|t - 1|$.

Any divergence between distributions satisfies the information- processing inequality. In particular, if $P, Q$ are measures on $\Omega$, and we define a new random variable $f : \Omega \to X$ with corresponding probability measures $P_f, Q_f$ so that $P_f(A) = P(\cset{\omega \in \Omega}{f(\omega) \in A}$ then we have that
\begin{equation}
  \label{eq:inf-proc}
  D(P_f\|Q_f) \leq D(P \| Q).
\end{equation}

\begin{frame}
  \frametitle{Learning outcomes}
  \begin{block}{Understanding}
    \begin{itemize}
    \item The axioms of probability, marginals and conditional distributions.
    \item The philosophical underpinnings of Bayesianism.
    \item The simple conjugate model for Bernoulli distributions.
    \end{itemize}
  \end{block}
  
  \begin{block}{Skills}
    \begin{itemize}
    \item Be able to calculate with probabilities using the marginal and conditional definitions and Bayes rule.
    \item Being able to implement a simple Bayesian inference algorithm in Python.
    \end{itemize}
  \end{block}

  \begin{block}{Reflection}
    \begin{itemize}
    \item How useful is the Bayesian representation of uncertainty?
    \item How restrictive is the need to select a prior distribution?
    \item Can you think of another way to explicitly represent uncertainty in a way that can incorporate new evidence?
    \end{itemize}
  \end{block}
  
\end{frame}

%%% Local Variables:
%%% mode: latex
%%% TeX-engine: xetex
%%% TeX-master: "notes.tex"
%%% End:
