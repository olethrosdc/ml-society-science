\only<article>{
\chapter{Privacy}
\label{ch:privacy}
  One interpretation of privacy is simply the ability to
  maintain a personal secret. Is it possible to maintain perfect
  secrecy? Can the thoughts in our head be perfectly safe, or can they
  be revealed indirectly through our actions?

  While we certainly \emph{can} securely store and transmit data using
  cryptography, the problems we will discuss in this chapter are
  \emph{not} solvable solely through cryptography. We are interested
  in scenarios where we must make a public decision or release some
  summary statistics that depend on private data, in such a way as to
  minimise harm to the individuals contributing their data.

  If we never have to reveal any of our computations, cryptography is
  what is needed. For example, through homomorphic computation, an
  untrusted party can even perform some computations on encrypted
  data, returning an encrypted result to us, while learning nothing
  about the original secret.  As long as the data, and the results of
  any computation on it, are kept under lock and key, our personal
  information cannot be revealed.

  However, sometimes we must make public decisions or release
  public information based on this data. Then it can be revealed
  indirectly. For example, you can trust your doctor to maintain
  confidentiality, but when you go to the pharmacy to get the prescribed
  medicine, somebody can infer the medical condition you suffer from.

  It is even possible to learn personal information from aggregate
  statistics.  Let us say your doctor publishes a list of cases of
  different diseases every week, together with some other information
  such as the approximate patient age. Even though your own data is
  mixed with that of all other patients, it is possible to infer your
  diagnosis, especially with some additional side-information: If
  somebody knows you were the only person in that age group visiting
  the doctor that week, they will learn your diagnosis.
  
  From that point of view, it is not the data itself, but the complete
  process of data collection, treatment and public release that can be
  characterised as private or non-private.  For that reason, we will
  emphasise an \emph{algorithmic} view of privacy: participants
  entrust their data to an algorithm, which produces a useful output
  in return. The algorithmic process is typically not fully automated,
  as it also depends on some human input: One example is a medical
  study examining different treatments for a disease. While humans
  select and administer the treatments, they will typically rely on a
  randomised strategy for assigning treatments to individuals, and use
  a statistical method to report their results.  Given this mixture of
  ad-hoc decisions and formal algorithmic methods, is it possible to
  guarantee privacy in any sense? What kind of guarantees can we make?

  Generally speaking, an algorithm has good privacy properties, if the
  amount of information that can be revealed through the algorithm's
  output about any individual contributing data is bounded. In
  particular, we are interested in how much an adversary can learn
  about any individual's input to the algorithm from the algorithm's
  output.

  This does not preclude learning general facts about the population
  from the output, from which you can then make inferences about
  individuals. For example, a study about the use of steroids in
  sports may show that 90\% of sprinters with times under 10 seconds
  are using steroids, while only 50\% of slower sprinters do so. Any
  sprinter with a time under 10 seconds is thus suspected of using
  steroids by association. However, it does not matter if their data
  has been used in the study. The conclusion that steroids help
  improve performance would be true no matter if they contributed to
  the study data. The publication of the result does not impact their
  privacy, because the amount of harm it does to them does not depend
  in their participation: the same statistical result would have been
  obtained with or without them.
  
  In this chapter, we will look at two formal concepts of privacy
  protection: $k$-anonymity and \emph{differential privacy}. The first
  is a simple property of anonymised databases. However, it provides
  only limited resistance to identification, and is a property of the
  data, rather than the algorithm. Thus, from our point of view, it is
  not a useful privacy concepts as it does not characterise the
  privacy of a process. The latter is a more general concept, which
  provides full information-theoretic protection to
  individuals. However, it is conceptually not trivial to understand
  and its application has only recently been simplified due to the
  release of machine learning libraries that apply this privacy
  concept. A major problem with any privacy definition and method,
  however is correct interpretation of the privacy concept used, and
  correct implementation of the algorithm used.  }



\section{Things we do with data}

\only<presentation>{
  \begin{frame}
    \begin{block}{Reasons for data collection}
      \begin{itemize}
      \item To publish it.
      \item To publish specific statistics.
      \item To make decisions through arbitrary computations.
      \end{itemize}
    \end{block}
  \end{frame}
} \only<article>{ How do we use data in the first place? Sometimes we
  simply publish the data, perhaps after some initial
  processing. Publication of datasets is useful for researchers that
  want to do perform further analysis on the data. Usually though, we collect data in order to calculate specific statistics. For example the census collects data about the number of people in different households, wages, etc, and then publishes tables detailing the average age and number of people per household in different areas of the country. This demographic information is useful for policy makers, urban planning to organise voting centers and the distribution of police and fire stations, as well as other public services. }


\begin{frame}
  \begin{figure}
    \centering
    \includegraphics[height=\fheight]{../figures/smbc-the-problem}
    % Copyright: SMBC comics
  \end{figure}
\end{frame}
\begin{frame}

  \only<article>{ Fundamentally, privacy in statistics is an \emph{issue of
      trust.} The analyst, be it a human, or an automated service, will
    use your data to make decisions. You must also decide who to trust
    and how much. Do we trust the data analyst? How much privacy are
    we willing to sacrifice to the analyst? How much to the public at
    large?  What you want out of the service.  Is the service
    important enough to sacrifice significant amounts of privacy? What
    is an acceptable trade-off between utility and privacy?  These are
    difficult questions and are hard to quantify, hence we assume that
    we have already decided how to answer them, and we simply want to
    find an appropriate methodology for achieving a good result.  }
  
  \only<presentation>{
    \begin{block}{An issue of trust}
      \begin{itemize}
      \item Who to trust and how much.
      \item What you want out of the service.
      \end{itemize}
    \end{block}
  }

  \only<article>{ Consider a researcher wishing to collect data for a  statistical analysis. If the analysis is eventually
    published,\footnote{If somebody knows that the analysis is being
      conducted, however, they could still learn something private from the fact that the analysis has \emph{not} been published.} this
    creates two possible scenarios for that may lead to privacy violations.}
  \begin{itemize}
  \item Publication of ``anonymised'' data.
    \only<article>{Sometimes we may collect data in order to publish the dataset itself for other researchers to use. This is common practice in machine learning, with image classification datasets being a good example. However, there is always some privacy risk even if identifying information such as names are removed.}
  \item Public data analysis. \only<article>{In this setting, we only publish summary statistics or models about the data. A prime example is a national census analysis, which may provide detailed demographic information for all towns and regions in a nation. Although only aggregate data is published, it is theoretically possible to infer personal information. For that reason, the US Census Bureau conducted its analysis in 2020 using differential private algorithms.}
  \end{itemize}
  \begin{alertblock}{Cryptography is not enough.}
    \only<presentation>{
      Cryptography provides:
      \begin{itemize}
      \item Secure communication and computation.
      \item Authentication and verification.
      \end{itemize}
      Public outputs are by definition \alert{not} protected.
    }
    \only<article>{Cryptography provides secure communication and computation, authentication and verification. These are used to establish secure channels with somebody that we trust. However, the privacy violations we are concerned with relate to \alert{publicly released} outputs of algorithms. It is not important whether or not all the data and computation are encrypted: as long as the algorithm generates a public output, it is theoretically possible for somebody to learn something about the algorithm's input.}
  \end{alertblock}


\end{frame}



\section{Statistical disclosure}

\only<presentation>{
  \begin{frame}
    \tableofcontents[ 
    currentsection, 
    hideothersubsections, 
    sectionstyle=show/shaded
    ] 
  \end{frame}
}

\only<article>{
  \begin{centering}
    \emph{``Data is everywhere'', said the statistician, ``data, data!''.}
  \end{centering}

  In the past, statistical analysis was performed
  with laboriously collected and annotated datasets. Even as
  recently as in the early 21st century, databases for machine
  learning were limited to a few thousand entries at most. At the
  time of writing, not only has the size of datasets become
  extremely large, but the sources of data are much more
  diverse. Data are collected and commercialised whenever we visit a
  website and even as we walk around with our phone. To a limited
  extent, there is a tradeoff between what we can get out of a
  service and what we pay into it. Many free services such as
  navigation software rely on collecting user data to perform
  better: If you can tell that there is a traffic jam in Central
  Avenue, you can after all try and take another route.  As long as
  informed consent exists, use of private data is generally regarded
  as unproblematic.\footnote{This is actually underscored by the
    GDPR legislation, which focuses on consent and data use
    methods.}

  However, even apparently benign data collected with appropriate
  consent can lead to serious and unexpected privacy
  violations. There are three famous examples of this: Firstly, the
  identification of people in supposedly anonymous health data in
  the 1990s in the state of Massachussets, which we will go over in
  detail in this chapter. Secondly, the identifications of users
  through anonymised movie ratings in the Netflix dataset. Finally,
  the ability to discover if any given individual's data is
  contained in a pooled genomic study.
}
\begin{frame}
  \frametitle{Anonymisation}
  \only<article>{Data is collected for many reasons. Any typical service you might want to use will require a minimal amount of data. For example, a dating service will at a minimum require your age and location, as shown in the example below.}
  \begin{example}[Typical relational database in a dating website.]
    \begin{table}[H]
      \centering
      \begin{tabular}{l|l|l|l|l|l|l}
        Birthday & Name & Height  & Weight & Age & Postcode & Profession\\
        \hline
        06/07 & \only<1>{Li Pu} & 190 & 80 & 60-70 & 1001 & Politician\\
        06/14 & \only<1>{Sara Lee} & 185 & 110 & 70+ & 1001 & Rentier\\
        01/01 & \only<1>{A. B. Student} & 170 & 70 & 40-60 & 6732 & Time Traveller
      \end{tabular} 
    \end{table}
    \only<article>{ If somebody is a user in a dating website, you
      expect them to give some minimal personal information to be
      stored in the site's database. This might include their
      birthday, location and profession, for example.} 
  \end{example}
  \only<article>{When you submit your data to a service, you expect it to be used responsibly. For the dating service, you expect it to use the information to find good matches, based on your preferences and location. Whenever somebody uses the service, they obtain some information about you, at least indirectly. For example, if they make a query for similar singles in their neighbourhood, and they see your profile, then they have learned that you live nearby.

    If we wish to publish a database, frequently we need to protect the identities of the people involved. A simple idea is to erase directly identifying information. However, this does not really work most of the time, especially since attackers can have side-information that can reveal the identities of individuals in the original data, when combined with the published dataset.
  }
  
  \only<2>{The simple act of hiding or using random identifiers is called anonymisation.}
  \only<article>{However this is generally insufficient as other identifying information may be used to re-identify individuals in the data.   In particular, even if somebody is unable to infer the information of any individual from the published, anonymised, dataset, they may be able to do so via some side-information. All that is needed is another dataset with some columns in common with the dataset they want to target.}
\end{frame}


\begin{frame}
  \frametitle{Record linkage}
  \begin{figure}
    \begin{subfigure}{0.65\textwidth}
      \centering \def\firstcircle{(0,0) circle (7em)}
      \def\secondcircle{(3,0) circle (7em)}
      
      \begin{tikzpicture}
        \uncover<1,3>{ \draw \firstcircle node[text width=7em]
          {Ethnicity\newline Date\newline Diagnosis \newline Procedure
            \newline Medication \newline Charge }; } \uncover<3>{
          \begin{scope}
            \clip \firstcircle; \fill[red] \secondcircle;
          \end{scope}
        } \uncover<2,3>{\draw \secondcircle node [text width=2em,
          align=right] {Name \newline Address \newline Registration
            \newline Party \newline Lastvote}; } \node [text width=4em]
        (QI) at (1.5, 0) {Postcode \newline Birthdate \newline Sex};
        \uncover<3>{ \node [text width=16em] (qi-text) at (1.5, -3)
          {87\% of Americans identifiable}; \path[->]<1-> (qi-text) edge [bend left]
          (QI); }
      \end{tikzpicture}
      \caption{Voter registration}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
      \centering
      \includegraphics[height=0.8\fheight]{../figures/Bill_Weld}
      %% Copyright: CC BY 2.0 
      \caption{Bill Weld}
    \end{subfigure}

    \caption{Linkage attack on a anonymised patient information database. A misguided attempt to release medical information as public data (left circle in figure (a)) by then-governor Bill Weld resulted in a simple linkage attack. This was done by accessing a party registration database (right circle in figure (b)) which contained three common fields with the medical database. }
  \end{figure}
  \only<article>{
    As an example, in the 1990s, the governor of Massachussets,
    decided to publish anonymised information about the health records
    of individual state employs. They were careful to hide all
    obviously identifying information such as their name, and thus
    claimed that there are no potential privacy violations. However,
    they left in some information that they thought would be useful
    for researchers: the postcode, the birthdate, and the sex of each
    individual.
    
    This allowed a PhD student, McSweeney, to perform the following
    linkage attack. She ordered a floppy disk with a database of
    registered voters in the state. This contained names and addresses,
    as well as the postcode, birth date and sex of every voter registration. Since these three fields were also present in the health data, she was able to cross-reference the two datasets, and so obtain the
    identities of many individuals in that database. The first record she
    obtained was that of the governor himself, Bill Weld. Later, she
    estimated that approximately 87\% of Americans are uniquely
    identifiable through those three attributes.
  }
\end{frame}


% \begin{frame}
%   \frametitle{Data linkage with SQL}
%   The original database \verb|database| and adversary side information \verb|side-information| can be combined using the following simple SQL query:
% \begin{verbatim}
% SELECT * FROM database JOIN side-information ON [condition]
% \end{verbatim}
%   where \verb|condition| describes how to match the records.

%   \begin{example}
%     For the databases given above, we could use
% \begin{verbatim}
% SELECT * FROM tinder JOIN tax ON tinder.height = tax.height AND tinder.age = tax.age
% \end{verbatim}
%     to create a joint table.
%   \end{example}
% \end{frame}

\only<article>{ Clearly, anonymisation is not enough. So, is there a
  way to formally guarantee privacy? In the next section we will go
  over the solution of McSweeny, which provides us with an data-oriented
  definition of anonymity. While this provides some degree of
  protection against certain linkage attacks, it is sadly insufficient
  to protect privacy in general, as this anonymity concept is applied to a dataset, not the process. Later, we will introduce the notion   of differential privacy, which protects individuals against statistical disclosure in an information-theoretic manner, giving guarantees about the complete data processing process, that is applicable for any dataset being processed.  }


\section{Algorithmic privacy.}
\only<article>{ The above discussion should help emphasise that
  \emph{a dataset} cannot be characterised as
  private or non-private. We should actually start from the
  observation that \emph{any data contributed by an individual} should
  be considered private, because it could be combined with other
  datasets to obtain sensitive personal information.

  For that reason, we wish to ensure that, whenever individuals
  contribute data for processing, the result of the processing cannot
  be used to reveal whatever information they had contributed. Thus,
  the idea of privacy only applies on \emph{algorithms} that are used
  on personal data. This is similar to cryptography: it is e.g. the encryption algorithm that is secure. And if it is, it should be secure for all messages sent using it. It is not possible to say if a specific encrypted message is secure, unless you know how it was encrypted.
  
  Generally speaking, we wish for our privacy-preserving
  data-processing algorithms to both be useful and have good privacy
  properties. Unfortunately, as we shall see in the sequel, there is a
  distinct trade-off between privacy and utility. However, let us
  first specify more precisely what we mean by an algorithm.  }
\begin{frame}  
  \frametitle{What is an algorithm?}

  \only<article>{Any functional process is an algorithm. The
    information given to an algorithm for processing is called an
    \emph{input} to the algorithm. The result of the process is called
    the algorithm's \emph{output}.  In this book, we identify inputs with
    observations or features $\CX$, and outputs with actions $\CA$ by
    the algorithm: i.e. we view the algorithm as taking actions that
    have an observable effect on its external environment.  Many common algorithms
    are deterministic: given the same input, they produce the
    same output. However, in privacy and security, it is necessary to consider
    \emph{stochastic} algorithms, whose output randomly changes, even
    when the input is the same.
    
  }
  
  \begin{definition}[Stochastic algorithm $\pi$]
    A stochastic algorithm $\pi$ with input domain $\CX$ and output
    domain $\CA$ is a mapping $\pi : \CX \to \Simplex(\CA)$ from
    observations $\CX$ to \emph{distributions} over outputs
    $\Simplex(\CA$). When $\CA$ is finite, we will write
    $\pol(a \mid x)$ to denote the conditional probability that the
    algorithm outputs $a$ given input $x$.
  \end{definition}
  \only<article>{

    So, one way of seeing a stochastic algorithm is as a
    \emph{collection} of probability distributions $\pol(a \mid x)$
    over $\CA$, one for each value of $x$. Let us contrast this 
    to a deterministic algorithm $f : \CX \to \CA$,
    which would be a simple function. This would define one specific
    value in $\CA$ (instead of a distribution) for each value of $x$.

    The above definition is fine when the output domain $\CA$ is
    finite. Then outputs $a \in \CA$ cannot have infinitesimally small
    probabilities. However, in many problems $\CA$ is not finite, but a Euclidean
    subset, e.g. the interval $[0,1]$. In that case, we have two
    choices: either treat $\pol$ as a probability density, or as a
    probability measure. The latter is more general, and applies no
    matter what $\CA$ is. Formally, we can write this as follows.
    
    \begin{theoryblock}{The general case.}
      It is best to think of $\pol(\cdot | x)$ as a different probability
      measure over $\CA$ for every possible value of $x$. Then we write:
      \[
        \pol(A \mid x) \defn \Pr_\pol(a \in A \mid x), \qquad A \subset \CA,
      \]
      for the conditional probability that algorithm's output is in
      some set $A \subset \CA$ given input $x$. \footnote{Formally, $A$ is not an arbitrary subset, but belongs to a $\sigma$-algebra of $\CA$, in order to avoid strange complications. However, we not need be concerned with this technicality in this book.} This allows us to treat
      the case when $\CA$ is continuous or discrete with the same notation.
    \end{theoryblock}

    How can we actually obtain a stochastic algorithm? The usual
    method is to define an algorithm as usual, but with a source of
    randomness as part of its inputs, fancifully called \emph{random
      coins}. By using those random coins as part of the calculations,
    we obtain a random result, even if every step of the algorithm is
    deerministic.

    \begin{exampleblock}{Algorithmic randomness.}
      We can construct a random algorithm through access to a random
      coin $\omega$ taking values in $\Omega$. We can then define the
      output through a deterministic function $a_\pi(\omega, x)$. Since
      $\omega$ is random, the output of the function is also random.
      If $P$ is the probability distribution of $\omega$, then:
      \[
        \pi(A \mid x) = P(\cset{\omega \in \Omega}{a_\pi(\omega, x) \in A}),
      \]
      i.e. the probability that the algorithm's output is in $A$ is
      equal to the measure of the values of $\omega$ for which the
      $a_\pi(\omega, x) \in A$.
    \end{exampleblock}
  }
\end{frame}

\begin{frame}
  \frametitle{What is private?}

  \only<article>{
    The overall framework we will use in this book is that everything
    pertaining to an individual is by definition private. While in
    reality there might be some attributes that individuals might want
    to remain secret and some which are not considered important, it is
    generally impossible to predict potential harm from the disclosure
    of any individual information.  Thus, this chapter takes an
    expansive view of privacy: any information about an individual,
    which includes even the very fact that they might be part of a
    database is considered private.

    We assume that there is a \emph{private input}, which is processed
    by an \emph{algorithm}, which then generates a \emph{public output}.
    So, our main focus is how the algorithm generates the output,
    because the algorithm defines how the output and the input are
    linked. Our goal is to obtain algorithms that can be generally
    applied and that can have good privacy-preserving properties.
  }
\end{frame}



\section{Simple anonymisation and $k$-anonymity}
\label{sec:k-anonymity}
\only<article>{ One of the basic ideas for protecting individual
  information is to simply remove identifying information. However,
  this is easier said than done, as potentially any information can be
  used identify an individual in a dataset. Indeed, simply removing the names of people from a database is a very weak method, as there almost always exist enough information in the database to re-identify individuals. However, a slightly stronger, though not perfect, method for preventing re-identification is given by the framework of $k$-anonymity. }
\begin{frame}
  \frametitle{$k$-anonymity}
  \begin{figure}[H]
    \centering
    \begin{subfigure}{0.45\textwidth}
      \centering
      \includegraphics[width=0.9\fwidth]{../figures/samarati}
      \caption{Samarati}
    \end{subfigure}
    \hspace{0.05\textwidth}
    \begin{subfigure}{0.45\textwidth}
      \centering
      \includegraphics[width=0.9\fwidth]{../figures/sweeney}
      \caption{Sweeney}
    \end{subfigure}
  \end{figure}
  \only<article>{The concept of $k$-anonymity was introduced
    by~\citet{samarati1998protecting} and provides some guarantees
    against inferring personal information from a single
    database. This requires the analyst to first determine the
    variables of interest (which should not be modified), and then
    determine which variables are \emph{quasi-identifiers}, i.e. they
    could be potentially used to identify somebody in the database.}
  \alert{It's the analyst's job to define quasi-identifiers.} However, in general all variables should be considered quasi-identifiers.
  \begin{definition}[$k$-anonymity]
    A database provides $k$-anonymity if for every person in the
    database is indistinguishable from $k-1$ persons with respect to
    \emph{quasi-identifiers}.
  \end{definition}
  \only<article>{This hope is that, if the database satisfies
    $k$-anonymity it can be safely released, without revealing any
    private information directly. As you can see, the definition of
    $k$-anonymity relates to the algorithm \emph{output}, and not the
    algorithm itself. Because of this, it is not possible to give
    formal guarantees that hold generally for a $k$-anonymous
    database, as the result of the process does not tell us anything
    about how much information it conveys about the original input.

    But first, let us walk through an extended example to explain the
    concept.}
\end{frame}

\begin{frame}
  \frametitle{$k$-anonymity example}
  \only<article>{
    In particular, let us say that the analyst simply wants to calculate some statistics about how different professions correlate with age, weight, height and where people live. Some areas of the country might produce more politicians, for example. And taller people may be more successful in politics. The initial data collected might look like the table below. It was obvious to the analyst, that even if he did remove all the names, somebody knowing where A. B. Student lived and saw the table would have little trouble recognising them.
  }

  \only<1>{
    \begin{table}[H]
      \centering
      \begin{tabular}{l|l|l|l|l|l|l}
        Birthday & Name & Height  & Weight & Age & Postcode & Profession\\
        \hline
        06/07 & Li Pu & 190 & 80 & 65 & 1001 & Politician\\
        06/14 & Sara Lee & 185 & 110 & 67 & 1001 & Rentier\\
        06/12 & Nikos Karavas & 180 & 82 & 72+ & 1243 & Politician\\
        01/01 & A. B. Student & 170 & 70 & 52 & 6732 & Time Traveller\\
        05/08 & Li Yang & 175 & 72 & 35 & 6910 & Politician
      \end{tabular}
      \caption{1-anonymity.}
    \end{table}

  }
  \only<presentation>{
    \only<2>{
      \begin{tabular}{l|l|l|l|l|l|l}
        Birthday & Name & Height  & Weight & Age & Postcode & Profession\\
        \hline
        06/07 &  & 190 & 80 & 60+ & 1001 & Politician\\
        06/14 &  & 185 & 110 & 60+ & 1001 & Rentier\\
        06/12 &  & 180 & 82 & 60+ & 1243 & Politician\\
        01/01 &  & 170 & 70 & 40-60 & 6732 & Time Traveller\\
        05/08 &  & 175 & 72 & 30-40 & 6910 & Politician
      \end{tabular}
      1-anonymity
    }

    \only<3>{
      \begin{tabular}{l|l|l|l|l|l|l}
        Birthday & Name & Height  & Weight & Age & Postcode & Profession\\
        \hline
        06/07 &  & 180-190 & 80+ & 60+ & 1* & Politician\\
        06/14 &  & 180-190 & 80+ & 60+ & 1* & Rentier\\
        06/12 &  & 180-190 & 80+ & 60+ & 1* & Politician\\
        01/01 &  & 170-180 & 60-80 & 20-60 & 6* & Time Traveller\\
        05/08 &  & 170-180 & 60-80 & 20-60 & 6* & Politician
      \end{tabular}
      1-anonymity
    }
  }

  \only<article>{After thinking about it for a bit, the analyst decides to remove the birthday and name, and broadly categorise people according to their height in increments of 10cm, the weight in increments of 20cm, and keep just the first digit of the postcode. Now, that looks much more reasonable. }
  \only<4>{
    \begin{table}[ht]
      \centering
      \begin{tabular}{l|l|l|l|l|l|l}
        Height  & Weight & Age & Postcode & Profession\\
        \hline
        180-190 & 80+ & 60+ & 1* & Politician\\
        180-190 & 80+ & 60+ & 1* & Rentier\\
        180-190 & 80+ & 60+ & 1* & Politician\\
        170-180 & 60-80 & 20-60 & 6* & Time Traveller\\
        170-180 & 60-80 & 20-60 & 6* & Politician
      \end{tabular}
      \caption{2-anonymity: the database can be partitioned in sets of at least 2 records}
    \end{table}
  }


  \only<article>{However, with enough information, somebody may still
    be able to infer something about the individuals. In the example
    above, it remains true that if somebody knows that both
    A. B. Student and Li Yang are in the database, as well as their
    postcodes, as well as that Li Yang is a politician, they can infer
    A. B. Student's profession.  Fortunately, there is a way to
    protect individual information from adversaries with arbitrary
    side-information. This is given by differential privacy.  }
\end{frame}



\section{Differential privacy}
\label{sec:differential-privacy}
\only<article>{ This section introduces one of the main tools for
  giving formal guarantees about the privacy of any algorithm ran on
  a dataset, \emph{differential privacy}. This will provide
  individual-level privacy, in the sense that an algorithm that is
  differentially private guarantees that no adversary can
  significantly increase their knowledge about any particular
  individual by observing the algorithm's output. This is
  independent of the adversary's existing knowledge, or
  computational power. }

\only<article>{While $k$-anonymity can protect against specific
  re-identification attacks when used with care, it says little about
  what to do when the adversary has a lot of knowledge. For example,
  if the adversary knows the data of everybody that has participated
  in the database apart from the $i$-th person, it is trivial for them to infer what the $i$-th person's data
  is. For some particularly sensitive datasets, we may want for the
  adversary to be unable to tell whether or not somebody's data was part of
  the base at all! Differential privacy offers protection against adversaries
  with unlimited side-information or computational power. Intuitively,
  an algorithmic computation is differentially-private if an adversary
  cannot distinguish two ``neighbouring'' databases based on the result
  of the computation. Informally, two databases are neighbours when
  they are identical apart from the data of one person. If an adversary cannot tell if an individual's data has been included in the database, this is equivalent to saying that he cannot distinguish between two databases $x, x'$ where $x$ includes the data of this one person and $x'$ does not. Differential privacy gives such a guarantee for \emph{any} pair of neighbouring databases.  
  Differentially private algorithm must use randomness in order to make
  it impossible for somebody to tell from the algorithm's output
  whether any specific individual's data was in the database with absolute certainty. The more random the algorithm, the harder it is for the adversary to correctly guess something about the data. However, this also makes the algorithm less useful.}

\begin{frame}
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[node distance=0.4\textwidth]
      \node[label=left:$x$] at (0,0) (data) {\includegraphics[width=0.1\columnwidth]{../figures/medical}};

      \node[label=$x_1$] at (-2,3)(patient1) {\includegraphics[width=0.05\columnwidth]{../figures/me-recent}};
      \uncover<3->{
        \node[label=$x_2$] at (2,3) (patient2) {\includegraphics[width=0.1\columnwidth]{../figures/judge}};
      }
      \uncover<4->{
        \node[label=$a$] at (4,0)   (statistics) {\includegraphics[width=0.2\columnwidth]{../figures/coronary-disease}};
      }
      \uncover<2->{
        \draw[->] (patient1) -- (data);
      }
      \uncover<3->{
        \draw[->] (patient2) -- (data);
      }
      \uncover<4->{
        \draw[->] (data) -- node[above]{$\pol$} (statistics);
      }
      \uncover<5->{
        \draw[line width=5, red, ->] (statistics) -- (patient2);
      }
    \end{tikzpicture}
    \caption{If two people contribute their data $x = (x_1, x_2)$ to a medical database, and an algorithm $\pol$ computes some public output $a$ from $x$, then it should be hard infer anything about the data from the public output.}
    \label{fig:privacy-data}
  \end{figure}
  \only<article>{ Consider the example given in
    Figure~\ref{fig:privacy-data}, where two people contribute their
    data to a medical database. The $i$-th individual contributes data
    $x_i$, and the complete dataset is $x = (x_1, x_2)$. The algorithm
    $\pol$ defines a distribution over the set of possible outputs
    $a \in \CA$, with $\pol(a | x)$ being the probability that the
    algorithm outputs $a$ if the data is $x$.  If the algorithm is a
    deterministic function then it might be
    possible for an adversary to invert the computation and obtain $x$
    from $a$.  But even if that is not possible, maybe they can learn
    \emph{something} about the data from the output. In the section
    below, we will formalise this notion.

    \begin{example}
      To better develop your intuition, assume that we have a
      deterministic algorithm $f : \CX \to \CA$, and that there is a
      finite set $\CA$ of possible outputs $a$. Then each database $x$
      maps to one of those possible outputs, and so we can partition
      the space $\CX$ of datasets into $\CX_1, \ldots, \CX_{|\CA|}$,
      with $\CX_a$ being the set of values of $x$ for which the
      $f(x) = a$. Then, by observing $a$, you already know with
      certainty that $x \in \CX_a$. Now assume that the adversary has
      some prior knowledge that $x \in S \subset \CX$. Combined with
      the output, his new knowledge is now that $x \in S \cap \CX_a$,
      so they can definitely narrow down the set of possible
      datasets. If we allow the adversary to compute other functions
      $f_1, \ldots, f_K$ on the data, they can eventually discover
      $x$. This is because, at every step $k$, they compute a
      different function $f_k$ with answer $a_k$, which further limits
      the set of possible values of $x$ to
      $S \cap \CX_{a_1} \cap \CX_{a_2} \cap \cdots \cap
      \CX_{a_k}$. This set can eventually shrink to a single point if
      the adversary is allowed to compute arbitrary functions on the
      data.
    \end{example}
    }
\end{frame}

\begin{frame}
  \frametitle{Privacy desiderata}
  \only<article>{
    Consider a scenario where $n$ persons give their data $x_1, \ldots, x_n$ to an analyst. This analyst then performs some calculation on the data and publishes the result through a randomised algorithm $\pol$, where for any output $a$, and any dataset $x$, $\pol(a | x)$ is the probability that the algorithm generates $a$.
    The following properties are desirable from a general standpoint.

    \paragraph{Anonymity.} Individual participation in the study remains a secret. From the release of the calculations results, nobody can significantly increase their probability of identifying an individual in the database. So, if you do participate in the database, this cannot be easily detected.

    \paragraph{Secrecy.} The data of individuals is not revealed. The release does not significantly increase the probability of inferring individual's information $x_i$. So, if you do participate in the database, it is not an easier to retrieve personal information compared to not participating.

    \paragraph{Side-information.} Even if an adversary has arbitrary side-information, he cannot use that to amplify the amount of knowledge he would have obtained from the release. In other words, your participation to the database can only slightly increase the adversary's knowledge about you, irrespective of what other information they have.

    \paragraph{Utility.} The released result has, with high probability, only a small error relative to a calculation that does not attempt to safeguard privacy. Consequently, there is an incentive to participate in the computation as long as its results are useful to you.
  }
  \only<presentation>{
    We wish to calculate something on some private data and publish a \alert{privacy-preserving}, but \alert{useful}, version of the result.
    \begin{itemize}
    \item Anonymity: Individual participation remains hidden.
    \item Secrecy: Individual data $x_i$ is not revealed.
    \item Side-information: Linkage attacks are not possible.
    \item Utility: The calculation remains useful.
    \end{itemize}
  }
\end{frame}

\subsection{The randomised response mechanism}
\only<article>{
  In this section, we will explain one of the simplest possible privacy-preserving mechanisms. This is a mechanism where the data of the individuals are randomised \emph{before} they are sent to the analyst. Thus, the analyst does not know if the data they have from any one individual corresponds to the truth or not. This type of mechanism is called \emph{local}, because the privacy algorithm is applied locally at the user level. The user does not have to trust anybody. Indeed, the simplest variant of this algorithm is so simple that every participant can implement it using just a coin. We will illustrate this mechanism with an example, where we wish to measure the amount of illicit drug use in sport, but any other example where we want to calculate the expected value of a binary random variable will do.
}
\begin{frame}
  \begin{exampleblock}{The prevalence of drugs in sport}
    
    \only<article>{
      Let's say you need to perform a statistical analysis of the drug-use habits of athletes. Obviously, even if you promise the athlete not to reveal their information, you still might not convince them. Yet, you'd like them to be truthful. The trick is to allow them to randomly change their answers, so that you can't be \emph{sure} if they take drugs, no matter what they answer.
    }

    \only<presentation>{
      \begin{itemize}
      \item $n$ athletes
      \item Ask whether they have doped in the past year.
      \item Aim: calculate \% of doping.
      \item How can we get truthful / accurate results?
      \end{itemize}
      \only<1>{
        \alert{Write responses in class}
      }
    }
    \only<2->{
      \only<article>{
        \begin{groupactivity}{Algorithm for randomising responses about drug use}
        }
        \begin{enumerate}
        \item Flip a coin.
        \item If it comes heads, respond truthfully. 
        \item Otherwise, flip another coin and respond \texttt{yes} if it comes heads and \texttt{no} otherwise.
        \end{enumerate}
        \only<article>{
        \end{groupactivity}
        }
        \only<article>{
          \begin{exerciseblock}{Calculating the true rate of responses.}
            Assume that the observed rate of positive responses in a sample is $p$, that everybody follows the protocol, and the coin is fair. Then, what is the true rate $q$ of drug use in the population?
          \end{exerciseblock}
        }
      
    }
  \end{exampleblock}
  \only<3->{
  }
  \onslide<3->{
    \only<presentation>{
      \begin{proof}[Solution]
        Since the responses are random, we will deal with expectations first
        \begin{align*}
          \E p
          &= \frac{1}{2} \times \frac{1}{2} + q \times \frac{1}{2}
            \uncover<4->{= \frac{1}{4} + \frac{q}{2}}
            \uncover<5->{\\
          q &= 2 \E p - \frac{1}{2}.}
        \end{align*}
        
      \end{proof}
    }
  }
  \only<article>{The problem with this approach, of course, is that half of the people will just give a random response. Thus, we are effectively throwing away half of our data sources. In particular, if we repeated the experiment with a coin that came heads at a rate $\epsilon$, then our error bounds would scale as $O(1/\sqrt{\epsilon n})$ for $n$ data points.}
\end{frame}

\begin{frame}
  \only<article>{Another drawback of this algorithm is that it is very specific: it assumes binary responses, and it uses a fair coin, which introduces a lot of noise.  Since the coin flips make the responses noisy, we may want to have some way of controlling it. In fact, the above mechanism can be seen slightly differently. With probability $75\%$, the mechanism outputs the true value, i.e. $a_i = x_i$. With probability $25\%$, it outputs the opposite value, i.e. $a_i = 1 - x_i$. 

    In general, we want to consider an algorithm that takes data $x_1, \ldots, x_n$ from $n$ users transforms it randomly to $a_1, \ldots, a_n$ using the following mapping, with a fixed probability of flipping the data of the $i$-th individual.}
  \begin{block}{The binary randomised response mechanism}
    \begin{definition}[Randomised response]
      The $i$-th user, whose data is $x_i \in \{0,1\}$, responds with $a_i \in \{0, 1\}$ with probability
      \[
        \pol(a_i = j \mid x_i = k) = p,  \qquad  \pol(a_i = k \mid x_i = k) = 1 - p,
      \]
      where $j \neq k$.
    \end{definition}
  \uncover<2->{Given the complete data $x$, the algorithm's output is $a = (a_1, \ldots, a_n)$.}
  \uncover<3->{Since the algorithm independently calculates a new value for each data entry, the output probability is
    \[
      \pol(a \mid x) = \prod_i \pol(a_i \mid x_i)
    \]
  }
  \end{block}


  \only<article>{The randomised response mechanism satisfies the
    formal notion of $\epsilon$-differential privacy, which will be
    given later in Definition~\ref{def:epsilon-dp}. In a more general
    setting, we may have multiple possible responses, each one
    corresponding to a different feature of each individual. While the
    algorithm can be generalised to $n$-ary outputs, the special case
    of when the outputs are integers is deferred until later.

  }
\end{frame}

\begin{theoryblock}{The original randomised response mechanism~\cite{werner:1965}}
  As first proposed by~\citeauthor{werner:1965}, the mechanism distributes spinners to people. The spinner has a probability $p$ of landing on $A$ and $1 - p$ of landing on $B$. This can be easily arranged by having the corresponding areas to have the appropriate proportions. The interesting thing about this mechanism is that the responders must merely say whether or not the spinner landed on the group they identify with. They do \emph{not} have to reveal a group. This makes the mechanism feel like you are revealing less, even if it is just a special case of a general randomised response mechanism.
\end{theoryblock}

\begin{frame}
  \frametitle{What can we learn from the output?}

  \only<article>{
    Before we go into the formal privacy definition, it is instructive to see how much an adversary can learn from observing the output of the randomised response mechanism. For simplicity, we can think of the response of a single individual: the arguments easily carry over for the general case.
    
    In particular, let us consider the Bayesian setting: we can think
    of the adversary as having some prior belief $\bel(x_i)$,
    expressed as a probability distribution over the secret value of
    each individual.

    After the adversary observes the output $a_i$ corresponding to
    individual $i$, and assuming they know the algorithm that
    generated the output was the randomised response mechanism with
    parameter $p$, they can form a posterior belief
    $\bel(x_i \mid a_i)$, representing the information they
    collected. The following example quantifies the amount of
    information gained by the adversary.  }
  \begin{example}
    \only<article>{
      For simplicity, consider only one individual, and let the
      adversary have a prior $\bel(x = 0) = 1 - \bel(x = 1)$ over the
      values of the true value of an individual. We use the
      randomised response mechanism with parameter $p$ and the
      adversary observes the randomised data $a = 1$ for that
      individual, then what is $\Pr^\pol_\bel(x = 1 \mid a = 1)$, assuming the
      adversary knows the mechanism?}
    \only<presentation>{
      If the adversary has prior $\bel(x)$, with $\bel(0) = \theta$, and we use randomised-response with flipping probability $p$, what is
      \[
        \bel(x = 1 \mid a = 1 )
      \]
    }
    Bayes's theorem states that\footnote{If there are too many symbols for you, you can write Bayes theorem simply with $P(x | a) = P(a | x) P(x) / P(a)$. However, throughout the book we use $\bel$ for subjective belief and $\pol$ for algorithms, while $\Pr_\bel^\pol$ denotes probabilities of events given a specific $\pol$ and $\bel$.}
    \[
      \Pr^\pol_\bel(x \mid a) = \pol(a \mid x) \bel(x) / \Pr^\pol_\bel(a),
    \]
    where
    \[
      \Pr^\pol_\bel(a) = \sum_{x'} \pol(a \mid x') \bel(x'),
    \]
    is the marginal distribution over answers $a$ given the prior and the mechanism.
    Define $q = \bel(x = 1)$ to save on notation. Then we can write the above as:
    \[
      \Pr^\pol_\bel(x = 1 \mid a = 1) =
      \frac{\pol(a = 1 | x = 1) q}
      {\pol(a = 1 | x = 1) q + \pol(a = 1 | x = 1) (1 - q)}
      =
      \frac{(1 - p) q}{(1 - p)q + p(1 - q)}.
    \]
    Let us plot this function in Figure~\ref{fig:randomised-response}. On the left side, $p = 0$, and so the outcome $a$ gives us perfect information about $x$. However, when $p \to 1/2$, the answers become completely random and so the posterior distribution approaches the prior, as the answer give us no information.
    \begin{figure}[h]
      \centering
      \begin{tikzpicture}[domain=0:0.5]
        \begin{axis}[xlabel={$p$},
          ylabel={$\Pr^\pol_\bel(x = 1\mid a= 1)$},
          grid=both,
          legend style={anchor=north west}
          ]
          \addplot[color=blue, dotted] (x, {(1 - x)*0.9 / ((1 - x)*0.9 +  x*0.1)});
          \addlegendentry{$q=0.9$};
          \addplot[color=green, solid] (x, {(1 - x)*0.5 / ((1 - x)*0.5 +  x*0.5)});
          \addlegendentry{$q=0.5$};
          \addplot[color=red, dashed](x, {(1 - x)*0.2 / ((1 - x)*0.2 +  x*0.8)});
          \addlegendentry{$q=0.2$};
      \end{axis}
    \end{tikzpicture}
      \caption{The posterior belief that the data of an individual is $x= 1$ if $a = 1$ has been observed, as we vary the flipping probability $p$. for three different values of the prior probability $q \in \{0.2, 0.5, 0.9\}$. In all cases, when the flipping probability is 0, we are perfectly sure that $x=1$. As $p$ approaches $1/2$, our posterior approaches the prior, since for $p$ closer to $1/2$, the answers are more random, and hence the evidence is weaker.}
      \label{fig:randomised-response}
    \end{figure}
    Analytically, the case where the prior is $q = 1/2$ is the simplest. Then: 
    \[
      \Pr^\pol_\bel(x = 1 \mid a = 1) = 1 - p.
    \]
    so we only have limited evidence for whether $x=1$. Now consider the case where we have some arbitrary prior $q$, and $p=1/2$. This means that the output of the algorithm is completely random. Consequently:
    \[
      \bel(x = 1 \mid a = 1) = q = \bel(x=1).
    \]
    So, in this scenario we learn nothing from the algorithm's output, as is also shown in the figure. 
  \end{example}
\end{frame}


\subsection{Differential privacy.}
\begin{frame}
  \only<presentation>{
    \includegraphics[width=0.2\textwidth]{../figures/dwork} \hspace{1em}
    \includegraphics[width=0.2\textwidth]{../figures/mcsherry} \hspace{1em}
    \includegraphics[width=0.2\textwidth]{../figures/nissim} \hspace{1em}
    \includegraphics[width=0.2\textwidth]{../figures/smith}
  }
  
  \only<article>{Now let us take a look at a way to characterise the  the inherent privacy properties of algorithms. We will focus on differential privacy, which can be seen as a bound on the information an adversary with arbitrary power or side-information could extract from the result of a computation $\pol$ on the data. As we already implied from previous examples, and from the Bayesian computation of randomised response, this computation has to be stochastic.}

  \only<article>{First, let us say what we mean by a dataset, and how
    it relates to individual. We assume that algorithms are applied to
    datasets $x = (x_1, \ldots, x_n)$ composed of the data of $n$
    individuals. Thus, all privacy guarantees relate to the data
    $x_i$ contributed by individuals $x_i$.    We use $\CX$ to denote the space of all datasets, and we are interested in how the output distribution of the algorithm $\pol$ changes for different datasets $x, x' \in \CX$.

    The notion of differential privacy was introduced
    by~\citet{dwork2006calibrating}, where the authors specifically
    related the ability of an adversary to distinguish neighbouring
    dataset $x, x' \in \CX$ to how similar the distributions of
    outputs would be in each case.  Intuitively, this says, that if
    $x, x'$ only differ in one entry, then it will be hard to tell
    from the algorithm's output whether it was generated by $x$ or
    $x'$, because the output distribution will be almost the same.
    So, it will be difficult for an adversary to guess the data by
    seeing the output.  And because each individual only has a unique
    entry in the database, this also means it is hard to tell
    something about that individual's data. Let us now give the formal definition.}
  
  \only<article>{
    \begin{theoryblock}{$\epsilon$-Differential Privacy}
      }
    \begin{definition}
      \label{def:epsilon-dp}
      Consider stochastic algorithm $\pol : \CX \to \Simplex(\CA)$, mapping from datasets to distributions over outputs $\CA$. In addition, $\CX$ is endowed with a neighbourhood relation $\neigh$. The algorithm $\pol$ is  $\epsilon$-differentially private if, for all $A \subset \CA$:
      \begin{equation}
        \label{eq:epsilon-dp}
        \left|\ln \frac{\pol(A \mid x)}{\pol(A \mid x')}\right| \leq \epsilon , \qquad \forall x \neigh x', \quad 
      \end{equation}
    \end{definition}
    \only<article>{
    \end{theoryblock}
  }
  \only<article>{
    More commonly, the definition of differential privacy uses the following inequality:
    \begin{equation}
      \pol(A \mid x) \leq e^\epsilon \pol(A \mid x').
      \label{eq:epsilon-dp-alt}
    \end{equation}
    In practice---and particularly either when $\CA$ is finite, or when $\pol(a | x)$ is a density on $\CA$---we can also use
    $\pol(a \mid x)$ without any technical difficulties in our mathematical expressions.}
    
  We used the inequality~\eqref{eq:epsilon-dp} in the definition,
  because it makes it easy to see that it is similar to standard
  notions of statistical divergences such as the KL divergence:
  $\sum_a \ln \frac{\pol(a \mid x)}{\pol(a \mid x')} \pol(a \mid x)$
  and the total variation $\max_A |\pol(A \mid x) - \pol(A \mid
  x')|$. Indeed, it is possible to define privacy in terms of a bound
  on these quantities, as for example in
  Exercise~\ref{exer:kl-dp}. However these definitions are weaker.


  Now let us go a bit deeper into the notion of neighbourhood for datasets. Changing the definition of neighbourhood results in a different meaning 
or privacy. There are two common definitions, and we will go through them in detail.
\end{frame}

\begin{frame}
  \frametitle{Neighbourhoods}

  \only<article>{
    Differential privacy guarantees that
    it is hard to distinguish neighbouring datasets. Hence, the
    definition of neighbourhood we use reflects what we want to
    protect. It makes sense to define neighbourhoods in terms of
    changes in one individual's data, because then an adversary cannot
    learn about the values of any particular individual.
    
    In this book we will use two definitions of neighbourhoods, which
    are standard in the differential privacy literature. The first is
    constructed so that the adversary cannot distinguish whether or
    not any particular individual's information \emph{is in the dataset}. If
    not, then they cannot infer the individual's presence from the
    output of the algorithm.

  }
  \begin{definition}[Membership neighbourhood]
    If two datasets $x,x'$ are neighbours, then we write $x \neigh x'$, and it holds that
    \[
      x = (x_1, \ldots, x_{i-1}, x_i, x_{i+1}, \ldots, x_n),
      \qquad
      x' = (x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n),
    \]
    for some $i$,  i.e. if one dataset is
    missing an element.
  \end{definition}
  \only<article>{ Under this neighbourhood relation, two datasets are
    neighbours if one contains the data of one individual, and the
    other does not, but they are otherwise the same. For an example, see Table~\ref{tab:data-x}. Then, if the
    algorithm is differentially private with respect to this
    neighbourhood, it is hard to tell if any single person's data has
    been used in the calculation.  This is an important concept if the
    membership in the dataset is itself sensitive, as we then want to
    protect against membership attacks. For example, if somebody is
    enlisted in study for experimental treatment of a rare disease,
    then the mere fact that they are part of the study is strong
    evidence that they have the disease.
    
    The second definition is slightly weaker. Here, two datasets are
    neighbours if they are identical, apart from the data of one
    individual, which is changed. It determines whether or not we can
    distinguish between different \emph{values} of individual data. If
    not, then they cannot infer whether the data submitted by the
    individual has a particular value.}
  \begin{definition}[Edit neighbourhood]
    We say that two datasets $x, x'$ are neighbours, and write $x \neigh_e x'$ if
    \[
      x = (x_1, \ldots,  x_i, \ldots, x_n),
      \qquad
      x' = (x_1, \ldots, x'_i, \ldots,  x_n),
      \qquad
      x_i \neq x'_i.
    \]
    i.e. if one dataset has an altered element.
  \end{definition}
  \only<article>{ If $x,x'$ are 1-neighbours under the second
    definition, then they are 2-neighbours under the first
    definition. To see this, create a new dataset $\hat{x}$ from $x$,
    without the data of person $i$. Then $x \neigh \hat{x}$ and
    $\hat{x} \neigh x'$, since we can change the data of one person by
    removing the original data $x_i$ and then re-inserting the altered
    data $x'_i$. This is illustrated in the example below.}
\end{frame}



\begin{frame}
  \begin{exampleblock}{Neihgbourhood example}
    \only<article>{
      In this example, we have three datasets, $x, x'$ and $\hat{x}$. In the second dataset, $\hat{x}$, the highlighted row in $x$ is missing. In the third dataset, $x'$, another row with the same name is added, but the height and weight are changed.
    }
    \begin{table}[H]
      \centering
      \begin{tabular}{l|l|l|l}
        Birthday & Name & Height  & Weight \\
        \hline
        06/07 & Li Pu & 190 & 80 \\
        06/14 & Sara Lee & 185 & 110  \\
        \alert<1>{06/12} & \alert<1>{John Smith} & \alert<1>{170} & \alert<1>{82} \\
        01/01 & A. B. Student & 170 & 70 \\
        05/08 & Li Yang & 175 & 72 
      \end{tabular}
      \caption{Data $x$}
      \label{tab:data-x}
    \end{table}
    \only<article>{
      In fact, $\hat{x}$ is obtained through a deletion from $x$. Of course, the operation can be reversed: $x$ can be obtained from an addition to $\hat{x}$.
    }
    \only<1>{
      \begin{table}[H]
        \centering
        \begin{tabular}{l|l|l|l}
          Birthday & Name & Height  & Weight \\
          \hline
          06/07 & Li Pu & 190 & 80 \\
          06/14 & Sara Lee & 185 & 110  \\
          01/01 & A. B. Student & 170 & 70 \\
          05/08 & Li Yang & 175 & 72 
        \end{tabular}
        \caption{$\hat{x}$, 1-Neighbour of $x$.}
        \label{tab:data-x-delete}
      \end{table}
    }
    \only<article>{We can now instead add another row to $\hat{x}$, which will be similar to the row we had removed. This will have the effect of altering the original data in $x$.}
    \only<2>{
      \begin{table}[H]
        \centering
        \begin{tabular}{l|l|l|l}
          Birthday & Name & Height  & Weight \\
          \hline
          06/07 & Li Pu & 190 & 80 \\
          06/14 & Sara Lee & 185 & 110  \\
          06/12 & John Smith & \alert{180} & \alert{80} \\
          01/01 & A. B. Student & 170 & 70 \\
          05/08 & Li Yang & 175 & 72 
        \end{tabular}
        \caption{$x'$, 2-Neighbour of $x$.}
        \label{tab:data-x-edit}
      \end{table}
      \only<article>{Since $x,x'$ only differ in the contents of a single row, they are edit-neighbours.}
    }
  \end{exampleblock}

\end{frame}
Since two datasets $x, x'$ that are neighbours in the edit neighbourhood are actually 2-neighbours in the membership neighbourhood, this means that the former definition is weaker. It seems like that the 
In particular, it is easy to show that a guarantee of $\epsilon$-DP under the membership neighbourhood implies a $2 \epsilon$-DP guarantee under the edit neighbourhood.  The converse relationship does not hold. 
\begin{frame}
  \frametitle{The privacy of randomised response.}
  \only<article>{ As we hinted at earlier, the randomised response
    mechanism satisfies $\epsilon$-DP.  The $\epsilon$ parameter is
    dependent on $p$, with values closer to $1/2$ giving a smaller $\epsilon$,
    and thus better privacy protection. This is because when $p = 1/2$, the responses are completely random, and when $p = 0$, the responses are always equal to the data. }
  \begin{remark}
    The randomised response mechanism with $p \leq 1/2$ is
    $(\ln \frac{1 - p}{p})$-DP with respect to the edit neighbourhood
    $\neigh_e$.
  \end{remark}
  \begin{proof}
    Consider $x = (x_1, \ldots, x_j,  \ldots, x_n)$, $x' = (x_1, \ldots, x'_j,  \ldots, x_n)$. Then
    \begin{align*}
      \pol(a \mid x)
      \uncover<2->{&= \prod_i \pol(a_i \mid x_i)}
                     \uncover<3->{\\ &= \pol(a_j \mid x_j) \prod_{i \neq j} \pol(a_i \mid x_i) }
                                       \uncover<4->{\\ &\leq \frac{1-p}{p} \pol(a_j \mid x'_j) \prod_{i \neq j} \pol(a_i \mid x_i) }
                                                         \uncover<5>{\\ &= \frac{1-p}{p} \pol(a \mid x')}.
    \end{align*}
    \only<article>{where the first equality comes from the independence of responses in the definition of $\pol(a \mid x)$. The second equality is simply a re-arrangement. For the inequality, note that    $\pol(a_j \mid x_j)$ has two possible values, depending on whether or not $a_j = k$ or $a_j \neq k$ when $x_j = k$. For the case $\pol(a_j \neq k \mid x_j = k) = p$, we have necessarily that $x'_j \neq k$, because $x_j \neq x'_j$, so $\pol(a_j \neq k \mid x'_j \neq k) = 1 - p$. Similarly, we have}
    \only<4>{$\pol(a_j = k\mid x_j = k) = 1 - p$ so the ratio $\pol(a_j \mid x'_j) / \pol(a_j \mid x'_j) \leq \max\{(1-p)/p, p/(1 - p)\} \leq (1 - p)/p$ for $p \leq 1/2$.}
    \only<article>{The last equality comes simply from the independence of responses in the definition of $\pol(a \mid x')$.}
  \end{proof}

  \only<article>{
  \begin{groupactivity}{Moving to a new neighbourhood}
    Is the randomised-response mechanism differentially
    private with respect to the membership-neighbourhood definition? If
    not, is it possible to modify it in order to satisfy that privacy
    definition?  \emph{Hint: The mechanism must be able to hide the
      participation of a single individual in the database.}
  \end{groupactivity}
  }
  % Given this hint, the solution is obvious: The mechanism must
  % contain a non-identifying information aggregator. Let us take an
  % example, where individual responses are {0,1}. We can also add X,
  % as 'declined to respond'. Then the analyst will (by default) know
  % who has participated. Note that
  % $ln |p(a | x) / p(a|x')| < \epsilon$
  % requires that values of 0,1,X have non-zero probability for
  % everybody.  This is tricky, as typically the number of X will
  % depend on how many people do not participate, and the size of a
  % will be equal to the number of potential responders.


  
  \only<article>{Finally, it may be convenient to the look at
    neighbourhoods in terms of a distance between datasets. Let
    $\Naturals^{|\CX|}$ be the set of all possible dataset histograms,
    i.e. counts of different possible rows in each dataset. Then the
    distance between two datasets is simply the total difference in
    counts:
    \begin{definition}[Hamming distance between datasets]
      Consider two datasets, $x, x'$ with $n$ and $n'$ elements
      respectively. Then, we define their Hamming distance as:
      \begin{align}
        \|x - x'\|_1
        &= \sum_j |\sum_{i=1}^n \ind{x_i =j} - \sum_{i=1}^{n'} \ind{x_i =j}|
          = \sum_{j \in \CX}|n_j(x) - n_j(x')|,
      \end{align}
      where $n_j(x)$ is the number of elements in $x$ equal to $j$.
    \end{definition}
    Let us see how the Hamming distance relates to the two
    neighbourhoods we defined. In particular, $\|x - x'\|_1 = 1$
    if and only if $x \neigh x'$.  To see this, notice that if $x'$
    has one row less than $x$, but is otherwise identical, then
    there is exactly one value $j$ for which
    $n_j(x) = n_j(x') + 1$, with $n_j(x) = n_j(x')$ otherwise.
    Consequently, $\|x - x'\|_1 = 1$. The reverse direction
    follows by contradiction: if the Hamming distance is one, then the
    two databases must differ in at least one element. If they differ
    in more, then their distance has to be larger than one.
    
    On the other hand, for the second neighbourhood definition, things
    are slightly different. There, we assume that $x_i \neq x'_i$ for
    one $i$. Without loss of generality, let us say that $x_i = j$ and
    $x'_i = k$, with $j \neq k$. Then $n_j(x) = n_j(x') + 1$ and
    $n_k(x') = n_k(x) + 1$. Consequently $\|x - x'\|_1 = 2$.
  }
\end{frame}


\section{The local and central privacy models}
\label{sec:local-central-dp}
\only<article>{ So far, we have only seen the concept of differential
  privacy applied as a method to generate a privatised version of a
  dataset. This can then be used to perform arbitrary
  computations. Typically, however, we want to perform some specific
  calculations on the data. Is it possible to compute things privately
  using the original data, but so that the result of the computation
  does not leak any information? Clearly this should depend somehow on
  the computation you wish to perform. And secondly, and most
  importantly, can we obtain the same privacy guarantee but with
  better utility?

  For concreteness, let us assume that you have defined a function
  $f : \CX \to \CY$, which you wish to compute on arbitrary data
  $x \in \CX$. If you wish to preserve privacy, however, you need the
  computation to satisfy some formal guarantee like differential
  privacy. The simplest way to do this is by simply generating a
  dataset $a$ with a DP algorithm and then processing the data with
  the function we have already specified. This corresponds to the
  local privacy model.}


\begin{frame}
  \begin{block}{The local privacy model.}
    \only<presentation>{
      \begin{itemize}
      \item Individuals generate $a_i$ from $x_i$ using a DP policy $\pol(a_i | x_i)$.
      \item The analyst calculates $f(\ba)$.
      \end{itemize}
    }
    \only<article>{Given a function $f : \CX \to \CY$, and a set of
      individual data $x = (x_i)_{i=1}^n$, use a differentially
      private algorithm $\pol : \CX \to \Simplex(\CX)$, defining the conditional distribution $\pol(a | x) = \prod_i \pol_i(a_i \mid x_i)$, to generate $a = (a_i)_{i=1}^n$ with $a_i \in
      \CX$. Then output $f(a)$.}
  \end{block}

  \only<article>{ As we will show in the following chapter, the
    composition of $f$ and $\pol$ is differentially private, because
    $\pol$ is differentially private, and $f$ amounts to just post-processing the output $a$
    
    The advantage of the local model is that the
    calculation $f$ does not need to be modified. The individuals do
    not have to trust anybody, as they can modify their data
    locally. However, they must take care that the DP calculation is
    performed correctly.  }

  \only<article>{ Typically, in the local privacy model, the $i$-th
    individual's data $x_i$ is used to generate a private response
    $a_i$. This means that individual data is only available to a
    local private mechanism. This model allows us to publish a
    complete dataset of private responses.  In the central privacy
    model, however, the data $x$ is collected and the result $a$ is published
    by a \alert{trusted curator.}}
  
  \begin{block}{The central privacy model.}
    \only<presentation>{
      \begin{itemize}
      \item A \alert{trusted analyst} obtains $x$.
      \item She designs a DP policy $\pol(\ba \mid x)$ so that $\ba \approx f(x)$.
      \end{itemize}
    }
    \only<article>{Given a function $f : \CX \to \CY$, a trusted curator obtains the data $x$ of all individuals and selects a DP algorithm $\pol : \CX \to \Simplex(\CY)$, defining the conditional distribution  $\pol(a \mid x)$, to generate  the output $a$, so that $a \approx f(x)$.}
  \end{block}
  \only<article>{
    In the above definition we do not say anything about how $\ba$ approximates $f(x)$. Ideally, we would like it to be unbiased, i.e. so that $\E_\pi[\ba \mid x] = f(x)$. The accuracy can for example be measured with respect to the variance of $\ba - f(x)$.

    The main advantage of the central model is that
    approximating $f$ with a differentially-private version can be
    much more accurate than simply using the original function with
    noisy data. We have already seen this in the binary randomised
    response mechanism, where the ratio of positive responses in the
    noisy data is always at least $p$, even if the true positive ratio
    is zero. However, obtaining a differentially private approximation
    of an arbitrary function is not always easy.

    The dependency diagrams in Figures~\ref{fig:privacy-diagrams} shows
    how the output depends on the individual data, the non-private and
    private algorithm. In the first case, everybody has access to $x, y, f$. In the second case, only the local mechanism has access to $x$, but $a, \pi, f, y$ are public. In the third case, the analyst has access to $x$, and $\pi_f, a$ are public.
  }
  
  \begin{figure}[H]
    \begin{subfigure}{0.3\textwidth}
      \centering
      \begin{tikzpicture}
        \node[RV] at (0,0) (x1) {$x_1$};
        \node[RV] at (0,1) (x2) {$x_2$};
        \node[RV] at (0,2) (xn) {$x_n$};
        \node[RV] at (2,1) (y) {$y$};
        \node[select] at (1,-1) (pol) {$f$};
        \draw[->] (x1) -- (y);
        \draw[->] (x2) -- (y);
        \draw[->] (xn) -- (y);
        \draw[->] (pol) -- (y);
      \end{tikzpicture}
      \caption{Non-private calculation}
      \label{fig:non-private}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
      \centering
      \begin{tikzpicture}
        \node[hidden, RV] at (0,0) (x1) {$x_1$};
        \node[hidden, RV] at (0,1) (x2) {$x_2$};
        \node[hidden, RV] at (0,2) (xn) {$x_n$};
        \node[RV] at (2,0) (a1) {$a_1$};
        \node[RV] at (2,1) (a2) {$a_2$};
        \node[RV] at (2,2) (an) {$a_n$};
        \draw[->] (x1) -- (a1);
        \draw[->] (x2) -- (a2);
        \draw[->] (xn) -- (an);
        \node[select] at (2,-1) (f) {$f$};
        \node[select] at (1,-1) (p) {$\pol$};
        \node[RV] at (3,1) (y) {$y$};
        \draw[->] (a1) -- (y);
        \draw[->] (a2) -- (y);
        \draw[->] (an) -- (y);
        \draw[->] (f) -- (y);
        \draw[->] (p) -- (a1);
        \draw[->] (p) -- (a2);
        \draw[->] (p) -- (an);
      \end{tikzpicture}
      \caption{Local privacy}
      \label{fig:local-privacy}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
      \centering
      \begin{tikzpicture}
        \node[analyst, RV] at (0,0) (x1) {$x_1$};
        \node[analyst, RV] at (0,1) (x2) {$x_2$};
        \node[analyst, RV] at (0,2) (xn) {$x_n$};
        \node[RV] at (2,1) (a) {$a$};
        \node[select] at (1,-1) (pol) {$\pol_f$};
        \draw[->] (x1) -- (a);
        \draw[->] (x2) -- (a);
        \draw[->] (xn) -- (a);
        \draw[->] (pol) -- (a);
      \end{tikzpicture}
      \caption{Central privacy}
      \label{fig:central-privacy}
    \end{subfigure}
    \caption{Non-privacy, local privacy, and central privacy models. In the first case, the value of $y$ depends directly on the function $f$ to be calculated, as well as the data $x$. In the local privacy model, noise is added to the individual data with a differentially private algorithm $\pol$, and the function is calculated on the output. In the central model, a stochastic version $\pol_f$ of the original function $f$ is calculated on the data. The variables that are only privately known, denoted with dashed lines. The variables that are only seen by the analyst, are denoted with dotted lines.}
    \label{fig:privacy-diagrams}
  \end{figure}

\only<article>{
  \begin{groupactivity}{Trust models.}
    Google uses a local privacy model to collect data from Android
    phone users. How does this compare to Google gathering data in the
    clear and then performing private computations? Who do the users
    have to trust? What are the privacy risks in either case?
  \end{groupactivity}
  }
\end{frame}


\section{Properties of differential privacy.}
\begin{frame}

  
  \only<article>{
    Differential privacy has a number of interesting properties. Firstly, any differentially private algorithm must be stochastic, because deterministic algorithms will enable an adversary to distinguish between some pairs of datasets with absolute certainty. Secondly, if we combine multiple differentially private operations sequentially, i.e. by computing many functions on the same data, then the total privacy loss is the sum of privacy losses of each operation. Thirdly, if we compute multiple private operations on different subsets of the data, each subset suffers its own individual loss. Finally, we can use the output of the differentially private algorithm in any way we like, and this does not impact privacy. We will know give a more formal description of these properties.

\subsection{Randomness}  
    Let us start with some intuition. Consider the example of counting how many people take drugs in a competition. We can perform the counting via secret ballot, so that in the end we only know the total number of people that take and do not take drugs. However, side information can be very powerful here.  If an adversary does not know your drug status, and wishes to discover it, clearly the counts are not enough by themselves. However, if he knows how many people \emph{apart from you} take drugs, it's trivial to discover your own drug habits by looking at the total. This is because in this case, $f(x) = \sum_i x_i$ and the adversary knows $\sum_{i \neq j} x_i$. Then, by observing $f(x)$, he can recover $x_j = f(x) - \sum_{i \neq j} x_i$. Consequently, it is not possible to protect against adversaries with arbitrary side information in this way. However, randomising the calculation ensures that the adversary can never be perfectly sure of what any individual's data is, no matter what side information he has. In fact, as we will show below, randomness is essential for differential privacy. If an algorithm is differentially private, then it either has to be stochastic, or it has to always return the same answer $a$, no matter what the data is.
\begin{remark}
      Any differentially private algorithm must either be stochastic, or deterministic with a constant output.
    \end{remark}
    \begin{proof}
    The definition of differential privacy requires a bounded ratio of probabilities for any neighbouring datasets. Imagine our algorithm deterministically outputs $a$ for some input $x$ so that $\pol(a|x) = 1$, while for all other $a'$, $\pol(a' | x) = 0$. Let us say that for some $a'$ and a neighbouring $x'$, we have $\pol(a' | x') > 0$. By the DP definition, however $\pol(a' | x') \leq \pol(a' | x) e^{\epsilon} = 0$, which is a contradiction. Consequently, we have shown that if, for some $x$, there is $a'$ with zero probability under $\pol$, then it must have zero probability for all its neighbours. Consequently, the neighbours of its neighbours must also assign zero probability to it. Hence, the algorithm must have a constant output $a$.

    Conversely, if the algorithm assigns a positive probability to some $a$, then all neighbours must assign a positive probability. By induction, all $x \in \CX$ must result in a positive probability $a$. In other words, if $\pol(a | x) > 0$ for some $x,a$ then $\pol(a | x') > 0$ for all $x'$. If more than one $a$ have a positive probability, then the algorithm is necessarily stochastic, otherwise it is a constant deterministic function.
    \end{proof}
  }
\end{frame}

\only<article>{ Randomness is also necessary in cryptography.  In that
  setting, Alice wants to communicate a message $x$ to Bob.  In a
  secret key cryptographic scheme, information theoretic security is
  achieved by making sure that the encrypted message $a$ comes from a
  uniform distribution $\pol(a | x)$. This is achieved by randomly
  selecting a key $\omega$ and then combining it with $x$ to obtain $a$. Without randomness, the cyphertext eventually becomes predictable. In practice, of course, we use high-quality pseudo-random number generators, with some additional random source for seeding them. }


\begin{frame}


\subsection{Composition}  
\only<article>{
  In practice, we do not actually perform a single calculation from our data $x$. Even simple data visualisations are a form of calculation on the data, and every calculation might leak privacy. Training or hyperparameter tuning for a machine learning model also involves performing some calculation on the data in some way. Finally, if we wish to provide private access to our database so third parties can analyse the data, then every analysis performed is a separate calculation. How can we calculate the total privacy loss after a long sequence of data manipulations? 

  For the purposes of this chapter, we will call every calculation on the data a \emph{query.}
  Depending on how exactly we employ our DP scheme, each query answered may leak privacy.
  We begin by the idea of sequential composition, whereby we perform a \emph{sequence} of queries $\pol_1, \pol_2, \ldots$ to the database. 
}

  \only<presentation>{
    \begin{block}{Properties of $\epsilon$-DP algorithms}
      \begin{itemize}
      \item Composition: If $\pol_1, \pol_2$ are $\epsilon_1, \epsilon_2$-DP respectively, their composition is $(\epsilon_1 + \epsilon_2)$-DP.
      \item Post-processing: Any transformation of the output of an $\epsilon$-DP algorithm does not induce further privacy loss.
      \end{itemize}
    \end{block}
  }

  \begin{definition}[$n$-fold composition]
    In this privacy model, we compose a mechanism $\pol$ responding with $a = (a_1, \ldots, a_n)$ out of $T$ differential private mechanisms $\pol_1, \ldots, \pol_n$, each mechanism $\pol_i$ responding with $a_t$, to create the composed mechanism:
    \[
      \pol(a \mid x) = \prod_{i=1}^n \pol_i(a_i \mid x).
    \]
    This composition is fixed ahead of time, in the sense that $\pol_t$ is selected independently of what $a_1, \ldots, a_{t-1}$ have been.
    % The composition is \alert{adaptive}, in the sense that the next query is allowed to depend on the previous queries and their results.
  \end{definition}
  \only<article>{As an example, consider calculating $a_1$, the sum of $x_{t,i}$ as well as the total number of records $a_2$, using an $\epsilon$-DP mechanism $\pol_1, \pol_2$ in each case. The following theorem gives us the total privacy loss if we make $a_1, a_2$ public.
  }
  \begin{theorem}
    For any $\epsilon > 0$, the class of $\epsilon$-differentially private mechanism satisfy $n \epsilon$-differential privacy under $n$-fold composition. More generally, if we have $n$ mechanisms, and each mechanism is $\epsilon_i$-DP, with mechanism $\pol_i$ releasing $a_i \sim \pol_i(a_i \mid x)$, the composed mechanism, releasing $(a_1, \ldots, a_n)$ with probability $\pol(a_1, \ldots, a_n | x) = \prod_{i=1}^n \pol_i(a_i \mid x)$  is $\sum_{i=1}^n \epsilon_i$-DP.
  \end{theorem}
  \begin{proof}
    We give the proof for 2-fold composition.
    \begin{align*}
      \frac{\pol(a_1, a_2 | x)}{\pol(a_1, a_2 | x')}
      &=
        \frac{\pol(a_1 | x) \pol(a_2 | x)}{\pol(a_1 | x') \pol(a_2 | x')}
      \leq
        \frac{\pol(a_1 | x') e^{\epsilon_1} \pol(a_2 | x') e^{\epsilon_2}}{\pol(a_1 | x') \pol(a_2 | x')}
      =
        e^{\epsilon_1 + \epsilon_2}.
    \end{align*}
    Since this holds for two mechanisms, $n$-fold composition follows by induction.
   \end{proof}
  \only<article>{
    Continuing the counting and summation example, if we have a mechanism composed of one application of $\epsilon$-DP summation and one application of $\epsilon$-DP counting, then the total output is $2 \epsilon$-DP. 
    
    On the other hand, if we perform multiple queries, but each query is applied to a completely different part of the data, then we do not lose any more privacy. This can apply to asking about a different subset of the records. This notion is formalised below.}
  \begin{theorem}[Parallel composition]
    If we apply a set of $\epsilon$-DP mechanisms $\pi_k$ to disjoint subsets of the data so that $\pi_k$ is applied to a subset $D_k$ so that $D_j \cap D_k  = \emptyset$ for all $j \neq k$, then we are performing a parallel composition. This composition is $\epsilon$-DP.
  \end{theorem}
  \begin{proof}
    Consider two neighbouring datasets $x, x'$, with $x'$ containing an additional record $x_{T+1}$.
    Let $D_1, D_2$ be a partition of $x'$, with $x_{T+1} \in D_2$ and the corresponding partition of $x'$ being $D_1, D_2 \cap x$. Then
    \[
      \frac{\pol(a | x)}{\pol(a | x')}
      =
      \frac{\pol(a_1 | D_1) \pol(a_2 \mid D_2 \cap x)}{\pol(a_1 | D_1) \pol(a_2 \mid D_2)}
      \leq
      e^\epsilon.
    \]
  \end{proof}
  \only<article>{
    Finally, going back to our example, let us say we wish to release the mean using the DP-calculated sum $a_1$ and count $a_2$, i.e. release $a_1 / a_2$. Does that lead to any more privacy loss? According to the post-processing theorem below, any transformation of the outputs of a DP mechanism does not result in any more privacy loss. This is inherently because post-processing \emph{does not involve access to the data}.
  }
  \begin{theorem}[Post-processing]
    Let mechanism $\pol(a \mid x)$ be $\epsilon$-DP. Applying any transformation $f : A \to Y$ to the output of the mechanism to obtain $y = f(a)$, results in another $\epsilon$-DP mechanism.
  \end{theorem}
  \begin{proof}
    To do this proof properly we need to use the measure-theoretic definition of conditional probabilities. Applying $f$ to $a$ results in a new random variable $y$ with $x$-conditional measure
    \[
      \pol_f(Y | x) \defn \pol\left(\cset{a}{f(a) \in Y} ~\middle|~ x \right),
    \]
    where we assume that $f$ is measurable with respect to $\pol( \cdot | x)$ for all $x$.
    We need to show that this is differentially private. Let $A_Y \defn \cset{a}{f(a) \in Y}$ be the set of $a$ values that result in $f(a) \in Y$. Then we can simplify the above to
    \[
      \pol_f(Y | x) \defn \pol(A_Y | x).
    \]
    From this follows that
    \[
      \frac{\pol_f(Y | x)}{\pol_f(Y | x')}
      = \frac{\pol(A_Y | x)}{\pol(A_Y | x')}
      \leq e^\epsilon,
    \]
    as the DP property holds for all subsets of $\CA$.

    It should be noted here that this theorem is essentially a version of the data-processing inequality.
  \end{proof}
  \only<presentation>{
    \begin{alertblock}{Composition}
      If we answer $T$ queries with an $\epsilon$-DP mechanism, then our cumulative privacy loss is $\epsilon T$.
    \end{alertblock}
  }
  \only<article>{
    These theorem are very useful tools, as they allows us to create new mechanisms that are composed of simpler parts. As a first example, we can look at how we can use sequential copmosition, to create a randomised response algorithm when the respondents provide multiple attributes.}
  \begin{exampleblock}{Randomised response for multiple attributes.}
    \only<article>{Up to now we have been discussing the case where each individual only has one attribute. However, in general each individual $t$ contributes multiple data $x_{t.i}$, which can be considered as a row $x_t$ in a database. Then the mechanism can release each $a_{t,i}$ independently.}
    
    For $n$ users and $k$ attributes, if the release of each attribute $i$ is $\epsilon$-DP then 
    the data release is $k \epsilon$-DP. Thus to get $\epsilon$-DP overall,  we need $\epsilon / k$-DP per attribute.
    
    \only<article>{The result follows immediately from the composition theorem. We can see each attribute release as the result of an individual query. More generally, if each attribute $i$ is released with an $\epsilon_i$-DP mechanism, the overall mechanism is $\sum_i \epsilon_i$-DP.}
  \end{exampleblock}


\end{frame}


\begin{frame}

  \only<article>{
    \begin{exerciseblock}{Differential privacy from a Bayesian viewpoint.}
    }
    \begin{example}
     \only<article>{Bayesian inference offers a simple way to explain the meaning of differential privaacy. Intuitively, using the Bayesian formalism, we can show that, no matter what prior knowledge the adversary has, they cannot infer a lot from the private release. We are specifically interested in adversary knowledge about the dataset, before and after the mechanism's release. In particular, assume that the adversary knows that the data is either $x$ or $x'$. For concreteness, assume the data is either }
    \[
      x = (x_1, \ldots, x_j = 0, \ldots,  x_n)
    \]
    \only<article>{where $x_i$ is the data of person $i$, or the alternative dataset:}
    \[
      x' = (x_1, \ldots, x'_j=1, \ldots, x_n).
    \]
    \only<article>{In other words, the adversary knows the data of all people apart from one, the $j$-th person. They only need to dinstiguish one possible value from another. Without loss of generality, we can model the adversary as having some arbitrary prior belief}
    \[
      \bel(x) = 1 - \bel(x')
    \]
    \only<article>{for the two cases. Assume the adversary knows
      the output $a$ of a mechanism $\pol$.}
    \only<presentation>{
      \onslide<2->{
        \[
          a_t, \qquad \pol(a_t \mid x_t) \Rightarrow
          \begin{cases}
            \pol(a_t \mid x_t = x)\\
            \pol(a_t \mid x_t = x')
          \end{cases}
        \]
      }
    }
    What can we say about the posterior distribution of the adversary $\bel(x \mid a, \pol)$ after having seen the output, if $\pol$ is $\epsilon$-DP? How does it depend on $\epsilon$?
    \label{ex:Bayesian-DP}
    \end{example}
    \only<article>{
      \begin{proof}[Solution]
        We can write the adversary posterior as follows.
        \begin{align}
          \Pr^\pol_\bel(x \mid a)
          &=
            \frac{\pol(a  \mid x) \bel(x)}
            {\pol(a  \mid x) \bel(x) + \pol(a  \mid x') \bel(x')}
          \\
          &\geq
            \frac{\pol(a  \mid x) \bel(x)}
            {\pol(a  \mid x) \bel(x) + \pol(a  \mid x) e^\epsilon \bel(x')} \tag{from DP definition}
          \\
          &=
            \frac{\bel(x)}
            {\bel(x) +  e^\epsilon \bel(x')}.
        \end{align}
        Note that $\pol(a \mid x) \leq e^\epsilon \pol(a \mid x')$ and conversely $\pol(a \mid x') \leq e^\epsilon \pol(a \mid x)$.
        We can also then bound the quantity from above:
        \[
          \Pr^\pol_\bel(x \mid a) \leq  \frac{\bel(x)} {\bel(x) +  e^{-\epsilon} \bel(x')}.
        \]
        Consequently, $\lim_{\epsilon \to 0} \Pr^\pol_\bel(x \mid a) = \beta(x)$, hence the information gained by the adversary is bounded by the prior and the information loss $\epsilon$.
      \end{proof}
    }
    \only<article>{
    \end{exerciseblock}
  }
  
\end{frame}


\only<article>{
  Now that we have seen the rudiments of differential privacy, and how local and central differential privacy operate, it is time to introduce another mechanism, the Laplace mechanism. This mechanism can be applied at either the local level, to add noise to individual responses, or at the central level, to add noise to a function we wish to compute. In any case, the noise must be appropriately tuned to ensure the correct level of privacy.
  
  }


\section{The Laplace mechanism}
\label{sec:laplace-mechanism}
\begin{frame}
  \only<article>{ Many times we already have a function
    $f : \CX \to \Reals$ we want to calculate on data, and we would
    like to make the function preserve privacy. This clearly falls
    within the \emph{central privacy} model: The analyst has access to
    the data and function, and wishes for the algorithm generating the
    final output to be private.  One solution, that can satisfy
    differential privacy, is to add noise to the function's output. We
    start by first calculating the value of the function for the data
    we have, $f(x)$, and then we add some random noise $\omega$, hence
    our calculation is random:
    \[
      a = f(x) + \omega.
    \]
    The amount and type of noise added, together with the smoothness of the function $f$, determine the amount of privacy we have. One of the simplest noise-adding mechanisms is to add zero-mean Laplace noise. Then we write $\omega \sim \Laplace(\lambda)$.\footnote{When unspecified, the mean parameter is assumed to be zero.} The mechanism is defined below.
  }
  \begin{definition}[The Laplace mechanism]
    For any function $f : \CX \to \Reals$, the output $a$ of the mechanism is sampled from a Laplace distribution with mean $f(x)$ and scaling parameter $\lambda$, i.e.
    \begin{equation}
      \label{eq:laplace-mechanism}
      \pol(a \mid x) = \Laplace(f(x), \lambda).
    \end{equation}
    The probability density function of the Laplace distribution with mean $\mu$ and scaling $\lambda$ is given by:
    \[
      p(\omega \mid \mu, \lambda) = \frac{1}{2 \lambda} \exp\left(-\frac{|\omega - \mu|}{\lambda}\right).
    \]
    and has mean $\mu$ and variance $2 \lambda^2$.
  \end{definition}
\end{frame}

  \only<article>{In the central privacy model, the non-private calculation directly outputs $y = f(x)$. We can approximate this with a DP calculation with distribution $\pol(a \mid x)$. The Laplace mechanism does so by first calculating $y = f(x)$ and then generating $\pol(a \mid y)$ so that $\E_\pol[a \mid x] = f(x)$.}


  \only<article>{Let us now talk about how the Laplace mechanism that can be used both in the central and local model when $\CX \subset \Reals^n$ and $\CY \subset \Reals$. In particular, we will go through an example where we apply Laplace noise to real-valued data.}


\begin{frame}
  \begin{exampleblock}{The Laplace mechanism for averages}
    \only<article>{Here we have $n$ individuals for which we wish to calculate the average salary. The salary of each individual is a positive integer, but it we can consider them to be a real number for simplicity.}
    \begin{itemize}
    \item The $i$-th person receives salary $x_i$.
    \item We wish to calculate the average salary $\sum_{i=1}^n x_i / n$ in a private manner. \only<article>{That is, we want the calculation to be differentially private.}
    \end{itemize}
    \only<article>{We can do this in two ways. By using a DP mechanism on each individual salary and then calculating the average, or by first calculating the average and then applying a DP mechanism to the result. In particular, we can add try adding Laplace noise to either each person's salary (local model) or to the final average salary (central model).}

    \textbf{The local model.}
    \only<article>{In this case, $\pol(a \mid x)$ is obtained by independent Laplace noise $\omega_i$ for each individual. This gives us an intermediate calculation, $y_i$, which is then averaged to obtained the output.}
    \begin{itemize}
    \item Obtain $y_i = x_i + \omega_i$, where $\omega_i \sim \Laplace(\lambda)$.
    \item Return $a = n^{-1} \sum_{i=1}^n y_i$.
    \end{itemize}

    \textbf{The central model.}
    \only<article>{In this case, $\pol(a \mid x)$ is obtained by averaging first and adding noise later.}
    \begin{itemize}
    \item Calcualte $y = n^{-1} \sum_{i=1}^n x_i$.
    \item Return $a = y + \omega$, where $\omega \sim \Laplace(\lambda')$.
    \end{itemize}
    \only<article>{We must tune $\lambda, \lambda'$ appropriately in to obtain the needed $\epsilon$-DP guarantee. This will be detailed in the next section.}
  \end{exampleblock}


  \begin{figure}
    \centering
    \begin{tikzpicture}
      \node[analyst, RV] at (0,0) (x1) {$x_1$};
      \node[analyst, RV] at (0,1) (x2) {$x_2$};
      \node[analyst, RV] at (0,2) (xn) {$x_n$};
      \node[analyst, RV] at (2,1) (y) {$y$};
      \node[select] at (1,-1) (f) {$f$};
      \node[select] at (3,-1) (pol) {$\pol$};
      \node[RV] at (4,1) (a) {$a$};
      \draw[->] (f) -- (y);
      \draw[->] (x1) -- (y);
      \draw[->] (x2) -- (y);
      \draw[->] (xn) -- (y);
      \draw[->] (y) -- (a);
      \draw[->] (pol) -- (a);
    \end{tikzpicture}
    \caption{Laplace mechanism. In this mechanism the analyst observes all data, $x$, generates a private calculation $y = f(x)$, and then generates a public output $a$ using a differentially private algorithm $\pol$.}
  \end{figure}
  

  
\end{frame}

\subsection{Sensitivity and the Laplace mechanism}
  \only<article>{To use the Laplace mechanism, either in the local or in the central setting, we must relate it to a specific function $f$ that we wish to compute. In particular, the mecnahism works by adding noise to the output of the function $f$ in a carefully calibrated manner so that we achieve exactly $\epsilon$-differential privacy. This is because, if the function's output changes a lot when we change a datapoint, it is easier to detect the change unless we add proportionally more noise. Below we will make this setting more precise.}

\begin{frame}
%  \frametitle{DP properties of the Laplace mechanism}
  \begin{definition}[Sensitivity]
    The sensitivity of a function $f : \CX \to \Reals^n$ is
    \[
      \sensitivity{f} \defn \sup_{x \neigh x'} \|f(x) - f(x')\|_1
    \]
    \only<article>{
      If we define a metric $d$, so that $d(x, x') = 1$ for $x \neigh x'$, and $d(x,x) = 0$ then:
      \[
        \|f(x) - f(x')\|_1 \leq \sensitivity{f} d(x, x'),
      \]
      i.e. $f$ is $\sensitivity{f}$-Lipschitz with respect to $d$.
    }
  \end{definition}
  \only<article>{
    In the above, we have defined sensitivity with respect to the $L_1$ norm. While alternative definitions are possible, this is the one that is the most convenient with respect to protecting individual privacy. This is because it captures the total change in all the components $f_i$ of the function $f$.

    Below are two examples. In the first one, the function only has a single one-dimensional input, and it is simply clamping the input onto the interval $[0,B]$. Hence its sensitivity is at most $B$.
    }
  \begin{example}
    If $f: \CX \to [0, B]$, e.g. $\CX = \Reals$ and $f(x) = \min\{B, \max\{0, x\}\}$, then
    \onslide<2->{
      $\sensitivity{f} = B$.
    }
    \only<article>{
      To prove this, we only need the fact that $f$ has a bounded range. Then for any $x, x'$ we know that $f(x) \leq B$ and $f(x') \geq 0$. Hence $f(x) - f(x') \leq B$. By symmetry, $|f(x) - f(x')| \leq B$. Hence the sensitivity is $B$.
    }
  \end{example}
  \only<article>{The next example simply takes the average of a number of points, each one bounded in the interval $[0,B]$. This makes its sensitivity bounded inversely proportionally to the number of points. This is because any single point can change the output of the function by at most $B/n$.}
  \onslide<3->{
    \begin{example}
      If $f: [0,B]^n \to [0,B]$ is
      $f = \frac{1}{n} \sum_{t=1}^n x_t$,
      then
      \onslide<4->{
        $\sensitivity{f} = B/n$.
      }
    \end{example}
    \only<article>{
      \begin{proof}
        The proof is simple, but instructive. We first give the proof
        for the edit neighbourhood.  Consider two neighbouring
        datasets $x, x'$ differing in example $j$. Then
        \[
          f(x) - f(x')
          = \frac{1}{n}\sum_{i=1}^n f(x_i) - f(x'_i)
          = \frac{1}{n}\left[f(x_j) - f(x'_j)\right]
          \leq \frac{1}{n}\left[B - 0\right].
        \]
        Now consider the case where $x'$ has one more data point than $x$. Then
        \[
          f(x) - f(x')
          = \left[
            \frac{1}{n} - \frac{1}{n+1}
          \right] \sum_{i=1}^n f(x_i)
          - \frac{1}{n+1} f(x_{n+1})
          =  \frac{1}{n(n+1)}
          \sum_{i=1}^n f(x_i)
          - \frac{1}{n+1} f(x_{n+1}).
        \]
        Both terms are bounded by $B / (n+1)$. Conversely, if we consider the neighbour $x'$ where one point is missing, the term is $B/n$. 
      \end{proof}
    }
  }
\end{frame}

\begin{frame}
  \begin{theorem}
    The Laplace mechanism on a function $f$ with sensitivity $\sensitivity{f}$, ran with $\Laplace(\lambda)$ is $\sensitivity{f} / \lambda$-DP. Consequently, if the Laplace mechanism is ran with $\lambda = \sensitivity{f} / \epsilon$, then it is $\epsilon$-DP. 
  \end{theorem}
  \begin{proof}
    \begin{align*}
      \frac{\pol(a \mid x)}{\pol(a \mid x')}
      &=
        \frac{e^{|a - f(x')|/\lambda}}{e^{|a - f(x)|/\lambda}}
        \leq
        \frac{e^{|a - f(x)|/\lambda + \sensitivity{f}/\lambda}}{e^{|a - f(x)|/\lambda}}
        = e^{\sensitivity{f} / \lambda}
    \end{align*}
    \only<article>{
      We first write the proof for a real-valued function $f$. 
      The first step follows from the definition of the Laplace mechanism. The inequality follows from the fact that for  $x \neigh x'$, we have $|f(x) - f(x')| \leq \sensitivity{f}$. In particular, 
      \begin{enumerate}[(a)]
      \item If $f(x') - a \geq 0$ then
        \[|f(x') - a| = f(x') - a  \leq \sensitivity{f} + f(x) - a \leq L + |a - f(x)|,\]
        as $f(x') \leq f(x) + \sensitivity{f}$ from the Lipschitz property.
      \item If $f(x') - a < 0$ then
        \[
          |f(x') - a| = a - f(x')   \leq \sensitivity{f} - f(x) + a \leq L + |a - f(x)|,
        \]
        as $-f(x') \leq \sensitivity{f} - f(x)$.
      \end{enumerate}
      Replacing into the exponential gives us the required inequality. The final result is obtained with elementary algebra.
      For the general case when $f(x) \in \Reals^m$, the same steps are performed. To deal with a multidimensional value of $f$, we use the fact that $\exp(\|z\|_1) = \prod_i \exp(|z_i|)$.
    }
  \end{proof}

  \begin{exampleblock}{DP properties of the average in the local and central model.}
    What is the effect of applying the Laplace mechanism in the local
    versus central model?  \only<article>{ Let us continue the
      average example.  Here let us assume $x_i \in [0, B]$ for all
      $i$.
      
      \emph{The Laplace mechanism in the local privacy model.}
        The sensitivity of the individual data is $B$, so to obtain $\epsilon$-DP we need to use $\lambda = B / \epsilon$. The variance of each component is $2(B/\epsilon)^2$, so the total variance is $2B^2/\epsilon^2 n$.

      \emph{The Laplace mechanism in the central privacy model}
        The sensitivity of $f$ is $B / n$, so we only need to use $\lambda = \frac{B}{n \epsilon}$. The variance of $a$ is $2(B / \epsilon n)^2$. 

      Thus the two models have a significant difference in the variance of the estimates obtained, for the same amount of privacy. While the central mechanism has variance $O(n^{-2})$, the local one is $O(n^{-1})$ and so our estimates will need much more data to be accurate under local privacy. In particular, we need square the amount of data in the local model as we need in the central model. Nevertheless, the local model may be the only possible route if we have no specific use for the data, or if the users do not trust the data curator.
    }
    
  \end{exampleblock}

  Calculating an average value is a common enough computation. Other common queries are counting and sums. Indeed, instead of releasing the differentially-private version of the average, we can calculate the sum and the total number of entries in a differentially private way, and release that instead.

  \begin{example}{Sensitivity of counting.}
    Consider a counting function $f_S : \CX \to \Naturals$, defined by counting the number of elements in some set $S$:
    \[
      f_S(x) = \sum_{t=1}^T \ind{x_t \in S}.
    \]
    This could be, for example all records with age in a certain age. If the set $S$ is the universal set, then this reduces to counting the number of records.
    The sensitivity of this query is 1, because if $x' = x \cup \{x_{T+1}\}$,
    \[
      f_S(x') - f_S(x) = \sum_{t=1}^{T+1} \ind{x_t \in S} -  \sum_{t=1}^T \ind{x_t \in S}
      = \ind{x_{T+1} \in S}.
    \]
  \end{example}

  \begin{example}{Sensitivity of sum.}
    Consider a sum function $f_i : \CX \to \Naturals$, defined by summing the value of one data column
    \[
      f_i(x) = \sum_{t=1}^T x_{t,i}
    \]
    This could be, for example the total age of people in the data. If $x_{t,i} \in [0, B]$, the sensitivity of this query is obviously $B$.
    \[
      f_S(x') - f_S(x) = \sum_{t=1}^{T+1} x_{t,i}  -  \sum_{t=1}^T x_{t,i}
      = x_{T+1, i}.
    \]
    Since the features are $B$-bounded, the result follows.
  \end{example}

  Combining the above two examples, we can replace releasing the average directly with first calculating the number of entries, adding Laplace noise with sensitivity $1$. We can then calculating the sum, adding Laplace noise with sensitivity $B$. We can then divide the two, and return the result. Intuitively, however, the Laplace noise in the denominator increases the variance of this estimate versus simply releasing the DP average.
  
  \begin{alertblock}{Sensitivity versus local sensitivity.}
    The sensitivity is a global property of a function $f$. Sometimes it is confused with the \alert{local} sensitivity of a function at a point $x$, which is simply $\max_{x': x \neigh x'} \|f(x) - f(x')\|$. Thus, local sensitivity at $x$ is analogous to the derivative of the function at $x$, while (global) sensitivity is the maximum local sensitivity over all possible points $x$. For that reason, it is generally impossible to calculate the sensitivity of a general function. While local sensitivity is easier to calculate, we cannot directly use it to create differentially private algorithms. This is because calculation of local sensitivity depends on the data, and so we must use a differentially private mechanism to calculate it in the first place. For that reason~\citet{nissim2007smooth} proposed to instead calculate a smoothed value of the local sensitivity.
  \end{alertblock}


\end{frame}

\section{Interactive data access.}
\only<article>{
  In a lot of applications, we cannot pre-define the computation that we want to perform. After we have collected the data, we need to be perform an adaptive data analysis.  This requires performing a sequence of computations on the data, where the result of one computation determines what computation we will do next.

  We can think of this as performing queries to the database. You may
  be familiar with database access languages such as SQL, where you
  ask a question such as ``what is the sum of the attribute
  \texttt{age} in table \texttt{students}?''and obtain the exact sum
  back. It is possible to answer such queries through a differentially
  private mechanism. However, the more queries you answer, the more
  you reveal about the original data. In addition, since an adversary
  may be cleverly adjusting the queries to more efficiently discover a
  secret, we need the concept of adaptive composition.
}
\begin{frame}
  \only<presentation>{
    \frametitle{Databases}
    \begin{example}[Typical relational database in a tax office]
      \begin{table}[H]
        \centering
        \begin{tabular}{l|l|l|l|l|l|l}
          ID & Name &  Salary & Deposits & Age & Postcode & Profession\\
          \hline
          1959060783 & Li Pu & 150,000 & 1e6 & 60 & 1001 & Politician\\
          1946061408 & Sara Lee & 300,000 & -1e9 & 72 & 1001 & Rentier\\
          2100010101 & A. B. Student & 10,000 & 100,000 & 40 & 1001 & Time Traveller
        \end{tabular}
      \end{table}
    \end{example}
    \only<1>{
      \begin{block}{Database access}
        \begin{itemize}
        \item When owning the database: Direct look-up.
        \item When accessing a server etc: Query model.
        \end{itemize}
      \end{block}
    }
  }
  \only<2>{
    \begin{figure}[H]
      \centering
      \begin{tikzpicture}
        \node[rectangle] at (0,0) (python) {Python program};
        \node[rectangle] at (8,0) (database) {Database System};
        \draw[thickarrow, bend right]   (python) to node[black]{Query} (database) ;
        \draw[thickarrow, bend right]   (database) to node[black]{response} (python) ;
      \end{tikzpicture}
      \label{fig:database-access}
      \caption{Database access model}
    \end{figure}
  }
  
\end{frame}

\only<presentation>{
  \begin{frame}
    \frametitle{SQL: A language for database access}
    \begin{block}{Creating and filling tables}
      \begin{itemize}
      \item \texttt{CREATE TABLE table-name (column1, column2)}
        \only<article>{Create a new table}
      \item \texttt{INSERT INTO table-name VALUES ('value1', 'value2')}
        \only<article>{Add specific values into a table}
      \item \texttt{INSERT INTO table-name VALUES (?, ?), variable}
        \only<article>{Fill in values from a variable}
      \end{itemize}
    \end{block}

    \begin{example}{Database creation}
      \url{src/privacy/database-creation.py}
      \\
      \url{src/privacy/database-access.py}
    \end{example}
  \end{frame}
  \begin{frame}
    \frametitle{Queries in SQL}
    \begin{block}{The \texttt{SELECT} statement}
      \begin{itemize}
      \item \texttt{SELECT column1, column2 FROM table;}
        \only<article>{This selects only some columns from the table}
      \item \texttt{SELECT * FROM table;}
        \only<article>{This selects all the columns from the table}
      \end{itemize}
    \end{block}

    \begin{block}{Selecting rows}
      \texttt{SELECT * FROM table WHERE column = value;}
    \end{block}

    \begin{exampleblock}{Arithmetic queries}
      \only<article>{Here are some example SQL statements}
      \begin{itemize}
      \item  \texttt{SELECT COUNT(column) FROM table WHERE condition;}
        \only<article>{This allows you to count the number of rows matching \texttt{condition}}
      \item  \texttt{SELECT AVG(column) FROM table WHERE condition;}
        \only<article>{This lets you to count the number of rows matching \texttt{condition}}
      \item  \texttt{SELECT SUM(column) FROM table WHERE condition;}
        \only<article>{This is used to sum up the values in a column.}
      \end{itemize}
    \end{exampleblock}

  \end{frame}
}

\only<presentation>{
  \begin{frame}
    \begin{figure}[H]
      \centering
      \begin{tikzpicture}
        \node[rectangle] at (0,0) (python) {Python program};
        \node[rectangle] at (8,0) (database) {Database System};
        \draw[thickarrow, bend right]   (python) to node[black]{Query $q$} (database) ;
        \draw[thickarrow, bend right]   (database) to node[black]{Private response $a$} (python) ;
      \end{tikzpicture}
      \label{fig:database-access}
      \caption{Private database access model}
    \end{figure}
    \begin{block}{Response policy}
      \index{policy!database response}
      The  policy defines a distribution over responses $a$ given the data $x$ and the query $q$.
      \[
        \pol(a \mid x, q)
      \]
    \end{block}
  \end{frame}
}

\only<presentation>{
  \begin{frame}
    \frametitle{Differentially private queries}
    \only<article>{There is no actual \texttt{DP-SELECT} statement, but we can imagine it.}
    \begin{block}{The \texttt{DP-SELECT} statement}
      \begin{itemize}
      \item \texttt{DP-SELECT $\epsilon$ column1, column2 FROM table;}
        \only<article>{This selects only some columns from the table}
      \item \texttt{DP-SELECT $\epsilon$ * FROM table;}
        \only<article>{This selects all the columns from the table}
      \end{itemize}
    \end{block}

    \begin{block}{Selecting rows}
      \texttt{DP-SELECT $\epsilon$  * FROM table WHERE column = value;}
    \end{block}

    \begin{exampleblock}{Arithmetic queries}
      \only<article>{Here are some example SQL statements}
      \begin{itemize}
      \item  \texttt{DP-SELECT $\epsilon$ COUNT(column) FROM table WHERE condition;}
        \only<article>{This allows you to count the number of rows matching \texttt{condition}}
      \item  \texttt{DP-SELECT $\epsilon$ AVG(column) FROM table WHERE condition;}
        \only<article>{This lets you to count the number of rows matching \texttt{condition}}
      \item  \texttt{DP-SELECT $\epsilon$ SUM(column) FROM table WHERE condition;}
        \only<article>{This is used to sum up the values in a column.}
      \end{itemize}
    \end{exampleblock}

  \end{frame}
}


\subsection{Utility of queries}

\begin{frame}
  \only<article>{Rather than saying that we wish to calculate a private version of some specific function $f$, sometimes it is more useful to consider the problem from the perspective of the utility of different answers to queries. More precisely, imagine the interaction between a database system and a user:}
  \begin{block}{Interactive queries}
    \only<article>{In this setting, we do not know \emph{a priori} what we want to compute on the data. We want to be able to reply to arbitrary queries, from a potentially adversarial party, in a useful manner. The interaction involves the following elements.}
    \begin{itemize}
    \item The system has data $x \in \CX$.
    \item At time $t$, an adversary asks a query $q_t \in \CQ$.
    \item The system answers with $a_t \in \CA$.
    \item There is a common utility function
      $\util : \CX, \CA, \CQ \to \Reals$.
    \end{itemize}
    We wish to give the most useful answer, by returning $a$ that maximises $\util$, i.e. $a = \argmax_{a'} \util(x, a', q)$, but are constrained by the fact that we also want to preserve privacy. To do that we will use a differentially private mechanism to answer.
  \end{block}
  \only<article>{The utility $\util(x,a,q)$  describes how appropriate each answer $a$ given by the system for a query $q$ is given the data $x$. It can be seen as how useful the response is.~\footnote{This is essentially the utility to the user that asks the query, but it could be the utility to the person that answers. In either case, the answer should maximise utility, but is constrained by privacy.} It allows us to quantify exactly how much we would gain by replying correctly. The exponential mechanism, described below is a simple differentially private mechanism for responding to queries while trying to maximise utility for \alert{any possible} utility function.}

\end{frame}
\subsection{The exponential mechanism}
\begin{frame}

  \only<article>{ Here we
    assume that we can answer queries $q$, whereby each possible
    answer $a$ to the query has a different utility $\util(q, a, x)$.  The idea is that the best answer to the query
    should have the highest utility. The further away from the correct
    answer the response is, the lower the utility should be. This
    allows us to quantify how much we value correct answers to a
    query.

    As an example, if the optimal response to a query is given by a
    real-valued function $f(q, x)$, then one possible utility function
    is $\util(q,a,x) = - [f(q,x) - a]^2$. This results in the correct
    response having a utility of zero, and responses whose value is
    further away will have negative values.

    The exponential mechanism allows us to answer queries in a ``soft'' manner, by using this utility function. It assigns lower probability to answers with lower utility, hence the better responses have a higher likelihood of being selected.
    
  }
  \begin{definition}[The Exponential mechanism]
    For any utility function $\util : \CQ \times \CA \times \CX \to \Reals$, define the policy, which implicitly depends on $\util$ and $\epsilon$, as:
    \index{policy!exponential mechanism}
    \begin{equation}
      \label{eq:exponential-mechanism}
      \pol(a \mid x, q) \defn \frac{e^{\epsilon \util(q, a, x) / 2\sensitivity{ \util(q)}}}{\sum_{a'} e^{\epsilon \util(q, a', x) / 2\sensitivity{\util(q)}}},
    \end{equation}
    where $\sensitivity{\util(q)} \defn \max_a \sup_{x N x'} |\util(q,
    a, x) -\util(q, a, x')|$ denotes the sensitivity of a query. 
  \end{definition}
  \only<presentation>{
    What happens when $\epsilon \to \infty$? What about when $\epsilon \to 0$?
  }
  \only<article>{
    Clearly, when $\epsilon \to 0$, this mechanism is uniformly random. When $\epsilon \to \infty$ the action maximising $\util(q,a,x)$ is always chosen (see Exercise~\ref{exer:exp-randomness}).
    Although the exponential mechanism can be used to describe some known DP mechanisms (see Exercise~\ref{exer:exp-laplace-gauss}), its best use is in settings where there is a natural utility function.
  }

  \begin{theorem}
    The exponential mechanism is $\epsilon$-differentially private.
  \end{theorem}
  \only<article>{
    \begin{proof}
      We only need to look at the ratio between two different distributions of answers to queries. Then we have:
      \begin{align*}
        \frac{\pol(a \mid x, q)}{\pol(a \mid x', q)}
        &=
          \frac{e^{\epsilon \util(q, a, x) / 2\sensitivity{ \util(q)}}}{\sum_{a'} e^{\epsilon \util(q, a', x) / 2 \sensitivity{\util(q)}}}
          \times          
          \frac{\sum_{a'} e^{\epsilon \util(q, a', x') / \sensitivity{\util(q)}}}         {e^{\epsilon \util(q, a, x') / 2\sensitivity{ \util(q)}}}
        \\
        &=
          e^{\epsilon \util(q, a, x) / 2 \sensitivity{ \util(q)}
          - \epsilon \util(q, a, x') / 2\sensitivity{ \util(q)}}
          \times          
          \frac{\sum_{a'} e^{\epsilon \util(q, a', x') / 2\sensitivity{\util(q)}}}
          {\sum_{a'} e^{\epsilon \util(q, a', x) / 2\sensitivity{\util(q)}}}
        \\
        &=e^{\epsilon [\util(q, a, x) - \util(q, a, x')] / 2\sensitivity{ \util(q)}}
          \times          
          \frac{\sum_{a'} e^{\epsilon \util(q, a', x') / 2\sensitivity{\util(q)}}}
          {\sum_{a'} e^{\epsilon \util(q, a', x) / 2\sensitivity{\util(q)}}}
        \\
        &\leq
          e^{\epsilon/2}
          \times          
          \frac{\sum_{a'} e^{\epsilon \util(q, a', x') / 2\sensitivity{\util(q)}}}
          {\sum_{a'} e^{\epsilon \util(q, a', x) / 2\sensitivity{\util(q)}}}
        \\
        &\leq
          e^{\epsilon/2}
          \times          
          \frac{\sum_{a'} e^{\epsilon [(\util(q, a', x) + \sensitivity{\util(q)}) / 2\sensitivity{\util(q)}}}
          {\sum_{a'} e^{\epsilon \util(q, a', x) / 2\sensitivity{\util(q)}}}
          \leq
          e^{\epsilon}
      \end{align*}
    \end{proof}
  }
\end{frame}

While the exponential mechanism is quite abstract, it is very flexible. Many problems in machine learning and database querying can be made differentially private through the use of the exponential mechanism. This has to be done by specifying appropriate values for the utility function for every possible query and answer combination. In the database setting, queries are simple statistical facts and the utility is a function of how far the answer is from the true answer. In the machine learning setting, we can consider two different types of queries: those where the intention is to create a machine learning model from data, and those where we only need to query a machine learning model.

\begin{exampleblock}{Answering database queries with the exponential mechanism.}
  The simplest typical database queries involve counting and summation over a subset of database rows. As an example, let us say we wish to count the number of rows where a certain condition is true, e.g. the number of individuals that are (a) male  and (b) have an income of more than 100,000. For any such counting query, the sensitivity is 1. However, the sensitivity of the utility function is another matter. Let $f(x,q)$ be the correct answer for the query. If we set $U(a,x,q) = -|f(x,q) - a|$, then the utility-maximising answer is $f(x,q)$, while the sensitivity of $U$ is equal to that of $f$. For other choices of the utility function, it might not be so easy to characterise its sensitivity, however.
\end{exampleblock}
\begin{exampleblock}{The exponential mechanism for machine learning models.}
  The most common scenario involving privacy in machine learning is  when we want to fit a model to privacy-protected training data. This means that the machine learning \emph{algorithm} itself has to be differentially private. As an example, consider the problem of estimating a classification model $P_\param(y | x)$ parametrised by $\param \in \Param$. The learning algorithm should define a probability distribution $\pol(\param \mid x)$ over parameters that is differentially private. This means $\param$ will not leak information about the training data. A natural way to achieve differential privacy in that setting is to use the exponential mechanism, where the utility $U$ is the classifier's performance on a hold-out data set.

The other possible scenario is protecting the data of people that interact with the model. Assume a given model $P_\param$, and a person $t$ with features $x_t$ giving their data to their model, and receiving some response $a_t$ back. If it is only this person interacting with the model, then there is little need for privacy protection. However, we might have a setting where $T$ people are submitting the data, and the system calculates a response that is visible to all. In that case, privacy would become an issue. In that case, if $P_\param(y_t | x_t)$ defines some a distribution over outcomes, and we make a joint decision $a$ with utility
  \[
    U(a, \param, x) = \sum_{t=1}^T \sum_y u(a, y) P_\param(y_t = y \mid x_t),
  \]
  it is natural to use the exponential mechanism to answer individual queries in a private manner.
\end{exampleblock}

\only<article>{
\begin{theoryblock}{Interactive differential privacy.}
  So far we only defined differential privacy in terms of a fixed
  mechanism and a fixed sequence of queries. However, in general the
  mechanism must respond to sequential queries $q_1, \ldots, q_t$,
  with answers $a_1, \ldots, a_t$, where the query at time $t$ depends
  on all the previous queries $q_1, \ldots, q_{t-1}$ and answers
  $a_1, \ldots, a_{t-1}$. In addition, the answer can also depend arbitrarily on the
  sequence of responses and queries. So, privacy must hold no matter
  what the previous sequence of questions and answers has been.  This gives
  rise to the following definition, where the answers are conditioned on the history of interactions and the data.
  
  \begin{definition}[Interactive DP]
    An algorithm $\pol$ satisfies interactive $\{\epsilon_1, \ldots, \epsilon_t\}$ differential privacy with respect to $\neigh$ if
    \begin{equation}
      \label{eq:interactive-dp}
      \ln
      \frac{\pol(a_t \mid x, q_1, \ldots, q_t, a_1, \ldots, a_{t-1})}
      {\pol(a_t \mid x', q_1, \ldots, q_t, a_1, \ldots, a_{t-1})}
      \leq \epsilon_t,
      \qquad
      \forall x \neigh x'.
    \end{equation}
  \end{definition}
  Since the queries are not defined in advance, this means that $q_t$ may also depend on all the previous answers $a_1, \ldots, a_{t-1}$.

Answering queries independently in the interactive setting is
sufficient for achieving differential privacy $\epsilon_t$ at each
step $t$. The total privacy loss is at most $\sum_{i=1}^t
\epsilon_i$. However, it is possible to bound the privacy loss more
tightly with advanced composition theorems.

  
\end{theoryblock}
}
\section{Advanced topics.}


\only<article>{
  In this section, we give a brief overview of some more advanced topics on privacy. The interested reader is urged to look into the given references.
}
\subsection{Variants of differential privacy.}
\only<article>{
  In practice, satisfying differential privacy with a small $\epsilon$ results in mechanisms with low utility. It is possible to relax the definition of differential privacy to include an additive term, as shown below, which greatly improves the privacy-utility trade-off we can achieve.
  \begin{definition}{Approximate differential privacy.}
    \label{def:epsilon-delta-dp}
    A stochastic algorithm $\pol : \CX \to \Simplex(\CA)$, where $\CX$
    is endowed with a neighbourhood relation $\neigh$, is said to be
    $(\epsilon,\delta)$ differentially private if
    \begin{equation}
      \label{eq:epsilon-delta-dp}
      \pol(A \mid x) \leq \pol(A \mid x') \epsilon + \delta, \qquad \forall x \neigh x', \quad A \subset \CA.
    \end{equation}
  \end{definition}
  One simple interpretation of this definition is that the mechanism is
  $(\epsilon, 0)$-DP with probability $1 - \delta$. Conversely, this
  implies that with a very small probability $\delta$, the complete
  dataset might be revealed. For that reason, the parameter $\delta$
  must be as small as possible.

  Approximate differential privacy is satisfied by the \emph{Gaussian} mechanism. This is similar to the Laplace mechanism, but scales noise to the $\ell_2$ sensitivity of the function.
  \begin{exampleblock}{The Gaussian mechanism}
    Given a function $f : \CX \to \Reals^k$, the Gaussian mechanism with parameter $\sigma$ outputs
    $a \sim \Normal(f(x), \sigma)$.

    Let $\ell_2$ sensitivity of $f$ be
    \[
      \sensitivity{f}_2 = \max\cset{\|f(x) - f(y)\|_2}{x N y}.
    \]
    For $\sigma \geq 2\ln(1.25 / \delta) \frac{\Delta_2(f)}{\epsilon}$, then the Gaussian mechanism is $(\epsilon, \delta)$-differentially private.
  \end{exampleblock}

  \begin{theoryblock}{Generalising differential privacy.}
    The careful reader will have noticed that differential privacy is essentially a bound on the distributional distance of mechanism outputs for similar inputs.
    Let us define $\pol_{x}(A)$ to be the probability measure $\pol(A \mid x)$ induced by a mechanism $\pol$ conditioned on $x$. 
    Then, $\pol$ is $\epsilon$-DP in terms of the infinity divergence:
    \[
      D_\infty(\pol_{x} \| \pol_{x'}) \leq \epsilon, \qquad \forall x \neigh x',
    \]
    where the infinity divergence is defined as:
    \[
      D_\infty(P \| Q) \defn \sup_A |\ln P(A)/Q(A)|.
    \]

    In order to obtain the same definition for $(\epsilon, \delta)$-DP, we need to consider the more general class of $f$-divergences, defined as:
    \begin{equation}
      \label{eq:f-divergence}
      D_f(P\|Q) \defn \E_Q[f(dP/dQ)].
    \end{equation}
    It is easy to see that the KL-divergence is obtained by
    $f(t) = t \ln t$ and the total variation by
    $f(t) = \frac{1}{2}|t - 1|$.  In particular, $(\epsilon, \delta)$
    differential privacy can be defined in terms of a type of
    $f$-divergence called a hockey-stick-divergence
    $\hock{\alpha}{P}{Q}$, where
    \[
      \hock{\alpha}{P}{Q} \defn \sup_A [P(A) - \alpha Q(A)] = \int_\Omega [\frac{d P}{d Q} (\omega) - \alpha]_+ dQ(\omega),
    \]
    which for finite $\Omega$ is just $\sum_{\omega \in \Omega} [P(A) - \alpha Q(A)]_+$.
Then a mechanism $\pol$ is $(\epsilon, \delta)$-DP iff
    \begin{equation}
      \hock{e^\epsilon}{\pol_x}{\pol_{x'}}\leq \delta.
      \label{eq:divergence-DP}
    \end{equation}
    We can use any other divergence between distributions, such as a Renyi divergence, as long as it becomes unbounded for distributions with unequal support.  This has led to a proliferation of DP variants including approximate-DP, Renyi-DP and $z$-DP. In the sequel, we will use these definitions to more finely characterise the privacy guarantees of different mechanisms.
  \end{theoryblock}

}

\subsection{Differential privacy as a hypothesis testing game.}

Another way of viewing differential privacy is as a simple game
between an adversary interacting with an honest data analyst. At time
$t$, the adversary chooses two arbitrary neighbouring databases
$x, x'$. The analyst randomly chooses between $x_t$ to be one of
the two databases. It then generates $a_t \sim \pol(a \mid x_t)$,
and shows it to the adversary. The adversary must now decide if
$x_t = x$ or $x_t = x'$, i.e. which dataset was used by the
analyst. More precisely, the adversary must choose between hypothesis
0 $(x_t = x)$ and hypothesis 1, $(x_t = x')$.

Since the adversary can only base their decision on $a_t$, they must
use a decision rule of the form $f : \CA \to \{0,1\}$ so as to decide
between the two hypotheses.  That means that they must partition $\CA$
in two subsets, $A_0, A_1$ so that whenever $a_t \in A_0$ they
decide for $x$ and whenever $a_t \in A_1$ they decide for
$x'$. Because we know the that the probability of the 
response given the dataset depends only on the mechanism $\pol$, we
can write the probability of a true and a false positive as:
\[
  p_{\textrm{TP}} = \pol(A_0 \mid x), \qquad p_{\textrm{FP}} = \pol(A_1 \mid x).
\]
This is simply because the adversary answers correctly whenever
$a_t \in A_0$, and incorrectly otherwise. Similarly, the probability
of a true and false negative as
\[
  p_{\textrm{TN}} = \pol(A_0 \mid x), \qquad p_{\textrm{FN}} = \pol(A_0 \mid x').
\]
The probability with which the mechanism selects either dataset does
not factor into these quantities, are these are simply the conditional
probability of the incorrect decision region for each possible
dataset.

\begin{theorem}
  If the mechanism $\pol$ is $(\epsilon, \delta)$-differentially private then
  the false positive rate (FPR) and false negative rate (FNR) of the
  adversary are linked as follows:
  \begin{align*}
    p_{\textrm{TP}}\leq p_{\textrm{FP}} e^\epsilon  + \delta,
  \end{align*}
  i.e. the true positive rate cannot be much higher than the false positive rate. and conversely:
  \begin{align*}
    p_{\textrm{TN}}\leq p_{\textrm{FN}} e^\epsilon  + \delta,
  \end{align*}
  i.e. the true negative rate cnanot be much higher than the false
  negative rate.
\end{theorem}
\begin{proof}
  The proof is by direct substitution and the definition of $(\epsilon, \delta)$-differential privacy:
  \begin{align*}
    p_{\textrm{TP}} = \pol(A_1 | x') \leq \pol(A_1 | x) e^\epsilon  + \delta = p_{\textrm{FP}} e^\epsilon  + \delta.
  \end{align*}
  Similarly
  \begin{align*}
    p_{\textrm{TN}} = \pol(A_0 | x) \leq \pol(A_0 | x') e^\epsilon  + \delta
    = p_{\textrm{FN}} e^\epsilon  + \delta.
  \end{align*}
\end{proof}

\only<article>{As it turns out, privacy in terms of hypothesis testing is a somewhat more general concept than differential privacy. In fact, we could define a mechanism with a complete false negative and positive testing \emph{profile}, or equivalently an $(\epsilon, \delta)$-profile. The concept is similar to the idea of a ROC curve, as we detail in the next section.}

\subsection{Privacy profile.}
\only<article>{
  Simply saying that an algorithm is $(\epsilon, \delta)$-DP does not characterise it very precisely. In fact, any given algorithm could satisfy privacy for a number of different values of $\epsilon, \delta$. What we need is a curve characterising the achievable value of $\delta_\pol(\epsilon)$ for a given mechanism $\pol$ and specific privacy parameter $\delta$. In order to create such a function, we will use our definition of privacy in terms of divergences given in \eqref{eq:divergence-DP}.
  \begin{definition}[Privacy profile]
    The privacy profile of $\pol$ is
    \begin{equation}
      \delta_{\pol}(\epsilon) = \sup_{x \neigh x'} \hock{e^\epsilon}{\pol_x}{\pol_{x'}}
    \end{equation}
    In other words, $\pol$ is $(\epsilon, \delta_{\pol}(\epsilon))$-DP.
  \end{definition}
  This definition perfectly characterises the approximate privacy level achievable for an algorithm for any given $\epsilon$. In fact, any algorithm with a privacy profile $\delta_{\pol}(\epsilon)$ satisfies $(\epsilon, \delta_\pol(\epsilon))$-DP for all $\epsilon > 0$.

  It is useful to look at the randomised response mechanism as a motivating example. 
  \begin{example}
  As an example, consider the randomised response mechanism $\pol$ on $\{0,1\}$ which w.p. $p \geq 1/2$ outputs $x$, otherwise $1 - x$. The privacy profile of $\pol$ is given by
  \begin{align}
    \delta_\pol(\epsilon) = [p - e^\epsilon(1 - p)]_+ ~.
    \label{eq:privacy-profile-rr}
  \end{align}
  This is easy to see,  as $x$ only takes two values. We only need to write the divergence for those:
  \begin{align*}
    \hock{e^\epsilon}{\pol_1}{\pol_0}
    &=
      [\pol_1(1) - e^\epsilon \pol_0(1)]_+ + [\pol_1(0) - e^\epsilon \pol_0(0)]_+
    =
      [p - e^\epsilon (1 - p)]_+ + [(1 - p) - e^\epsilon p]_+.
  \end{align*}
  Since $p \geq 1/2$, $(1 -p) \leq p$, and the result follows. As we know already, the mechanism is $\epsilon$-DP with $\epsilon = \ln[p/(1-p)]$, i.e. $\delta = 0$. As we see by the above function, however, we the mechanism satisfies $(\epsilon, \delta)$-DP for a number of different parameters. This is visualised in the function shown in Figure~\ref{fig:delta-rr}.
  \begin{figure}[h]
    \centering
    \begin{tikzpicture}
      \begin{axis}[domain=0:2.5,
        xlabel=$\epsilon$,
        ylabel=$\delta$,
        grid=both]
        % \draw[step=0.1, line width=0.1pt,color=gray] (0,0) grid (2.5,1.1);
        % \draw[step=1,very thin,color=cyan] (0,0) grid (2.5,1.1);
        % \draw[->] (0,0) -- (2.5,0) node[right] {$\epsilon$};
        % \draw[->] (0,0) -- (0,1.1) node[above] {$\delta_\pol(\epsilon)$};
        \addplot[samples=100,color=red] {max(0,0.6 - exp(x) *0.4)};
        \addlegendentry{$p=0.4$}
        \addplot[samples=100,color=magenta,dashed] {max(0,0.8 - exp(x) *0.2)}; % p=0.8
        \addlegendentry{$p=0.8$}
        \addplot[samples=100,color=blue,dotted]  {max(0,0.9 - exp(x) *0.1)};
        \addlegendentry{$p=0.9$}
      % \addplot[domain=0:2.5,samples=100]{x^2};
      \end{axis}
  \end{tikzpicture}
    \caption{Illustration of the dependence between $\epsilon, \delta$ for binary randomised response and various values of $p$. we see that every curve hits the $\delta = 0$ line for some value of $\epsilon$. This is the value for which we derived that the mechanism is $\epsilon$-DP. However, the mechanism is also $(\epsilon, \delta_\pi(\epsilon))$-DP for every point on the curve.}
    \label{fig:delta-rr}
  \end{figure}
   Here, the mechanism satisfies $(\epsilon, \delta)$-DP in a different way depending on the choice of $p$. For smaller values of $p$, the amount of privacy is higher. This means both that the value of $\epsilon$ for which $\delta = 0$ is smaller, and that the curve is lower than all the other curves with a higher $p$. This graph has a superficial similarity to ROC curves. However, there is a fundamental difference: a ROC curve is constructed by showing the performance of a system (e.g. type I and type II errors) over a range of parameter values (e.g. thresholds). However, these curves show that a single value of the parameter (in this case $p$) satisfies differential privacy for a \emph{range} of $(\epsilon, \delta)$ values.
  \end{example}
}
\subsection{Privacy amplification}
\only<article>{
  Intuitively, the way we collect data in the first place can lead to better privacy guarantees. If for example we perform a study on a sample of one thousand individuals in the city, we should have better privacy guarantees if the city population is ten million, compared to when it is only one million. The randomisation inherent in the data sampling procedure amplifies the differential privacy guarantees we have. The simplest results that we can obtain in this setting involve uniform sampling for a fixed population, i.e. subsampling:
}
\only<article>{
  \paragraph{Subsampling.}  Let us consider obtaining a sample $y$ from a dataset $x$. One of the simplest methods for doing so is Poisson subsampling. Here, $y$ includes an element $x_i$ with probability $\gamma$ so that the probability of obtaining $y$ from $x$ is $\mu_x(y) \defn \mu(y | x) = \gamma^{|y|}(1 - \gamma)^{|x| - |y|}$. By itself, subsampling does not give us any guarantees, but we can combine it with a mechanism $\pol_y(z) \defn \pol(z | y)$ with privacy profile $\delta_{\pol}(\epsilon)$. Then the combined mechanism $\pol'$ obtained by first subsampling with $\mu$ and then applying $\pol$, has a privacy profile $\delta_{\pol'}$ satisfying:
  \[
    \delta_{\pol'}(\epsilon') \leq \gamma \delta_{\pol}(\epsilon),
    \qquad
    \epsilon' = \ln[1 + \gamma(e^\epsilon -1)],
  \]
  as is proven by~\citet{balle2020privacy}. Thus, for small values of $\epsilon$, subsampling amplifies privacy by a factor of $\gamma$ for both privacy parameters. 
  %[See also: Privacy Amplification by Subsampling in Time Domain]
}

\paragraph{Shuffling.}
\only<article>{
  If data is anonymised, then it can be shuffled~\citep{erlingsson2019amplification,feldman2022hiding}, so it is impossible to tell which data point came from which individual. This can be seen as a privacy model somewhere in between the central model, where data is given to a trusted curator, and the local model, where each user randomises their data. Using shuffling, we can amplify privacy in the following way. If we use $n$ copies of a local $\epsilon$-DP mechanism, each with $\epsilon = O(\ln(n / ln(1/\delta)))$ then the total privacy loss is $\epsilon' = O(\min\{e, 1\} e^\epsilon / \sqrt{\ln(1/\delta)/n})$.
%[Hiding among the clones] gives a bond of $(1 - e^{-\epsilon_0}) \sqrt{e^{\epsilon_0} \ln(1/\delta) / T}$.
}
%\paragraph{Random check-ins.}
%\only<article>{
%  Privacy Amplification via Random Check-Ins
%}

\paragraph{Iteration.}
\only<article>{
  Most modern machine learning algorithms are iterative: they have parameters that are adjusted through multiple iterations over the data. Stochastic gradient descent is a classic example, where each iteration is over a random subsample of the data. Using results from subsampling to bound the privacy loss in each iteration, we can then use composition to bound the total privacy loss of the algorithm. In this analysis, we essentially assume that the output of the algorithm can be observed at every step.  \citet{privacy-amplification-iteration} show that (1-Lipschitz) contraction mappings with Gaussian noise have good differential privacy properties, as long as the intermediate results of the computation are not released. This allows them to show that the additional loss due to privacy for a population of $n$ individuals is bounded by $O(\epsilon^{-1} n^{-1/2})$.
  %In fact if $x_{t+1} = f(x_t) + \omega_t$, then for any two starting points $x_0, x_0'$: $\renyi{\alpha}{x_T}{x'_T} \leq ..>$
  %[Privacy Amplification by Mixing and Diffusion Mechanisms]
}

\subsection{Reproducibility}
\only<article>{
  %Training and testing, overfitting on the test set. \cdcomment{stub}
  Ideally, when we analyse data, we perform each analysis on a new sample. In practice, however, we perform an adaptive analysis of the same data multiple times. This inevitably makes all such analyses biased in some sense. To remove this bias we need to use some form of randomisation that removes the dependencies between the analyses. In fact, differential privacy offers a conceptually simple tool to achieve this.\cite{smith2017information}

}

\begin{frame}
  \frametitle{The unfortunate practice of adaptive analysis}
  \begin{figure}
    \centering
    \begin{tikzpicture}
      \node<1->[rectangle] at (0,4) (prior) {Prior};
      \node<2->[rectangle] at (0,0) (training) {Training data};
      \node<3->[rectangle] at (4,4) (posterior) {Posterior};
      \node<5->[rectangle] at (8,4) (posterior2) {Posterior'};
      \node<2->[rectangle] at (4,0) (holdout) {Holdout};
      \node<4->[RV] at (4,2) (result) {Result};
      \node<5->[RV] at (8,2) (result2) {Result'};
      \draw<3->[medarrow] (training)--(posterior);
      \draw<3->[medarrow] (prior)--(posterior);
      \draw<4->[medarrow] (posterior)--(result);
      \draw<4->[medarrow] (holdout)--(result);
      \draw<5->[red,medarrow] (posterior2)--(result2);
      \draw<5->[red,medarrow] (holdout)--(result2);
      \draw<5->[red,medarrow] (result)--(posterior2);
      \draw<5->[red,medarrow] (posterior)--(posterior2);
    \end{tikzpicture}
  \end{figure}
  \only<article>{In the ideal data analysis, we start from some prior hypothesis, then obtain some data, which we split into training and holdout. We then examine the training data and obtain a posterior that corresponds to our conclusions. We can then measure the quality of these conclusions in the independent holdout set.

    However, this is not what happens in general. Analysts typically use the same holdout repeatedly, in order to improve the performance of their algorithms. This can be seen as indirectly using the holdout data to obtain information, and so it is possible that you can overfit on the holdout data, even if you never directly see it. As a simple example, consider evaluating a binary linear classifier on a holdout set.

    \begin{figure}[h]
      \centering
      \includegraphics{../figures/holdout.pdf}
      
      \caption{Use of a linear classifier to reveal labels in the holdout set. On the right, we are making one mistake less than on the left. This means that there is a positive example in the area between the two hyperplanes.}
      \label{fig:learning-holdout}
    \end{figure}
    We can solve this problem if we use differential privacy, so that
    the analyst can only perform privacy-preserving queries on the holdout set.

  }
\end{frame}


\begin{frame}
  \frametitle{The reusable holdout~\cite{dwork2015reusable}\footnote{Also see \url{https://ai.googleblog.com/2015/08/the-reusable-holdout-preserving.html}}}
  \only<article>{One idea to solve this problem is to only allow the analyst to see a private version of the result. In particular, the analyst will only see whether or not the holdout result is $\tau$-close to the training result.}

  \begin{block}{Algorithm parameters}
    \begin{itemize}
    \item Performance measure $f$.
    \item Threshold $\tau$. \only<article>{How close do we want $f$ to be on the training versus holdout set?}
    \item Noise $\sigma$. \only<article>{How much noise should we add?}
    \item Budget $B$. \only<article>{How much are we allowed to learn about the holdout set?}

    \end{itemize}
  \end{block}
  \begin{block}{Algorithm idea}
    \begin{algorithmic}
      \State Run algorithm $\lambda$ on data $\Training$ and get e.g.\ classifier parameters $\theta$.
      \State Run a DP version of the function $f(\theta, \Holdout) = \ind{\util(\theta, \Training) \geq \tau \util(\theta, \Holdout)}$.
    \end{algorithmic}
  \end{block}
  \only<article>{So instead of reporting the holdout performance at all, you just see if you are much worse than the training performance, i.e.\ if you're overfitting. The fact that the mechanism is DP also makes it difficult to learn the holdout set. See the thresholdout link for more details.}
\end{frame}



\section{Discussion}
The privacy attack in the Massachusetts database is detailed
in~\citep{sweeney2002k}, while $k$-anonymity is described more
thoroughly in in~\citep{samarati1998protecting}. This unfortunately
does not provide a formal privacy guarantee.  Differential privacy was
the first rigorous mathematical definition of privacy, with strong
guarantees about protecting personal information.  Differential
privacy, not actually given that name at the time, was introduced
in~\citep{dwork2006calibrating}. A technical book on differential
privacy by \citet{dwork2014algorithmic} covers all of the
fundamentals.

However, this definition of privacy relies on each individual data
record containing the data of a single individual, and us only
protecting the privacy of individuals.  If we want to protect a
different \emph{kind} of secret, then our privacy definition must
differ. For this reason, two generalisations of differential privacy
have been proposed, Pufferfish~\citep{pufferfish} and
Blowfish~\citep{blowfish}, where we define specific \emph{secrets} to
protect.

Another frontier for privacy is looking at different divergences other than the max-divergence for privacy. These include R\'{e}nyi differential privacy~\citep{renyi-dp} and zero-concentrated differential privacy (zCPD)~\citep{zc-dp}. These provide guarantees of $(\epsilon, \delta)$-DP for any value of $\delta$, with $\epsilon$ being a function of $\delta$.  Thus, they are closely linked to the the hypothesis-testing view of differential privacy and privacy profiles. The \emph{Gaussian mechanism} satisfies zCDP.

Differential privacy is also linked to Bayesian inference. Firstly, differential privacy can be interpreted from a Bayesian point of view (see Example~\ref{ex:Bayesian-DP}). Secondly, we can analyse how posterior distributions change when a single datapoint changes. Under some mild conditions on the likelihood and prior, one can show that posteriors satisfy a differential privacy condition, as shown by \citet{bayesiandp}. This can then be leveraged to create a \emph{posterior sampling mechanism} that is differentially private. The idea of this is to simply take one or more samples from the posterior distribution. Each sample corresponds to a probabilistic models, which can then be used to answer queries.  Under similar conditions, \citet{pmlr-v37-wangg15} proved that approximate posterior sampling using MCMC can also satisfy differential privacy. \cite{foulds:bayesian-sampling:2016} took this idea in another direction, by making the sufficient statistics used in inference differentially private.

Differential privacy can also be useful in game theory and mechanism design. In fact, the exponential mechanism was introduced by \citet{mcsherry2007mechanism}, with an application to approximately truthful mechanism design. This direction was further investigated by~\citet{kearns:md-privacy} and~\citet{nissim:ao-md:2012}.


The effect of local differential privacy on statistical efficiency was
first analysed by~\citet{duchi2013local}. Versions of local
differential privacy have been used by tech companies to provide some
modicum of privacy. Differential privacy has been used very rigorously
in the US 2020 census \cite{garfinkel2020randomness}. This relied on a
careful implementation of privacy so that statistics could be released
at different levels of granularity while ensuring both
self-consistency across federal, state and municipal levels and the
privacy of individuals. Generally speaking, it is not entirely trivial
to employ differential privacy in practice, but recently a number of
publicly available libraries for differential privacy have been
released. These include the \texttt{diffprivlib} python library by IBM
and the \texttt{OpenDP} open source library.

%Local differential privacy and applications \cite{yang2020local}



Another question is how fast can we actually learn from 
\only<presentation>{
\begin{frame}
  \begin{block}{The definition of differential privacy}
    \begin{itemize}
    \item First rigorous mathematical definition of privacy.
    \item Relaxations and generalisations possible.
    \item Connection to learning theory and reproducibility.
    \end{itemize}
  \end{block}

  \begin{block}{Current uses}
    \begin{itemize}
    \item Apple. \only<article>{DP is used internally in the company to ``protect user privacy''. It is not clear exactly what they are doing but their efforts seem to be going in the right direction.}
    \item Google. \only<article>{The company has a DP API available based on randomised response, RAPPOR.}
    \item Uber. \only<article>{Elastic sensitivity for SQL queries, which is available as open source. This is a good thing, because it is easy to get things wrong with privacy.}
    \item US 2020 Census. \only<article>{It uses differential privacy to protect the condidentiality of responders' information while maintaining data that are suitable for their intended uses.}
    \end{itemize}
  \end{block}

  \begin{block}{Open problems}
    \begin{itemize}
    \item Complexity of differential privacy.
    \item Verification of implementations and queries.
    \end{itemize}
  \end{block}
\end{frame}


\begin{frame}
  \frametitle{Available privacy toolboxes}
  \begin{block}{Differential privacy}
    \begin{itemize}
    \item Open DP \url<https://opendp.org/>
    \item \url{https://github.com/bmcmenamin/thresholdOut-explorations}{Threshold out}
    \item \url{https://github.com/steven7woo/Accuracy-First-Differential-Privacy}{Accuracy-constrained DP}
    \item \url{https://github.com/menisadi/pydp}{Various DP algorithms}
    \item \url{https://github.com/haiphanNJIT/PrivateDeepLearning} Deep learning and DP
    \end{itemize}
  \end{block}
  \begin{block}{$k$-anonymity}
    \begin{itemize}
    \item \url{https://github.com/qiyuangong/Mondrian} Mondrian k-anonymity
    \end{itemize}
  \end{block}
\end{frame}
}
\only<presentation>{
\begin{frame}
  \frametitle{Learning outcomes}
  \begin{block}{Understanding}
    \begin{itemize}
    \item Linkage attacks and $k$-anonymity.
    \item Inferring data from summary statistics.
    \item The local versus central differential privacy model.
    \item False discovery rates.
    \end{itemize}
  \end{block}
  
  \begin{block}{Skills}
    \begin{itemize}
    \item Make a dataset satisfy $k$-anonymity with respect to identifying attributes.
    \item Apply the randomised response and Laplace mechanism to data.
    \item Apply the exponential mechanism to simple decision problems.
    \item Use differential privacy to improve reproducibility.
    \end{itemize}
  \end{block}

  \begin{block}{Reflection}
    \begin{itemize}
    \item How can potentially identifying attributes be chosen to achieve $k$-anonymity?
    \item How should the parameters of the two ideas, $\epsilon$-DP and $k$-anonymity be chosen?
    \item Does having more data available make it easier to achieve privacy?
    \end{itemize}
  \end{block}

\end{frame}
}

\only<presentation>{
  \begin{frame}
    \frametitle{Further reading}
    \begin{itemize}
    \item $k$-anonymity \cite{samarati1998protecting}
    \item Randomness, privacy, and the US 2020 census \cite{garfinkel2020randomness}
    \item The paper introducing differential privacy \cite{dwork2006calibrating}
    \item Differential privacy book \cite{dwork2014algorithmic}
    \item Bayesian inference and privacy \cite{bayesiandp}
    \item Local differential privacy and statistics \cite{duchi2013local}
    \item Local differential privacy and applications \cite{yang2020local}
    \item The exponential mechanism \cite{mcsherry2007mechanism}
      
    \end{itemize}
  \end{frame}
}

\only<article>{


}


\only<article>{
  \section{Exercises}
  \begin{exercise}
    Show that the randomised response mechanism, as originally defined, is not differentially private with respect to the addition/deletion neighbourhood. \label{exer:rp-neighbourhood}
  \end{exercise}
  \ifdefined\solution
    \begin{proof}
      Let $x_1, x_2, x_3$ be three datasets with
      $x_1 N x_2 N x_3$ and $x_1 N_e x_3$.  First of all, an
      algorithm that is $\epsilon$-DP with respect to the $N$, is
      $2\epsilon$-DP with respect to $N_e$.  This follows from the fact
      that
      $\pol(a | x_1) \leq e^\epsilon \pol( a | x_2) \leq
      e^{2\epsilon} \pol( a | x_3)$.  What about the converse?
      Consider the case where $x_1$ has $n$ entries and $x_2$ has
      $n+1$ entries. Let $A_n$ be the set of all responses with $n$
      entries. Then clearly $\pi(A_n | x_1) = 1$ and
      $\pi(A_{n} | x_2) = 0$. Consequently
      $|\ln\frac{\pi(A_n | x_1)}{\pi(A_{n} | x_2)}|$ is not bounded by
      any constant, and differential privacy is not satisfied with
      respect to insertions and deletions.
    \end{proof}
  \fi
  \begin{exercise}
    Define a variant of the binary randomised response mechanism that
    satisfies differential privacy with respect to the insertion and
    deletion neighbourhood. More precisely, it should hold that for
    any dataset pair $x = (x_1, \ldots, x_n)$, and
    $x' = (x_1, \ldots, x_n)$, $x = (x_1, \ldots, x_n, x_{n+1})$,
    the mechanism satisfies
    \[
      \pol(A | x) \leq e^\epsilon \pol(A | x'),
      \qquad \forall A \subset \CA.
    \]
    Use the assumption that there is a fixed population of size $N$ that we can sample from, i.e. that $n \leq N$.
    \label{exer:new-rp-neighbourhood}
  \end{exercise}
  \iftrue
    \begin{proof}
      First of all, for the mechanism to satisfy $\epsilon$-DP, it must be the case
      that an output size must have a non-zero probability: If with an
      $n$-size input we can only generate $\leq n$-size outputs, then we
      cannot generate a $n+1$-size output. Consequently, the randomised
      response mechanism defined for $\neigh$-neighbourhoods makes sense only
      when we know the maximum possible value of $n$. This is normally
      the case when we are surveying a population whose size is assumed
      to be common knowledge.

      In particular, consider a mechanism which samples person $i$ with
      probability $q$ so that $s_i = 1$ if a person is selected. If a
      person is selected, then $a_i$ is drawn from a standard randomised
      response mechanism (which flips $x_i$ with probability $p$),
      otherwise $a_i = \perp$.  Then, we can factorise the probability
      of a specific output as follows:
      \[
        \pol(a_i = k | x_i = k) =  \pol(a_i = k | s_i = 1, x_i = k) 
      \]
      % Note that subset selection of this form is very similar to DP in bandits.
    \end{proof}
  \fi
  \begin{exercise}
    The mean estimator $\hat{\theta}$ for the randomised response mechanism with flipping probability $p$, where the true data generation process is Bernoulli with parameter $\theta$, is defined as
    \[
      \hat{\theta}(a) = \frac{ar{a} - p}{1 - 2p},
    \]
    where $ar{a} = \frac{1}{T} \sum_{t=1}^T a_t$ is the mean of the observed answers. Prove that this estimator is unbiased, i.e. that
    \[
      \E[\hat{\theta}] = \theta,
    \]
    where the expectation is taken over the unobserved data randomness and the randomness of the mechanism.
    \label{exer:rp-unbiased-estimator}
  \end{exercise}
  
  \begin{exercise}
    Prove that the exponential mechanism is uniform when $\epsilon \to 0$ and deterministically returns the maximising action when $\epsilon \to \infty$.
    \label{exer:exp-randomness}
  \end{exercise}

  \begin{exercise}
    Prove that the exponential mechanism, when we used to calculate a noisy version of the function $q(x)$ with $q : \CX \to \Reals$ and utility $U(a,q,x) = -|q(x) - a|^p$, results in the Laplace mechanism for $p=1$ and the Gaussian mechanism for $p = 2$.
    \label{exer:exp-laplace-gauss}
  \end{exercise}

  \begin{exercise}
    Consider the following relaxation of differential privacy to KL divergences. A mechanism is $\epsilon$-KL-private if
    \[
      \sum_{a} \ln \frac{\pol(a | x)}{\pol(a | x')} \pol(a | x) \leq \epsilon
    \]
    Prove that two-folded composition of such a mechanism is $2\epsilon$-KL-private.
    \label{exer:kl-dp}
  \end{exercise}
  \iffalse
    \begin{proof}
      For simplicity, we compose the same mechanism twice over a finite alphabet. Then $\pol(a, a' | x) = \pol(a | x) \pol(a'|x)$ because the mechanism is not interactive. 
      \begin{align*}
        \sum_{a,a'} \ln \frac{\pol(a, a' | x)}{\pol(a, a' | x')} \pol(a, a' | x)
        &=
          \sum_{a,a'} \ln \frac{\pol(a | x) \pol(a' | x)}{\pol(a | x') \pol(a' | x')} \pol(a | x) \pol( a' | x)
        \\
        &=
          \sum_{a,a'} \ln \frac{\pol(a | x)}{\pol(a | x')} \pol(a | x) \pol( a' | x) + \ln \frac{\pol(a' | x)}{\pol(a' | x')} \pol(a | x) \pol( a' | x)
        \\
        &=
          \sum_{a'} \pol(a'|x) \sum_a \ln \frac{\pol(a | x)}{\pol(a | x')} \pol(a | x) + \sum_{a} \pol(a | x) \sum_{a'} \ln \frac{\pol(a' | x)}{\pol(a' | x')}  \pol( a' | x)
        \\
        &\leq
          \sum_{a'} \pol(a'|x) \epsilon + \sum_{a} \pol(a | x) \epsilon
          = 2\epsilon.
      \end{align*}
      
    \end{proof}
  \fi
  \begin{exercise}
    Why is the relaxation of differential privacy to KL divergences not useful? In what way is standard differential privacy stronger?
    \emph{Hint: Reason about what an adversary can learn about the data from the output of the mechanism.}
    \label{exer:kl-dp}
  \end{exercise}
  \iffalse
    \begin{proof}
      While both KL-DP and DP can be defined divergences between distributions, there is one difference. DP is defined over the maximum of all events. Consequently, it gives us a guarantee on the amount of information given no matter what the mechanism generates. KL-DP on the other hand, only gives a bound on the \emph{expected} information gain of an adversary.
    \end{proof}
  \fi

  
  
  
}
%%% Local Variables:
%%% mode: latex
%%% TeX-engine: xetex
%%% TeX-master: "book"
%%% End:

