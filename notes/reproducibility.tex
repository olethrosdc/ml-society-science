\chapter{Reproducibility}
\label{ch:reproducibility}

\section{Reproducibility}
\only<presentation>{
  \begin{frame}
    \tableofcontents[ 
    currentsection, 
    hideothersubsections, 
    sectionstyle=show/shaded
    ] 
  \end{frame}
}
\begin{frame}
  \only<article>{ One of the main problems in science is
    reproducibility: when we are trying to draw conclusions from one
    specific data set, it is easy to make a mistake. For that reason,
    the scientific process requires us to use our conclusions to make
    testable predictions, and then test those predictions with new
    experiments. These new experiments should bear out the results of
    the previous experiments.  In more detail, reproducibility can be
    thought of as two different aspects of answering the question ``can this research be replicated?''}


  \begin{block}{Computational reproducibility: Can the study be repeated?}
    Can we, from the available information and data, exactly reproduce the reported methods and results?

    \only<article>{This is something that is useful to be able to even to the original authors of a study. The standard method for achieving this is using version control tools so that the exact version of algorithms, text and data used to write up the study is appropriately labelled. Ideally, any other researcher should be able to run a single script to reproduce all of the study and its computations. The following tools are going to be used in this course:}

    \begin{itemize}
    \item \texttt{jupyter} notebooks \only<article>{for interactive note taking.}
    \item \texttt{svn}, \texttt{git} or \texttt{mercurial} version control systems \only<article>{for tracking versions, changes and collaborating with others.}
    \end{itemize}
  \end{block}

  \begin{block}{Scientific reproducibility: Is the conclusion correct?}
    Can we, from the available information and a \alert{new} set of data, reproduce the conclusions of the original study?

     \only<article>{Here followup research may involve using exactly the same methods. In AI research would mean for example testing whether an algorithm is really performing as well as it is claimed, by testing it in new problems. This can involve a re-implementation. In more general scientific research, it might be the case that the methodology proposed by an original study is flawed, and so a better method should be used to draw better conclusions. Or it might simply be that more data is needed.}
  \end{block}
 
  When publishing results about a \alert{new method}, computational reproducibility is essential for scientific reproducibility.
\end{frame}


\begin{frame}

  \includegraphics[width=\textwidth]{../figures/2016-election}
  \only<article>{A simple example is the 2016 presidential election in the United States, where polling forecasts were not able to predict the outcome. In general, while we can make models about people's opinions regarding candidates in order to predict voting totals, the test of these models comes in the actual election. Unfortunately the only way we have of tuning our models is on previous elections, which are not that frequent, and on the results of previous polls. In addition, predicting the winner of an election is slightly different from predicting how many people are prepared to vote for them across the country. This, together with other factors such as shifting opinions, motivation and how close the sampling distribution is to the voting distribution have a significant effect on accuracy.}
  
\end{frame}
\begin{frame}
  \only<article>{The same thing can be done in when dealing purely
    with data, by making sure we use some of the data as input to the
    algorithm, and other data to measure the quality of the algorithm
    itself. In the following, we assume we have some algorithm
    $\alg : \Datasets \to \Pol$, where $\Datasets$ is the universe of
    possible input data and $\Pol$ the possible outputs, e.g. all
    possible classification policies. We also assume the existence of
    some quality measure $U$. How should we measure the quality of our
    algorithmic choices? 
    
    Take classification as an example. For a given training set, simply memorising all the labels of each example gives us perfect performance on the training set. Intuitively, this is not a good measure of performance, as we'd probably get poor results on a freshly sampled set. We can think of the training data as input to an algorithm, and the resulting classifier as the algorithm output. The evaluation function also requires some data in order to measure the performance of the policy. This can be expressed into the following principle.
  }
  \begin{alertblock}{The principle of independent evaluation}    
    Data used for estimation cannot be used for evaluation.
  \end{alertblock}
  \only<article>{This applies both to computer-implemented and human-implemented algorithms.}
\end{frame}


\begin{frame}
  \begin{figure}[H]
    \begin{center}
      \begin{tikzpicture}[line width=2pt]
        \onslide<1->{
          \node[select,label=above:Data Collection] at (0,2) (experiment) {$\chi$};
        }
        \onslide<2->{
          \node[RV,label=above:Training] at (4,2) (training) {$\Training$};
          \draw[blue,->] (experiment) -- (training);
        }
        \onslide<3->{
          \node[select,label=below:{Algorithm, hyperparameters}] at (0,0) (alg) {$\alg$};
        }
        \onslide<4->{
          \node[RV,label=below:Classifier] at (4,0) (pol) {$\pol$};
          \draw[red,->,dashed] (experiment) -- (pol);
          \draw[red,->] (alg) -- (pol);
          \draw[red,->] (training) -- (pol);
        }
        \onslide<5->{
          \node[RV,label=above:Holdout] at (8,2) (holdout) {$\Holdout$};
        }
        \onslide<6->{
          \draw[blue,->] (experiment) to [bend left=45] (holdout);
        }
        \onslide<7->{
          \node[utility,label=below:Measurement] at (8,0) (util) {$\util$};
          \draw[red,->] (pol) -- (util);
          \draw[red,->] (holdout) -- (util);
        }
      \end{tikzpicture}
    \end{center}
    \caption{The decision process in classification.}
  \end{figure}
  \only<article>{One can think of the decision process in classification as follows. First, we decide to collect some data according to some experimental protocol $\chi$. We also decide to use some algorithm (with associated hyperparameters) $\alg$ together with data $\Training$ we will obtain from our data collection in order to obtain a classification policy $\pol$.\index{policy!classification} Typically, we need to measure the quality of a policy according to how well it classifies on unknown data. This is because our policy has been generated using $\Training$, and so any measurement of its quality using the same data $\Training$, is going to be biased.

    \paragraph{Classification accuracy.} For classification problems, there is a natural metric $U$ to measure: The classification accuracy of the classifier. If the classification decisions are stochastic, then the classifier assigns probability $\pol(a \mid x)$ to each possible label $a$, and our utility is simply the identity function $U(a, y) \defn \ind{a = y}$. 
  }
  \uncover<8->{
    \begin{block}{Classification accuracy}
      \only<article>{
        For any fixed classifier $\pol$, the classification accuracy under the data distribution $\chi$ is:}
      \only<8>{
      \[
      \E_\chi[\util(\pol)] = \sum_{x,y} \underset{\textrm{Data probability}}{\underbrace{\Pr_\chi(x, y)}} \overset{\textrm{Decision probability}}{\overbrace{\pol(a = y \mid x)}}
      \]
      }
      \only<article>{The classification accuracy of policy $\pol$ under $\chi$ is the expected number of times the policy decides $\pol$ chooses the correct class. However, when approximating $\chi$ with a sample $\Holdout$, we instead obtain the empirical estimate:}\index{policy!classification accuracy}
      \only<9>{
      \[
      \E_{\Holdout} \util(\pol) = \sum_{(x,y) \in \Holdout} \pol(a = y \mid x) / |\Holdout|.
      \]
      }
    \end{block}
  }
  \only<article>{Of course, there is no reason to limit ourselves to the identity function. The utility could very well be such that some errors are penalised more than other errors. Consider for example an intrusion detection scenario: it is probably more important to correctly classify intrusions. This can be taken into account by weighing correct decisions about intrusions more relative to correct decisions about normal data.  }
\end{frame}

\subsection{The human as an algorithm}
\begin{frame}
  \frametitle{The human as an algorithm.}
  \only<article>{The same way with which an algorithm creates a model from some prior assumptions and data, so can a human select an algorithm and associated hyperparamters by executing an algorithm herself. This involves trying different algorithms and hyperparametrs on the same training data $\Training$ and then measuring their performance in the holdout set $\Holdout$.}
  \begin{figure}[H]
  \centering
  \begin{tikzpicture}[line width=2pt]
    \node[select,label=above:Data Collection] at (0,2) (experiment) {$\chi$};
    \node[RV,label=above:Training] at (4,2) (training) {$\Training$};
    \node[RV,label=above:Holdout] at (8,2) (holdout) {$\Holdout$};
     \draw[blue,->] (experiment) -- (training);
    \draw[blue,->] (experiment) to [bend left=45] (holdout);
    \node<2->[select,label=below:{Algorithm, hyperparameters}] at (0,0) (alg) {$\alg_1$};     
    \node<3->[RV,label=below:Classifier] at (4,0) (pol) {$\pol_1$};
    \draw<3->[red,->] (alg) -- (pol);
    \draw<3->[red,->] (training) -- (pol);
    \node<4->[utility,label=below:Measurement] at (8,0) (util) {$\util_1$};
    \draw<4->[red,->] (pol) -- (util);
    \draw<4->[red,->] (holdout) -- (util);
    \node<5->[select,label=below:{Algorithm, hyperparameters}] at (0,-2) (alg2) {$\alg_2$};
    \node<6->[RV,label=below:Classifier] at (4,-2) (pol2) {$\pol_2$};
    \node<7->[utility,label=below:Measurement] at (8,-2) (util2) {$\util_2$};
    \draw<6->[red,->] (alg2) -- (pol2);
    \draw<6->[red,->] (training) to [bend left] (pol2);
    \draw<7->[red,->] (pol2) -- (util2);
    \draw<7->[red,->] (holdout) to [bend right] (util2);
  \end{tikzpicture}
    \caption{Selecting algorithms and hyperparameters through holdouts}
    \label{fig:human-as-algorithm}
  \end{figure}

\end{frame}

\begin{frame}
  \frametitle{Holdout sets}
  \only<article>{To summarise, holdout sets are used in order to be able to evaluate the performance of specific algorithms, or hyparameter selection.}
 \only<presentation>{
  \begin{itemize}
  \item Original data $\Data$, e.g. $\Data = (x_1, \ldots, x_T)$.
  \item Training data $\Training \subset \Data$, e.g. $\Training = x_1, \ldots, x_n$, $n < T$.
  \item Holdout data $\Holdout = D \setminus \Training$, used to measure the quality of the result.
  \item Algorithm $\alg$ with hyperparametrs $\hyperparam$.
  \item Get algorithm output $\pol = \alg(\Training, \hyperparam)$.
  \item Calculate quality of output $U(\pol, \Holdout)$
  \end{itemize}
  }
  \only<article>{
    We start with some original data $\Data$, e.g. $\Data = (x_1, \ldots, x_T)$. We then split this into a training data set $\Training \subset \Data$, e.g. $\Training = x_1, \ldots, x_n$, $n < T$ and holdout dataset $\Holdout = D \setminus \Training$. This is used to measure the quality of selected algorithms $\alg$ and hyperparameters $\hyperparam$. We run an algorithm/hyperparameter combination on the training data and obtain a result $\pol = \alg(\Training, \hyperparam)$.    \footnote{As typically algorithms are maximising the quality metric on the training data, 
    \[
    \alg(\Training) = \argmax_y U(y, \Training)
    \]
    we typically obtain a biased estimate, which depends both on the algorithm itself and the training data. For \KNN{} in particular, when we measure accuracy on the training data, we can nearly always obtain near-perfect accuracy, but not always perfect. Can you explain why?}
 We then calculate the quality of the output $U(\pol, \Holdout)$ on the holdout set.
    Unfortunately, the combination that appears the best due to the holdout result may look inferior in a fresh sample. Following the principle of ``data used for evaluation cannot be used for estimation'', we must measure performance on another sample.
This ensures that we are not biased in our decision about what is the best algorithm.
  }
  \begin{block}{Holdout and test sets for unbiased algorithm comparison}
    \only<article>{
      Consider the problem of comparing a number of different algorithms in $\Alg$. Each algorithm $\alg$ has a different set of hyperparameters $\Hyperparam_\alg$. The problem is to choose the best parameters for each algorithm, and then to test them independently. A simple meta-algorithm for doing this is based on the use of a \emph{holdout} set for choosing hyperparameters for each algorithm, and a \emph{test} set to measure algorithmic performance.}
    \begin{algorithm}[H]
      \begin{algorithmic}
        \State Partition data into $\Training, \Holdout, \Testing$.
        \For {$\alg \in \Alg$} \For
        {$\hyperparam \in \Hyperparam_\alg$} \State
        $\pol_{\hyperparam, \alg} = \alg(\Training, \hyperparam)$.
        \EndFor
        \State Get $\pol^*_{\alg}$ maximising
        $\util(\pol_{\hyperparam, \alg}, \Holdout)$.  \State
        $u_\alg = \util(\pol^*_\alg, \Testing)$.
        \EndFor
        \State $\alg^* = \argmax_\alg u_\alg$.
      \end{algorithmic}
      \caption{Unbiased adaptive evaluation through data partitioning}
    \end{algorithm}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Final performance measurement}
  \only<article>{When comparing many algorithms, where we must select a hyperparameter for each one, then we can use one dataset as input to the algorithms, and another for selecting hyperparameters. That means that we must use another dataset to measure performance. This is called the testing set. Figure~\ref{fig:final-measurement} illustrates this.}
  \begin{figure}[H]
    \centering
  \begin{tikzpicture}[line width=2pt]
    \node[select,label=above:Data Collection] at (0,0) (experiment) {$\chi$};
    \node[RV,label=above:Training] at (2,0) (training) {$\Training$};
    \node[RV,label=above:Holdout] at (4,0) (holdout) {$\Holdout$};
    \node[RV,label=above:Testing] at (6,0) (testing) {$\Testing$};
    \draw[blue,->] (experiment) -- (training);
    \draw[blue,->] (experiment) to [bend left=15] (holdout);
    \draw[blue,->] (experiment) to [bend left=45] (testing);
    \node[select,label=below:{human}] at (0,-2) (human) {$\eta$};
    \node[RV,label=below:Algorithm 1] at (2,-2) (alg1) {$\alg_1$};
    \node[RV,label=below:Classifier 1] at (4,-2) (pol1) {$\pol_1$};
    \node[RV,label=below:Result 1] at (6,-2) (util1) {$\util^*_1$};
    \node[RV,label=below:Algorithm 2] at (2,-4) (alg2) {$\alg_2$};
    \node[RV,label=below:Classifier 2] at (4,-4) (pol2) {$\pol_2$};
    \node[RV,label=below:Result 2] at (6,-4) (util2) {$\util^*_2$};
    \draw[->,red] (human) -- (alg1);
    \draw[->,red] (alg1) -- (pol1);
    \draw[->,red] (pol1) -- (util1);
    \draw[->,red] (human) -- (alg2);
    \draw[->,red] (alg2) -- (pol2);
    \draw[->,red] (pol2) -- (util2);
    \draw[->,red] (training) -- (pol1);
    \draw[->,red] (holdout) -- (pol1);
    \draw[->,red] (testing) -- (util1);
    \draw[->,red] (training) -- (pol2);
    \draw[->,red] (holdout) to [bend left = 45] (pol2);
    \draw[->,red] (testing) to [bend left = 45] (util2);
  \end{tikzpicture}
  \only<article>{
    \caption{Simplified dependency graph for selecting hyperparameters for different algorithms, and comparing them on an independent test set. For the $i$-th algorithm, the classifier model is }
  }
  \label{fig:final-measurement}
\end{figure}
\end{frame}

\subsection{Algorithmic sensitivity}
\only<article>{The algorithm's output does have a dependence on its input, obviously. So, how sensitive is the algorithm to the input?}
\begin{frame}
  \frametitle{Independent data sets}
  \only<article>{One simple idea is to just collect independent datasets and see how the output of the algorithm changes when the data changes. However, this is quite expensive, as it not might be easy to collect data in the first place.}
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[line width=2pt]
      \node[select,label=above:Experiment] at (0,0) (experiment) {$\chi$};
      \node[select,label=below:{Algorithm}] at (8,0) (alg) {$\alg$};
      \node[RV,label=below:1st sample] at (4,0) (sample1) {$D_1$};
      \node[RV,label=below:1st Result] at (6,0) (pol1) {$\pol_1$};
      \node<2>[RV,label=above:2nd Sample] at (4,2) (sample2) {$D_2$};
      \node<2>[RV,label=above:2nd Result] at (6,2) (pol2) {$\pol_2$};
      \draw[blue,->] (experiment) -- (sample1);
      \draw[red,->] (alg) -- (pol1);
      \draw[red,->] (sample1) -- (pol1);
      \draw<2>[blue,->] (experiment) -- (sample2);
      \draw<2>[red,->] (alg) -- (pol2);
      \draw<2>[red,->] (sample2) -- (pol2);
    \end{tikzpicture}
    \caption{Multiple samples}
    \label{fig:multiple-samples}
  \end{figure}
\end{frame}
\begin{frame}
  \frametitle{Bootstrap samples}
  \only<article>{A more efficient idea is to only collect one dataset, but then use it to generate more datasets. The simplest way to do that is by sampling with replacement from the original dataset, new datasets of the same size as the original. Then the original dataset is sufficiently large, this is approximately the same as sampling independent datasets. As usual, we can evaluate our algorithm on an independent data set.}
  \begin{figure}[H]
  \centering
  \begin{tikzpicture}[line width=2pt]
    \node[select,label=above:Experiment] at (0,0) (experiment) {$\chi$};
    \node[RV,label=below:training] at (2,0) (training) {$\Training$};
    \draw[blue,->] (experiment) -- (training);
    \node[select,label=below:{Algorithm}] at (8,0) (alg) {$\alg$};
    \node[RV,label=below:1st sample] at (4,0) (sample1) {$D_1$};
    \node[RV,label=below:1st Result] at (6,0) (pol1) {$\pol_1$};
    \node[RV,label=above:2nd Sample] at (4,2) (sample2) {$D_2$};
    \node[RV,label=above:2nd Result] at (6,2) (pol2) {$\pol_2$};
    \draw[red,->] (alg) -- (pol1);
    \draw[red,->] (sample1) -- (pol1);
    \draw[red,->] (training) -- (sample1);
    \draw[red,->] (training) -- (sample2);
    \draw[red,->] (alg) -- (pol2);
    \draw[red,->] (sample2) -- (pol2);
  \end{tikzpicture}
  \caption{Bootstrap replicates of a single sample}
  \label{fig:bootstrap}
\end{figure}
\end{frame}



\begin{frame}
  \frametitle{Bootstrapping}
  Bootstrapping is a general technique that can be used to:
  \begin{itemize}
  \item Estimate the sensitivity of $\alg$ to the data $x$.
  \item Obtain a distribution of estimates $\pol$ from $\alg$ and the data $x$.
  \item When estimating the performance of an algorithm on a small dataset $\Testing$, use bootstrap samples of $\Testing$. \only<article>{This allows us to take into account the inherent uncertainty in measured performance. It is very useful to use bootstrapping with pairwise comparisons.}
  \end{itemize}
  \begin{block}{Bootstrapping}
    \begin{enumerate}
    \item \textbf{Input} Training data $\Data$, number of samples $k$.
    \item \textbf{For} $i = 1, \ldots, k$
    \item \quad $\Data^{(i)} = \textrm{Bootstrap}(\Data)$
    \item \textbf{return} $\cset{\Data^{(i)}}{i = 1, \ldots, k}$.
    \end{enumerate}
    where  $\textrm{Bootstrap}(\Data)$ samples with replacement $|\Data|$ points from $\Training$.
  \end{block}

\only<article>{
  In more detail, remember that even though the test score is an \emph{independent} measurement of an algorithm's performance, it is \emph{not} the actual expected performance. At best, it's an unbiased estimate of performance. Hence, we'd like to have some way to calculate a likely performance range from the test data. Bootstrapping can help: by taking multiple samples of the test set and calculating performance on each one, we obtain an empirical distribution of scores.

Secondly, we can use it to tell us something about the sensitivity of our algorithm. In particular, by taking multiple samples from the training data, we can end up with multiple models. If the models are only slightly different, then the algorithm is more stable and we can be more confident in its predictions. 

Finally, bagging also allows us to generate probabilistic predictions from deterministic classification algorithms, by simply averaging predictions from multiple bootstrapped predictors. This is called \emph{bagging predictors}\cite{breiman96bagging}.}
\end{frame}


\begin{frame}
  \frametitle{Cross-validation}
  \only<article>{While we typically use a single training, hold-out and test set, it might be useful to do this multiple times in order to obtain more robust performance estimates. In the simplest case, cross-validation can be used to obtain multiple training and hold-out sets from a single dataset. This works by simply partitioning the data in $k$ \emph{folds} and then using one of the folds as a holdout and the remaining $k - 1$ as training data. This is repeated $k$ times. When $k$ is the same size as the original training data, then the method is called \emph{leave-one-out cross-validation.}}
  \begin{block}{$k$-fold Cross-Validation}
    \begin{algorithm}
      \STATE x
    \end{algorithm}
    \begin{enumerate}
    \item \textbf{Input} Training data $\Training$, number of folds $k$, algorithm $\alg$, measurement function $\util$
    \item Create the partition $\Data^{(1)} \ldots, \Data^{(k)}$ so that $\bigcup_{i=1}^k \Data^{(k)} = \Data$.
    \item Define $\Training^{(i)} = \Data \setminus \Data^{(i)}$
    \item \quad $\pol_i = \alg(\Training^{(i)})$
    \item \textbf{For} $i = 1, \ldots, k$:
    \item \quad $\pol_i = \alg(\Data^{(i)})$
    \item \quad $u_i = \util(\pol_i)$
    \item \textbf{return} $\{y_1, \ldots, y_i\}$.
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Online evaluation}
  \only<article>{
    We can get around this problem if we consider online evaluation of learning algorithms. This means that the learning algorithm is always evaluated on previously unseen data. However, when new data is seen, it can be used by the algorithm to learn. 
  }
  \begin{example}{Online prediction accuracy}
    \begin{itemize}
    \item Adaptive decision rule $\pol$
    \item At time $t$
      \begin{enumerate}
      \item $\pol$ predicts $a_t$
      \item The true data $x_t$ is observed and we see whether $a_t = y_t$.
      \item $\pol$ adapts to the new data $x_t$
      \end{enumerate}
    \end{itemize}
    \only<article>{For this example, you can consider the decision rule $\pol$ as being conditioned on the previous data, i.e.
      \[
      \pol(a_t \mid x_1, \ldots, x_t)
      \]
    }
  \end{example}
\end{frame}

\subsection{Beyond the data you have: simulation and replication}
\only<article>{
In the end, however, you are always limited by the data you actually have. The more you tweak your models and algorithms to improve performance with your current dataset, the more you are simply engineering a solution that is adapted to the specific data. This may not generalise well, even if you are using cross-validation or bootstrapping every step of the way. How can you then make sure that your methodology is robust? 

The first method is simply to simulate data from an artificial process, where the ground truth (e.g. the labels) is known, and where the dataset size and dimensionality is similar to the one you have.  You can use this to see whether the overall process is robust, and this can give you confidence that you will get reasonable results when using real data.
The second method requires actually collecting new data, and repeating the study. If the results can be replicated, the original study was not a fluke.


}
\begin{frame}
\frametitle{Simulation}
\only<article>{Simulation can be extremely useful. It allows you to examine the performance of various methods as you change aspect of the data-generating process without ever having to look at the data in detail. Since the data is synthetically generated, you always know the ground truth, so you know precisely how good your methods are going to be. This is useful in particular when you want to perform a null hypothesis test, and want to see under which conditions you actually accept or reject a null hypothesis.

A good example of the use of simulation to validate a method is in the article by~\citet{Bennett2009ThePC} where they discuss the use of corrections for multiple comparison tests. This followed their study of uncorrected methods for fMRI analysis~\citep{bennett2012journal}, where they found that commonly used such methods would detect meaningful brain activity in a dead salmon. They use simulation to select a correction method that would be neither too conservative (i.e. not detecting any significant brain activity) nor too sensitive (i.e. detecting activity where there is none). 
}
\begin{block}{Steps for a simulation pre-study}
  \begin{enumerate}
  \item \textbf{Criteria}: Define your quality criteria, e.g. a utility function $\util$.
  \item \textbf{Algorithms}: Define the class of algorithms, models or to consider.
  \item \textbf{Simulation}: Create a simulation that allows you to collect data similar to the real one.
  \item \textbf{Protocol}: This defines the processing and model pipeline, as you would do it with the actual data.
  \item \textbf{Pre-study}: Collect data from the simulation and analyse it with the created according to your protocl.
  \item \textbf{Adjustment:} If the results are not as expected, alter the simulation, protocol etc as needed. The aim is to find a \emph{protocol} for the pre-study that would lead to (a) reproducible results (b) allows you to analyse the behaviour of different algorithms under different assumptions.
  \item \textbf{Use:} Apply the protocol to actual data, whether it has been already collected, or will be collected according to the protocol.
  \end{enumerate}
\end{block}
\end{frame}



\begin{frame}
\frametitle{Independent replication}
\only<article>{The gold standard for reproducibility is independent replication. Simply have another team try and reproduce the results you obtained, using completely new data. If the replication is successful, then you can be pretty sure there was no flaw in your original analysis. In typical scientific publishing, the replication study is done by a different set of authors than those of the original study.}
\begin{block}{Replication study}
  \begin{enumerate}
  \item Reinterpret the original hypothesis and experiment.
  \item Collect data according to the original protocol, \alert{unless flawed}. \only<article>{It is possible that the original experimental protocol had flaws. Then the new study should try and address this through an improved data collection process. For example, the original study might not have been double-blind. The new study can replicate the results in a double-blind regime.}
  \item Run the analysis again, \alert{unless flawed}. \only<article>{It is possible that the original analysis had flaws. For example, possible correlations may not have been taken into account.}
  \item See if the conclusions are in agreement.
  \end{enumerate}
\end{block}
\end{frame}

\begin{frame}
  \frametitle{Learning outcomes}
  \begin{block}{Understanding}
    \begin{itemize}
    \item What is a hold-out set, cross-validation and bootstrapping.
    \item The idea of not reusing data input to an algorithm to evaluate it.
    \item The fact that algorithms can be implemented by both humans and machines.
    \end{itemize}
  \end{block}
  
  \begin{block}{Skills}
    \begin{itemize}
    \item Use git and notebooks to document your work.
    \item Use hold-out sets or cross-validation to compare parameters/algorithms in Python.
    \item Use bootstrapping to get estimates of uncertainty in Python.
    \end{itemize}
  \end{block}

  \begin{block}{Reflection}
    \begin{itemize}
    \item What is a good use case for cross-validation over hold-out sets?
    \item When is it a good idea to use bootstrapping?
    \item How can we use the above techniques to avoid the false discovery problem?
    \item Can these techniques fully replace independent replication?
    \end{itemize}
  \end{block}
  
\end{frame}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "notes"
%%% End:

