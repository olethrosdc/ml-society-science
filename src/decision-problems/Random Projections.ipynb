{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden layers and random projections\n",
    "\n",
    "Neural networks hold the promise of being able to perform very well in highly complex classification problems. It is frequently claimed that 'deep' neural networks do that by virtue of their large number of hidden layers. Indeed, the number of layers is important.\n",
    "\n",
    "## The Perceptron\n",
    "\n",
    "One of the first neural networks, the perceptron, had no hidden layer and was thus a simple linear classifier. This meant that it could only correctly classify linearly separable classes. After this was recognised, neural network research was abandoned for a long time.\n",
    "\n",
    "## MLPs\n",
    "\n",
    "Multi-Layer Perceptrons simply added more /non-linear/ layers. This allowed them to separate arbitrary classes. However, one large problem was the complexity of adjusting the parameters of the network. The most commonly used methods employ some sort of stochastic gradient descent.\n",
    "\n",
    "## Random projections\n",
    "\n",
    "One can think of each layer in the network as projecting the original data to some other space, with the last layer projecting it to the final labelling space. The idea of random projections is simply to project the data using a /random/ transformation. This avoids having to estimate the right parameters for it. In practice, RPs perform quite well: for large dimension projections, they male classes separable. If the initial features are very high dimensional, then projecting to a smaller dimensionality space randomly has good guarantees according to the Johnson-Lindenstrauss lemma https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma\n",
    "\n",
    "## DNNs\n",
    "\n",
    "However, sometimes we'd like our hidden layers to have more /meaning/ than just be random. This is especially the case in complex architectures where we might want to perform multiple tasks. For example in vehicular vision applications, you'd like to do background, road and object detection, weather recognition simultaneously. Rather than training one network for each task, they can share the first few layers and only use one specialised output layer for each task. The idea is that somehow you would be able to learn a globally useful representation in the first few layers: which is why a lot of research in DNNs is about learning representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Training result\n",
      "KNN score: 0.802817\n",
      "LogisticRegression score: 0.718310\n",
      "MLP score: 0.401408\n",
      "Testing result\n",
      "KNN score: 0.694444\n",
      "LogisticRegression score: 0.750000\n",
      "MLP score: 0.388889\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "import pandas\n",
    "from sklearn import datasets, neighbors, linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "features = [\"Alcohol\", \"Malic acid\", \"Ash\", \"Alcalinity of ash\",\n",
    "    \"Magnesium\", \"Total phenols\", \"Flavanoids\", \"Nonflavanoid phenols\",\n",
    "    \"Proanthocyanins\", \"Color intensity\", \"Hue\",\n",
    "    \"OD280/OD315 of diluted wines\", \"Proline\"]\n",
    "\n",
    "\n",
    "target = 'Class'\n",
    "\n",
    "df = pandas.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', names=[target] + features)\n",
    "train_data_s, test_data_s = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Reduce number of features used\n",
    "features = [\"Alcohol\", \"Malic acid\", \"Ash\", \"Alcalinity of ash\",\n",
    "    \"Magnesium\"]\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=3).fit(train_data_s[features], train_data_s[target])\n",
    "score = accuracy_score(test_data_s[target], model.predict(test_data_s[features]))\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "knn.fit(train_data_s[features], train_data_s[target])\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(train_data_s[features], train_data_s[target])\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(10, 5, 2), random_state=1)\n",
    "mlp.fit(train_data_s[features], train_data_s[target])\n",
    "\n",
    "\n",
    "print ('Training result')\n",
    "print('KNN score: %f' % knn.score(train_data_s[features], train_data_s[target]))\n",
    "print('LogisticRegression score: %f' %logistic.score(train_data_s[features], train_data_s[target]))\n",
    "print(\"MLP score: %f\" % mlp.score(train_data_s[features], train_data_s[target]))\n",
    "\n",
    "\n",
    "print ('Testing result')\n",
    "print('KNN score: %f' % knn.score(test_data_s[features], test_data_s[target]))\n",
    "print('LogisticRegression score: %f' % logistic.score(test_data_s[features], test_data_s[target]))\n",
    "print('MLP score: %f' % mlp.score(test_data_s[features], test_data_s[target]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random projections\n",
    "\n",
    "A random projection is simply a function which has been randomly selected. In this particular case, the projection has the form $$f(x) = \\tanh(A x) $$ with $A$ being a matrix with Gaussian-distributed entries. Here $x \\in R^n$ and $A \\in R^{m \\times n}$. If $m < n$ then the projection compresses the input. If on the other hand $m >n$, the input is expanded. This means that even very simple models can classify the training data perfectly when $m$ is large enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training result\n",
      "KNN score: 0.626761\n",
      "LogisticRegression score: 0.577465\n",
      "MLP score: 0.605634\n",
      "Testing result\n",
      "KNN score: 0.555556\n",
      "LogisticRegression score: 0.555556\n",
      "MLP score: 0.250000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/random_projection.py:376: DataDimensionalityWarning: The number of components is higher than the number of features: n_features < n_components (5 < 10).The dimensionality of the problem will not be reduced.\n",
      "  DataDimensionalityWarning)\n",
      "/usr/lib/python3/dist-packages/sklearn/random_projection.py:376: DataDimensionalityWarning: The number of components is higher than the number of features: n_features < n_components (5 < 10).The dimensionality of the problem will not be reduced.\n",
      "  DataDimensionalityWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import random_projection\n",
    "import numpy\n",
    "\n",
    "n_p_features = 10\n",
    "transform = random_projection.GaussianRandomProjection(n_p_features)\n",
    "X_train_new = numpy.tanh(transform.fit_transform(train_data_s[features]))\n",
    "X_test_new = numpy.tanh(transform.fit_transform(test_data_s[features]))\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "knn.fit(X_train_new, train_data_s[target])\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X_train_new, train_data_s[target])\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "mlp.fit(X_train_new, train_data_s[target])\n",
    "\n",
    "print ('Training result')\n",
    "print('KNN score: %f' % knn.score(X_train_new, train_data_s[target]))\n",
    "print('LogisticRegression score: %f' %logistic.score(X_train_new, train_data_s[target]))\n",
    "print(\"MLP score: %f\" % mlp.score(X_train_new, train_data_s[target]))\n",
    "\n",
    "\n",
    "print ('Testing result')\n",
    "print('KNN score: %f' % knn.score(X_test_new, test_data_s[target]))\n",
    "print('LogisticRegression score: %f' % logistic.score(X_test_new, test_data_s[target]))\n",
    "print('MLP score: %f' % mlp.score(X_test_new, test_data_s[target]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine performance\n",
    "\n",
    "In this lab, spend some time to gauge the performance of each method, with and without random projections applied. See whether using a bigger and more complex dataset from the UCI repository makes a difference. Can you say that one method is necessarily \"better\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
