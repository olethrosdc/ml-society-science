{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow tutorial\n",
    "\n",
    "In this tutorial we will cover some of the basic consepts of optimization using [tensorflow](https://www.tensorflow.org/) machine learning library.\n",
    "\n",
    "some usefull links:  \n",
    "1. tensorflow guides: https://www.tensorflow.org/guide/basics\n",
    "2. tensorflow tutorials: https://www.tensorflow.org/tutorials/keras/classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Decent\n",
    "\n",
    "**Gradient Desent (GD)** is a interative method to find a local minimum. (Or the global minimum of a **convex function**)\n",
    "\n",
    "**Gradient descent** is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks.\n",
    "\n",
    "In machine learning, Gradient descent is a way to minimize an objective function whit respect to model’s parameters by updating the parameters in the direction stepest descent.\n",
    "\n",
    "<figure>\n",
    "  <img style=\"float: left;\" src=\"./figures/gd.png\" width=\"400\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In gradient decent we change the value of the paremets as follow:\n",
    "    $$ w = w -  ∇_w Loss$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example\n",
    "in the following example we will find the minimum of a function using gradient decent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-474784adfc80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunction_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoordination_config_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/core/framework/function_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattr_value_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_attr__value__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnode_def_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_node__def__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mop_def_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_op__def__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/core/framework/attr_value_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/core/framework/tensor_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresource_handle_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_resource__handle__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/core/framework/resource_handle_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/core/framework/tensor_shape_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mcontaining_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   fields=[\n\u001b[0;32m---> 36\u001b[0;31m     _descriptor.FieldDescriptor(\n\u001b[0m\u001b[1;32m     37\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tensorflow.TensorShapeProto.Dim.size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mnumber\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpp_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/google/protobuf/descriptor.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0mhas_default_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontaining_oneof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m                 file=None, create_key=None):  # pylint: disable=redefined-builtin\n\u001b[0;32m--> 560\u001b[0;31m       \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_CheckCalledFromGeneratedFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mis_extension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFindExtensionByName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"\n",
    "    A convex function\n",
    "    f(x) = (x-5)^2\n",
    "    \"\"\"\n",
    "    y = x**2 - 10*x + 25\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 15,100)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, f(x), label = \"(x-5)^2\")\n",
    "plt.legend()\n",
    "plt.title(\"Convex function\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlabel(\"w\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using tensorflow we can get the gradient of the function at a given point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_var = tf.Variable(1.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = f(weight_var)\n",
    "\n",
    "gradient_w =  tape.gradient(y, weight_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gradient ∇f(x)|x=1 using tensorflow: \", gradient_w.numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    \"\"\"\n",
    "    gradient of f(x).\n",
    "    g(x) = d(f(x))/dx = 2*(x-5)\n",
    "    \"\"\"\n",
    "    dydx = 2*(x-5)\n",
    "    return dydx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gradient ∇f(w)|w=1 using theoritical gradient: \", g(1.0) )\n",
    "print(\"gradient ∇f(w)|w=1 using tensorflow: \", gradient_w.numpy() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update our parameter $w$ to find the minimum of the function we can apply the following formula:  $$ w = w -  \\alpha ∇_w Loss$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_new = weight_var - 0.1 * gradient_w.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x, f(x), label = \"(x-5)^2\")\n",
    "plt.scatter(weight_var, f(weight_var), color= \"green\", label = \"initial w\")\n",
    "plt.scatter(weight_new, f(weight_new), color=\"red\", label = \"updated w\")\n",
    "plt.legend()\n",
    "plt.title(\"Convex function\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply gradient decent algorith\n",
    "\n",
    "![gd.png](./figures/gd_algo.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "weight_var = tf.Variable(-3.0)\n",
    "lr = 0.1\n",
    "step_max = 500\n",
    "thr = 0.01\n",
    "\n",
    "# algorith\n",
    "weight_var = tf.Variable(-3.0)\n",
    "w_history = [weight_var.numpy()]\n",
    "\n",
    "step = 0\n",
    "while ( f(weight_var) > thr ) and (step < step_max):\n",
    "    \n",
    "    # step 1 get the grandient\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = f(weight_var)\n",
    "\n",
    "    # get gradient\n",
    "    gradient_x =  tape.gradient(y, weight_var)\n",
    "    \n",
    "    # w  = w  - lr * gradient_L(w)\n",
    "    weight_var.assign_sub(lr * gradient_x)\n",
    "    \n",
    "    # just keep the values of x\n",
    "    w_history += [weight_var.numpy()]\n",
    "    \n",
    "    step = step +1\n",
    "w_history = np.array(w_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total steps:\", step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.linspace(-5, 15,100)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(w, f(w), label = \"(x-5)^2\")\n",
    "plt.plot(w_history, f(w_history),marker = \"o\", color=\"r\")\n",
    "plt.legend()\n",
    "plt.title(\"Convex function\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "weight_var = tf.Variable(-3.0)\n",
    "lr = 0.001\n",
    "step_max = 500\n",
    "thr = 0.01\n",
    "\n",
    "# algorith\n",
    "weight_var = tf.Variable(-3.0)\n",
    "w_history = [weight_var.numpy()]\n",
    "\n",
    "step = 0\n",
    "while ( f(weight_var) > thr ) and (step < step_max):\n",
    "    \n",
    "    # step 1 get the grandient\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = f(weight_var)\n",
    "\n",
    "    # get gradient\n",
    "    gradient_x =  tape.gradient(y, weight_var)\n",
    "    \n",
    "    # x_var = x_var - lr * gradient_x\n",
    "    weight_var.assign_sub(lr * gradient_x)\n",
    "    \n",
    "    # just keep the values of x\n",
    "    w_history += [weight_var.numpy()]\n",
    "    \n",
    "    step = step +1\n",
    "w_history = np.array(w_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total steps:\", step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.linspace(-5, 15,100)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(w, f(w), label = \"(x-5)^2\")\n",
    "plt.plot(w_history, f(w_history),marker = \"o\", color=\"r\")\n",
    "plt.legend()\n",
    "plt.title(\"Convex function\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lr = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "weight_var = tf.Variable(-3.0)\n",
    "lr = 1.0\n",
    "step_max = 500\n",
    "thr = 0.01\n",
    "\n",
    "# algorith\n",
    "weight_var = tf.Variable(-3.0)\n",
    "w_history = [weight_var.numpy()]\n",
    "\n",
    "step = 0\n",
    "while ( f(weight_var) > thr ) and (step < step_max):\n",
    "    \n",
    "    # step 1 get the grandient\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = f(weight_var)\n",
    "\n",
    "    # get gradient\n",
    "    gradient_x =  tape.gradient(y, weight_var)\n",
    "    \n",
    "    # x_var = x_var - lr * gradient_x\n",
    "    weight_var.assign_sub(lr * gradient_x)\n",
    "    \n",
    "    # just keep the values of x\n",
    "    w_history += [weight_var.numpy()]\n",
    "    \n",
    "    step = step +1\n",
    "w_history = np.array(w_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"maximum step reached:\",step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.linspace(-5, 15,100)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(w, f(w), label = \"(x-5)^2\")\n",
    "plt.plot(w_history, f(w_history),marker = \"o\", color=\"r\")\n",
    "plt.legend()\n",
    "plt.title(\"Convex function\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_lr(lr, step, decay_rate=0.1):\n",
    "    tmp_lr = lr * np.exp(-decay_rate*step)\n",
    "    return tmp_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.linspace(0,100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(steps,exponential_decay_lr(lr=1.2,step=steps,decay_rate = 0.05))\n",
    "plt.xlabel(\"optimization steps\")\n",
    "plt.ylabel(\"learning rate\")\n",
    "plt.title(\"learning rate decay\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "weight_var = tf.Variable(-3.0)\n",
    "lr = 1.0\n",
    "step_max = 500\n",
    "thr = 0.01\n",
    "\n",
    "# algorith\n",
    "weight_var = tf.Variable(-3.0)\n",
    "w_history = [weight_var.numpy()]\n",
    "\n",
    "step = 0\n",
    "while ( f(weight_var) > thr ) and (step < step_max):\n",
    "    # step 1 get the grandient\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = f(weight_var)\n",
    "\n",
    "    # get gradient\n",
    "    gradient_w =  tape.gradient(y, weight_var)\n",
    "    \n",
    "    # x_var = x_var - lr * gradient_x\n",
    "    tmp_lr = exponential_decay_lr(lr, step, decay_rate=0.01)\n",
    "    weight_var.assign_sub(tmp_lr * gradient_w)\n",
    "    \n",
    "    # just keep the values of x\n",
    "    w_history += [weight_var.numpy()]\n",
    "    \n",
    "    step = step +1\n",
    "w_history = np.array(w_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total steps:\", step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.linspace(-5, 15,100)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(w, f(w), label = \"(x-5)^2\")\n",
    "plt.plot(w_history, f(w_history),marker = \"o\", color=\"r\")\n",
    "plt.legend()\n",
    "plt.title(\"Convex function\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Adult dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow tutorial on training loops\n",
    "https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "data_types = OrderedDict([\n",
    "    (\"age\", \"int\"),\n",
    "    (\"workclass\", \"category\"),\n",
    "    (\"final_weight\", \"int\"),  # originally it was called fnlwgt\n",
    "    (\"education\", \"category\"),\n",
    "    (\"education_num\", \"int\"),\n",
    "    (\"marital_status\", \"category\"),\n",
    "    (\"occupation\", \"category\"),\n",
    "    (\"relationship\", \"category\"),\n",
    "    (\"race\", \"category\"),\n",
    "    (\"sex\", \"category\"),\n",
    "    (\"capital_gain\", \"float\"),  # required because of NaN values\n",
    "    (\"capital_loss\", \"int\"),\n",
    "    (\"hours_per_week\", \"int\"),\n",
    "    (\"native_country\", \"category\"),\n",
    "    (\"income_class\", \"category\"),\n",
    "])\n",
    "target_column = \"income_class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        names=data_types,\n",
    "        index_col=None,\n",
    "\n",
    "        comment='|',  # test dataset has comment in it\n",
    "        skipinitialspace=True,  # Skip spaces after delimiter\n",
    "        na_values={\n",
    "            'capital_gain': 99999,\n",
    "            'workclass': '?',\n",
    "            'native_country': '?',\n",
    "            'occupation': '?',\n",
    "        },\n",
    "        dtype=data_types,\n",
    "    )\n",
    "\n",
    "def clean_dataset(data):\n",
    "    # Test dataset has dot at the end, we remove it in order\n",
    "    # to unify names between training and test datasets.\n",
    "    data['income_class'] = data.income_class.str.rstrip('.').astype('category')\n",
    "    \n",
    "    # Remove final weight column since there is no use\n",
    "    # for it during the classification.\n",
    "    data = data.drop('final_weight', axis=1)\n",
    "    \n",
    "    # Duplicates might create biases during the analysis and\n",
    "    # during prediction stage they might give over-optimistic\n",
    "    # (or pessimistic) results.\n",
    "    data = data.drop_duplicates()\n",
    "    \n",
    "    # Binary target variable (>50K == 1 and <=50K == 0)\n",
    "    data[target_column] = (data[target_column] == '>50K').astype(int)\n",
    "    \n",
    "    # Categorical dataset\n",
    "    categorical_features = data.select_dtypes('category').columns\n",
    "    data[categorical_features] = data.select_dtypes('category').apply(lambda x: x.cat.codes)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# get and clean train dataset\n",
    "TRAIN_DATA_FILE = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "train_data = clean_dataset(read_dataset(TRAIN_DATA_FILE))\n",
    "train_data = train_data.dropna()\n",
    "print(\"Train dataset shape:\", train_data.shape)\n",
    "\n",
    "# get and clean test dataset\n",
    "TEST_DATA_FILE = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
    "test_data = clean_dataset(read_dataset(TEST_DATA_FILE))\n",
    "test_data = test_data.dropna()\n",
    "print(\"Test dataset shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"income_class\"\n",
    "features = train_data.columns.difference([target_column])\n",
    "sensitive_feature = [\"sex\"]\n",
    "non_sensitive_features = list(set(features).difference(set(sensitive_feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data[features], train_data[[target_column]], test_size=0.1)\n",
    "X_test, y_test = test_data[features], test_data[[target_column]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient decent in Machine learning\n",
    "\n",
    "In machine learning we apply **Gradient Decent** method in order to **minimise the expexted loss** of the model using the **empirical risk minimization** because we dont know the acctual distibution of our data. \n",
    "$$ \\min_{w} E_D[Loss]$$\n",
    "$$ \\min_{w} E_D[L(h(X;w), y)] = \\min_{w} \\dfrac{\\sum_{i=1}^{n} L(h(x_i;w), y_i)}{n}$$\n",
    "  \n",
    "\n",
    "So we compute the grandient of the loss function with respect to the model parameter to find parameters of the model **w**. \n",
    "\n",
    "While in many cases the loss function in not convex several gradient decent optimizer usualy find a acceptable aproximation for a very different problem. Moreover different methos like advence optimizer trying to escape from local minima.\n",
    "\n",
    "### Variants of  Gradient Decent\n",
    "\n",
    "> 1. **Gradient Decent**   \n",
    "In each step of the algorithm (updates of the model parameters), we estimate the gradient using the whole dataset : $E_D(L(h(D;w), Y))$\n",
    "> 2. **Stochastic Gradient Decent**   \n",
    "In each step of the algorithm (updates of the model parameters), we estimate the gradient using a single point dataset : $L(h(x;w), y)$. A hole pass of the datsaset D is called epoch.  \n",
    "> 3. **Minibatch Stochastic Gradient Decent**    \n",
    "In each step of the algorithm (updates of the model parameters), we estimate the gradient using a mini batch of the dataset : $E_D(L(h(batch_x; w), batch_y))$. A whole pass of the datsaset D is called epoch.\n",
    "\n",
    "<figure>\n",
    "  <img style=\"float: left;\" src=\"./figures/nn_training_loop.png\" width=\"600\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define Model using tensorflow\n",
    "\n",
    "<figure>\n",
    "  <img style=\"float: left;\" src=\"./figures/DNN2.png\" width=\"600\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img style=\"float: left;\" src=\"./figures/neuron.png\" width=\"600\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.Input((13,))\n",
    "\n",
    "layer_1_output = tf.keras.layers.Dense(64, activation='relu')(input_layer)\n",
    "\n",
    "final_output = tf.keras.layers.Dense(1, activation='sigmoid')(layer_1_output) \n",
    "\n",
    "# sigmoid bound the output [0,1] to respesent propability.\n",
    "\n",
    "# custruct graph\n",
    "neural_network = tf.keras.Model(inputs=input_layer, outputs=final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross entropy loss\n",
    "\n",
    "\n",
    "Cross-entropy loss, or log loss, measures the performance of a classification\n",
    "\n",
    "\n",
    "The **predicted probability** for each class $p(\\hat{y_i} | x_i)$ is compared to the **actual class** $y_ i$ and a loss is calculated that penalizes the probability based on how far it is from the actual class.\n",
    "\n",
    "\n",
    "**Cross entropy loss** minimize the negative log of the $p(\\hat{y_i} | x; w)$:\n",
    "\n",
    "$$  L = \\sum_{i=1}^{C}  - y_i log( p(\\hat{y_i} | x_i) ) $$ \n",
    "\n",
    "\n",
    "In the case of binary classification this is reduced to the following formula:\n",
    "\n",
    "$$  L(x,y) = \\sum_{i=1}^{2}  - y_i log( p(\\hat{y_i} | x_i) ) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = - y_1 log( p(\\hat{y_1} | x_i) ) - y_2 log( p(\\hat{y_2} | x_i) ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = - y_1 log( p(\\hat{y_1} | x_i) ) - (1-y_1) log( 1 - p(\\hat{y_1} | x_i) ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\min_{w} \\dfrac{\\sum_{i=1}^{n} L(h(x_i;w), y_i)}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loops for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img style=\"float: left;\" src=\"./figures/nn_training_loop.png\" width=\"600\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "batch_size = 256\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_step(model, input_data):\n",
    "    \n",
    "    x_batch_train, y_batch_train = input_data\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        prob = model(x_batch_train, training=True)\n",
    "        loss = loss_fn(y_batch_train, prob)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def validation_step(model, input_data):\n",
    "    x_batch_train, y_batch_train = input_data\n",
    "    output = model(x_batch_train, training=False)\n",
    "    val_loss = loss_fn(y_batch_train, output)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tf_dataset = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "y_tf_dataset = tf.data.Dataset.from_tensor_slices(y_train)\n",
    "train_tf_dataset = tf.data.Dataset.zip((x_tf_dataset, y_tf_dataset))\n",
    "train_tf_dataset = train_tf_dataset.shuffle(buffer_size=1000).batch(8)\n",
    "\n",
    "val_x_tf_dataset = tf.data.Dataset.from_tensor_slices(X_val)\n",
    "val_y_tf_dataset = tf.data.Dataset.from_tensor_slices(y_val)\n",
    "val_tf_dataset = tf.data.Dataset.zip((val_x_tf_dataset,val_y_tf_dataset))\n",
    "val_tf_dataset = val_tf_dataset.batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init_patient = 10\n",
    "\n",
    "best_val_loss = np.inf\n",
    "epochs = 50\n",
    "\n",
    "# in each epoch we iterate all over the dataset\n",
    "patient = init_patient\n",
    "history = {\"loss\":[],\n",
    "           \"val_loss\": []}\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\n--------\\nStart of epoch :\", epoch)\n",
    "    \n",
    "    # -------------------- Train ----------------------------\n",
    "    mean_loss = 0\n",
    "    \n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_tf_dataset):\n",
    "        # training step : update the model on minibatch and get the loss\n",
    "        loss = training_step(neural_network, input_data = (x_batch_train, y_batch_train))\n",
    "        \n",
    "        # monitor accumulative mean loss\n",
    "        mean_loss = (step * mean_loss + loss.numpy())/(step+1)\n",
    "        \n",
    "        \n",
    "    history[\"loss\"].append(mean_loss)\n",
    "    print(\"Online training loss: \", round(mean_loss,7) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # -------------------- Validation ------------------------\n",
    "    # after each epoch we compute the validation error\n",
    "    val_loss = validation_step(neural_network, (X_val,y_val)).numpy()\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    print(\"Validation loss: \", val_loss)\n",
    "    \n",
    "    \n",
    "    # -------------------- Stoping critirio ------------------\n",
    "    if val_loss < best_val_loss:\n",
    "        # if val is descresing we can wait for some more epochs\n",
    "        best_val_loss = val_loss\n",
    "        patient = init_patient\n",
    "        print(\"We find a better validation loss\")\n",
    "    else:\n",
    "        # if val loss is not descresing our patient decrease\n",
    "        patient = patient - 1\n",
    "        \n",
    "    # we dont have no increasing validation loss so we stop\n",
    "    if patient == 0:\n",
    "        print(\"We have no increasing val loss: \", val_loss)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history[\"loss\"], label=\"train\")\n",
    "plt.plot(history[\"val_loss\"], label=\"val\")\n",
    "plt.ylabel(\"cross entropy loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_propabilities_tesnor = neural_network(X_test[features].values)\n",
    "predicted_propabilities = predicted_propabilities_tesnor.numpy()\n",
    "predicted_class = (predicted_propabilities>0.5)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Build in Methods\n",
    "tensorflow library has a lot of already implemented methods which we can customise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.Input((13,))\n",
    "layer_1_output = tf.keras.layers.Dense(64, activation='relu')(input_layer)\n",
    "final_output = tf.keras.layers.Dense(1, activation='sigmoid')(layer_1_output)\n",
    "neural_network = tf.keras.Model(inputs=input_layer, outputs=final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "neural_network.compile(optimizer = optimizer,\n",
    "                       loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "                       metrics = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",\n",
    "                                      patience = 10)\n",
    "\n",
    "history = neural_network.fit(X_train,\n",
    "                             y_train,\n",
    "                             batch_size=256,\n",
    "                             validation_data=(X_val,y_val),\n",
    "                             callbacks = es,\n",
    "                             epochs = 100).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history[\"accuracy\"], label=\"train\")\n",
    "plt.plot(history[\"val_accuracy\"], label=\"val\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_propabilities_tesnor = neural_network(X_test[features].values)\n",
    "predicted_propabilities = predicted_propabilities_tesnor.numpy()\n",
    "predicted_class = (predicted_propabilities>0.5)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test,predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement your own network\n",
    "\n",
    "try to implenet a network with the following architeture:  \n",
    "2 dense layer with 124, 64 units respectively.\n",
    "\n",
    "then train the network using:  \n",
    "epochs = 500  \n",
    "patience = 50  \n",
    "lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
