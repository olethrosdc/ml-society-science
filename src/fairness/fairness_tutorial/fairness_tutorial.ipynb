{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img style=\"float: left;\" src=\"./figures/unine.png\" width=\"200\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img style=\"float: left;\" src=\"./figures/fair_1.png\" width=\"200\"/>\n",
    "</figure>\n",
    "\n",
    "# Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning (ML) usage is increasing every day. Companies and organisations build ML models with the purpose of minimising human effort or improving performance in various tasks. So today Machine learning arguably affects our lives.\n",
    "\n",
    "Examples:\n",
    "1. Recommendation system\n",
    "2. Face recognition\n",
    "3. Self-driving cars\n",
    "4. Candidate selection e.x college admissions, CV screening\n",
    "5. Loan admissions\n",
    "\n",
    "There are a lot of examples that researchers demonstrate inadvertently discriminating against several population groups.    \n",
    "The most know is [ProPublica's](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis) research in COMPAS Dataset.\n",
    "\n",
    "When machine learning models are being used to make decisions, they cannot be separated from the social and ethical context in which they are applied, and those developing and deploying these models must take care to do so in a manner that accounts for both performance and fairness. \n",
    "\n",
    "So in the last decade Fairness has become one of the most active research areas in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2. Sources of Bias.\n",
    "\n",
    "Bias may be introduced into a machine learning project at any step along the way, and it is important to carefully think through each potential source and how it may affect your results.\n",
    "\n",
    "\n",
    "Source of bias:\n",
    "- Data\n",
    "    1. Historical injustice.\n",
    "    2. Sample bias - Collection Bias.\n",
    "    3. Limited features.\n",
    "    4. Unbalanced dataset.\n",
    "    5. Proxy Variables.\n",
    "- Modelling\n",
    "    1. Data preprocessing\n",
    "    2. Model assumptions \n",
    "- Feedback loops.\n",
    "    1. decisions based on biassed models lead to biassed dataset.  \n",
    "\n",
    "<figure>\n",
    "  <img style=\"float: left;\" src=\"./figures/simple_pipeline.png\" width=\"600\"/>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3. Load Dataset\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "data_types = OrderedDict([\n",
    "    (\"age\", \"int\"),\n",
    "    (\"workclass\", \"category\"),\n",
    "    (\"final_weight\", \"int\"),  # originally it was called fnlwgt\n",
    "    (\"education\", \"category\"),\n",
    "    (\"education_num\", \"int\"),\n",
    "    (\"marital_status\", \"category\"),\n",
    "    (\"occupation\", \"category\"),\n",
    "    (\"relationship\", \"category\"),\n",
    "    (\"race\", \"category\"),\n",
    "    (\"sex\", \"category\"),\n",
    "    (\"capital_gain\", \"float\"),  # required because of NaN values\n",
    "    (\"capital_loss\", \"int\"),\n",
    "    (\"hours_per_week\", \"int\"),\n",
    "    (\"native_country\", \"category\"),\n",
    "    (\"income_class\", \"category\"),\n",
    "])\n",
    "target_column = \"income_class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        names=data_types,\n",
    "        index_col=None,\n",
    "\n",
    "        comment='|',  # test dataset has comment in it\n",
    "        skipinitialspace=True,  # Skip spaces after delimiter\n",
    "        na_values={\n",
    "            'capital_gain': 99999,\n",
    "            'workclass': '?',\n",
    "            'native_country': '?',\n",
    "            'occupation': '?',\n",
    "        },\n",
    "        dtype=data_types,\n",
    "    )\n",
    "\n",
    "def clean_dataset(data):\n",
    "    # Test dataset has dot at the end, we remove it in order\n",
    "    # to unify names between training and test datasets.\n",
    "    data['income_class'] = data.income_class.str.rstrip('.').astype('category')\n",
    "    \n",
    "    # Remove final weight column since there is no use\n",
    "    # for it during the classification.\n",
    "    data = data.drop('final_weight', axis=1)\n",
    "    \n",
    "    # Duplicates might create biases during the analysis and\n",
    "    # during prediction stage they might give over-optimistic\n",
    "    # (or pessimistic) results.\n",
    "    data = data.drop_duplicates()\n",
    "    \n",
    "    # Binary target variable (>50K == 1 and <=50K == 0)\n",
    "    data[target_column] = (data[target_column] == '>50K').astype(int)\n",
    "    \n",
    "    # Categorical dataset\n",
    "    categorical_features = data.select_dtypes('category').columns\n",
    "    data[categorical_features] = data.select_dtypes('category').apply(lambda x: x.cat.codes)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load & clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 8] nodename nor servname provided, or not known>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mgaierror\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py:1346\u001B[0m, in \u001B[0;36mAbstractHTTPHandler.do_open\u001B[0;34m(self, http_class, req, **http_conn_args)\u001B[0m\n\u001B[1;32m   1345\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1346\u001B[0m     \u001B[43mh\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1347\u001B[0m \u001B[43m              \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhas_header\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTransfer-encoding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1348\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py:1285\u001B[0m, in \u001B[0;36mHTTPConnection.request\u001B[0;34m(self, method, url, body, headers, encode_chunked)\u001B[0m\n\u001B[1;32m   1284\u001B[0m \u001B[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1285\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py:1331\u001B[0m, in \u001B[0;36mHTTPConnection._send_request\u001B[0;34m(self, method, url, body, headers, encode_chunked)\u001B[0m\n\u001B[1;32m   1330\u001B[0m     body \u001B[38;5;241m=\u001B[39m _encode(body, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m-> 1331\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mendheaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py:1280\u001B[0m, in \u001B[0;36mHTTPConnection.endheaders\u001B[0;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[1;32m   1279\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CannotSendHeader()\n\u001B[0;32m-> 1280\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py:1040\u001B[0m, in \u001B[0;36mHTTPConnection._send_output\u001B[0;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[1;32m   1039\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer[:]\n\u001B[0;32m-> 1040\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1042\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m message_body \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1043\u001B[0m \n\u001B[1;32m   1044\u001B[0m     \u001B[38;5;66;03m# create a consistent interface to message_body\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py:980\u001B[0m, in \u001B[0;36mHTTPConnection.send\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m    979\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_open:\n\u001B[0;32m--> 980\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    981\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py:1447\u001B[0m, in \u001B[0;36mHTTPSConnection.connect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1445\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnect to a host on a given (SSL) port.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1447\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1449\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tunnel_host:\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/client.py:946\u001B[0m, in \u001B[0;36mHTTPConnection.connect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    945\u001B[0m \u001B[38;5;124;03m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 946\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_connection\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    947\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msource_address\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    948\u001B[0m \u001B[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socket.py:823\u001B[0m, in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address)\u001B[0m\n\u001B[1;32m    822\u001B[0m err \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 823\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m \u001B[43mgetaddrinfo\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mSOCK_STREAM\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    824\u001B[0m     af, socktype, proto, canonname, sa \u001B[38;5;241m=\u001B[39m res\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socket.py:954\u001B[0m, in \u001B[0;36mgetaddrinfo\u001B[0;34m(host, port, family, type, proto, flags)\u001B[0m\n\u001B[1;32m    953\u001B[0m addrlist \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m--> 954\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m \u001B[43m_socket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetaddrinfo\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfamily\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproto\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    955\u001B[0m     af, socktype, proto, canonname, sa \u001B[38;5;241m=\u001B[39m res\n",
      "\u001B[0;31mgaierror\u001B[0m: [Errno 8] nodename nor servname provided, or not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mURLError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [4], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# get and clean train dataset\u001B[39;00m\n\u001B[1;32m      2\u001B[0m TRAIN_DATA_FILE \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 3\u001B[0m train_data \u001B[38;5;241m=\u001B[39m clean_dataset(\u001B[43mread_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mTRAIN_DATA_FILE\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m      4\u001B[0m train_data \u001B[38;5;241m=\u001B[39m train_data\u001B[38;5;241m.\u001B[39mdropna()\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrain dataset shape:\u001B[39m\u001B[38;5;124m\"\u001B[39m, train_data\u001B[38;5;241m.\u001B[39mshape)\n",
      "Cell \u001B[0;32mIn [3], line 2\u001B[0m, in \u001B[0;36mread_dataset\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread_dataset\u001B[39m(path):\n\u001B[0;32m----> 2\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_types\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \n\u001B[1;32m      7\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcomment\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m|\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# test dataset has comment in it\u001B[39;49;00m\n\u001B[1;32m      8\u001B[0m \u001B[43m        \u001B[49m\u001B[43mskipinitialspace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Skip spaces after delimiter\u001B[39;49;00m\n\u001B[1;32m      9\u001B[0m \u001B[43m        \u001B[49m\u001B[43mna_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcapital_gain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m99999\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mworkclass\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m?\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnative_country\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m?\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moccupation\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m?\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_types\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Phd/projects/env/ml-society/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[0;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Phd/projects/env/ml-society/lib/python3.9/site-packages/pandas/util/_decorators.py:317\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[1;32m    312\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    313\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39marguments),\n\u001B[1;32m    314\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[1;32m    315\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(inspect\u001B[38;5;241m.\u001B[39mcurrentframe()),\n\u001B[1;32m    316\u001B[0m     )\n\u001B[0;32m--> 317\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Phd/projects/env/ml-society/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m    936\u001B[0m     dialect,\n\u001B[1;32m    937\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    946\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m    947\u001B[0m )\n\u001B[1;32m    948\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m--> 950\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Phd/projects/env/ml-society/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    602\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    604\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 605\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    608\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m~/Phd/projects/env/ml-society/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1439\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1441\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1442\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Phd/projects/env/ml-society/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1729\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1727\u001B[0m     is_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   1728\u001B[0m     mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1729\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1730\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1731\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1732\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1733\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1734\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1735\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1736\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1737\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1738\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1739\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1740\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m~/Phd/projects/env/ml-society/lib/python3.9/site-packages/pandas/io/common.py:714\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    711\u001B[0m     codecs\u001B[38;5;241m.\u001B[39mlookup_error(errors)\n\u001B[1;32m    713\u001B[0m \u001B[38;5;66;03m# open URLs\u001B[39;00m\n\u001B[0;32m--> 714\u001B[0m ioargs \u001B[38;5;241m=\u001B[39m \u001B[43m_get_filepath_or_buffer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    715\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath_or_buf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    716\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    717\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    718\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    719\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    720\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    722\u001B[0m handle \u001B[38;5;241m=\u001B[39m ioargs\u001B[38;5;241m.\u001B[39mfilepath_or_buffer\n\u001B[1;32m    723\u001B[0m handles: \u001B[38;5;28mlist\u001B[39m[BaseBuffer]\n",
      "File \u001B[0;32m~/Phd/projects/env/ml-society/lib/python3.9/site-packages/pandas/io/common.py:364\u001B[0m, in \u001B[0;36m_get_filepath_or_buffer\u001B[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001B[0m\n\u001B[1;32m    362\u001B[0m \u001B[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001B[39;00m\n\u001B[1;32m    363\u001B[0m req_info \u001B[38;5;241m=\u001B[39m urllib\u001B[38;5;241m.\u001B[39mrequest\u001B[38;5;241m.\u001B[39mRequest(filepath_or_buffer, headers\u001B[38;5;241m=\u001B[39mstorage_options)\n\u001B[0;32m--> 364\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq_info\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m req:\n\u001B[1;32m    365\u001B[0m     content_encoding \u001B[38;5;241m=\u001B[39m req\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mContent-Encoding\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m content_encoding \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgzip\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    367\u001B[0m         \u001B[38;5;66;03m# Override compression based on Content-Encoding header\u001B[39;00m\n",
      "File \u001B[0;32m~/Phd/projects/env/ml-society/lib/python3.9/site-packages/pandas/io/common.py:266\u001B[0m, in \u001B[0;36murlopen\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    260\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    261\u001B[0m \u001B[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001B[39;00m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;124;03mthe stdlib.\u001B[39;00m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01murllib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrequest\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43murllib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py:214\u001B[0m, in \u001B[0;36murlopen\u001B[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001B[0m\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    213\u001B[0m     opener \u001B[38;5;241m=\u001B[39m _opener\n\u001B[0;32m--> 214\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py:517\u001B[0m, in \u001B[0;36mOpenerDirector.open\u001B[0;34m(self, fullurl, data, timeout)\u001B[0m\n\u001B[1;32m    514\u001B[0m     req \u001B[38;5;241m=\u001B[39m meth(req)\n\u001B[1;32m    516\u001B[0m sys\u001B[38;5;241m.\u001B[39maudit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124murllib.Request\u001B[39m\u001B[38;5;124m'\u001B[39m, req\u001B[38;5;241m.\u001B[39mfull_url, req\u001B[38;5;241m.\u001B[39mdata, req\u001B[38;5;241m.\u001B[39mheaders, req\u001B[38;5;241m.\u001B[39mget_method())\n\u001B[0;32m--> 517\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    519\u001B[0m \u001B[38;5;66;03m# post-process response\u001B[39;00m\n\u001B[1;32m    520\u001B[0m meth_name \u001B[38;5;241m=\u001B[39m protocol\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_response\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py:534\u001B[0m, in \u001B[0;36mOpenerDirector._open\u001B[0;34m(self, req, data)\u001B[0m\n\u001B[1;32m    531\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[1;32m    533\u001B[0m protocol \u001B[38;5;241m=\u001B[39m req\u001B[38;5;241m.\u001B[39mtype\n\u001B[0;32m--> 534\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_open\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\n\u001B[1;32m    535\u001B[0m \u001B[43m                          \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_open\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    536\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result:\n\u001B[1;32m    537\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py:494\u001B[0m, in \u001B[0;36mOpenerDirector._call_chain\u001B[0;34m(self, chain, kind, meth_name, *args)\u001B[0m\n\u001B[1;32m    492\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m handler \u001B[38;5;129;01min\u001B[39;00m handlers:\n\u001B[1;32m    493\u001B[0m     func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[0;32m--> 494\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    495\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    496\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py:1389\u001B[0m, in \u001B[0;36mHTTPSHandler.https_open\u001B[0;34m(self, req)\u001B[0m\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhttps_open\u001B[39m(\u001B[38;5;28mself\u001B[39m, req):\n\u001B[0;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhttp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mHTTPSConnection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1390\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_context\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_hostname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_hostname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/urllib/request.py:1349\u001B[0m, in \u001B[0;36mAbstractHTTPHandler.do_open\u001B[0;34m(self, http_class, req, **http_conn_args)\u001B[0m\n\u001B[1;32m   1346\u001B[0m         h\u001B[38;5;241m.\u001B[39mrequest(req\u001B[38;5;241m.\u001B[39mget_method(), req\u001B[38;5;241m.\u001B[39mselector, req\u001B[38;5;241m.\u001B[39mdata, headers,\n\u001B[1;32m   1347\u001B[0m                   encode_chunked\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39mhas_header(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTransfer-encoding\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m   1348\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n\u001B[0;32m-> 1349\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m URLError(err)\n\u001B[1;32m   1350\u001B[0m     r \u001B[38;5;241m=\u001B[39m h\u001B[38;5;241m.\u001B[39mgetresponse()\n\u001B[1;32m   1351\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "\u001B[0;31mURLError\u001B[0m: <urlopen error [Errno 8] nodename nor servname provided, or not known>"
     ]
    }
   ],
   "source": [
    "# get and clean train dataset\n",
    "TRAIN_DATA_FILE = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "train_data = clean_dataset(read_dataset(TRAIN_DATA_FILE))\n",
    "train_data = train_data.dropna()\n",
    "print(\"Train dataset shape:\", train_data.shape)\n",
    "\n",
    "# get and clean test dataset\n",
    "TEST_DATA_FILE = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
    "test_data = clean_dataset(read_dataset(TEST_DATA_FILE))\n",
    "test_data = test_data.dropna()\n",
    "print(\"Test dataset shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fig = sns.countplot(x=train_data[\"income_class\"])\n",
    "fig.set_xticklabels(['income <= 50K','income > 50K'])\n",
    "plt.title(\"Num of values in each category\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fig = sns.countplot(x=train_data[\"sex\"])\n",
    "fig.set_xticklabels(['woman','men'])\n",
    "# plt.title(\"gender\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fig = sns.countplot(data=train_data[[\"income_class\",\"sex\"]],\n",
    "                    x=\"income_class\",\n",
    "                    hue=\"sex\")\n",
    "fig.set_xticklabels(['income <= 50K','income > 50K'])\n",
    "plt.legend(title='sex', labels=['woman', 'men'])\n",
    "plt.title(\"Num of values in each category\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is quite imbalance between a gender, so we expect our model to be unfair.\n",
    "The source of the bias is coming from our societal bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4. Train a Model\n",
    "let's train our model and analyse the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_data.columns.difference([target_column])\n",
    "\n",
    "# we define sex as our sensitive feature\n",
    "sensitive_feature = [\"sex\"]\n",
    "\n",
    "# all the rest are non sensitive feature\n",
    "non_sensitive_features = list(set(features).difference(set(sensitive_feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model  = RandomForestClassifier(n_estimators=1000,\n",
    "                                max_depth=10)\n",
    "\n",
    "model.fit(X = train_data[sensitive_feature+non_sensitive_features],\n",
    "          y = train_data[target_column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure performance scores\n",
    "\n",
    "Bellow are some of the most know mesurements scores\n",
    "\n",
    "\n",
    ">1. Accuracy: $\\frac{TP+TN}{TN + FP + FN + TP}$  \n",
    "2. Recall: $\\frac{TP}{ TP + FN }$ \n",
    "3. Precision: $\\frac{TP}{ TP + FP }$  \n",
    "4. F1-score: $2* \\frac{ Recall * Precision}{Precision + Recall}$  \n",
    "\n",
    "<div>\n",
    "<img style=\"float: left;\" src=\"./figures/conf_matrix.png\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_true = test_data[target_column],\n",
    "                               y_pred = model.predict(test_data[sensitive_feature+non_sensitive_features]))\n",
    "conf_matrix = pd.DataFrame(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cbar=False, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Map\")\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN, FP, FN, TP = conf_matrix.values.ravel()\n",
    "\n",
    "acc_score = ( TN + TP ) / (TN + FP + FN + TP)\n",
    "precision_score = TP / (TP + FP)\n",
    "recall_score = TP / (TP + FN)\n",
    "f1_score = 2*(precision_score*recall_score)/(precision_score+recall_score)\n",
    "\n",
    "print(\"accuracy score:\", acc_score)\n",
    "print(\"precision score:\", precision_score)\n",
    "print(\"recall score:\", recall_score)\n",
    "print(\"f1 score:\", recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = accuracy_score(y_true = test_data[target_column],\n",
    "                           y_pred = model.predict(test_data[sensitive_feature+non_sensitive_features]))\n",
    "print(\"Accuracy_score:\", acc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A5. Analysis of model fairness\n",
    "\n",
    "\n",
    "Unfortunately there is not a single definition of fairness. There are many, often competing, ways to measure whether a given model is statistically “fair”, but it’s important to remember to start from the social and policy goals for equity and fairness and map those to the statistical properties we want in our models to help achieve those goals. Most of the definitions involve splitting the population into groups and compare metrics on those groups.\n",
    "\n",
    "Different definition preserve different fairness aspects. So the desired fairness definition is depending on the application. In general, this requires consideration of the project’s goals, and a detailed discussion between the data scientists, decision makers, and those who will be affected by the application of the model.\n",
    "\n",
    "Bellow we will define the most known fairness definitions.\n",
    "\n",
    "1. **Demographic Parity** - equal positive outcome rates\n",
    "2. **Equalized Opportunities** - equal true positive  rates\n",
    "3. **Equalized Odds** -  equal true positive and false positive rate\n",
    "4. **Predictive Parity** - equal Positive precision  \n",
    ".  \n",
    ".  \n",
    ".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data[sensitive_feature+non_sensitive_features])\n",
    "predictions = pd.Series(predictions,index = test_data.index)\n",
    "sensitive_atribute = test_data[sensitive_feature[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woman_mask = sensitive_atribute == 0\n",
    "conf_matrix_0 = confusion_matrix(y_true = test_data.loc[woman_mask,target_column],\n",
    "                                 y_pred = predictions.loc[woman_mask])\n",
    "conf_matrix_0 = pd.DataFrame(conf_matrix_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_mask = sensitive_atribute == 1\n",
    "conf_matrix_1 = confusion_matrix(y_true = test_data.loc[man_mask,target_column],\n",
    "                                 y_pred = predictions.loc[man_mask])\n",
    "conf_matrix_1 = pd.DataFrame(conf_matrix_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize = (8,4))\n",
    "sns.heatmap(conf_matrix_0, annot=True, fmt='g', cbar=False, cmap=\"Blues\", ax=axes[0])\n",
    "axes[0].set_title(\"Woman Confusion matrix\")\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xlabel('Prediction')\n",
    "\n",
    "sns.heatmap(conf_matrix_1, annot=True, fmt='g', cbar=False, cmap=\"Blues\", ax=axes[1])\n",
    "axes[1].set_title(\"Men Confusion matrix\")\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_xlabel('Prediction')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1.  Demographic Parity\n",
    "\n",
    "**Demographic parity** , also referred to as **statistical parity** , **acceptance rate parity**  and **benchmarking**\n",
    "\n",
    "Demographic Parity states that the positive outcome rates between sensitive group must be the same.\n",
    "\n",
    "---------------\n",
    "> **Definition Demographic Parity :** A classifier $C$ is said that satisfy **demographic parity** if $$ P_c(\\hat{Y}= 1 | S = s ) =  P_c(\\hat{Y} = 1), \\quad  \\forall s\\in S $$\n",
    "in case of 2 sensitive classes, $ s \\in \\{0,1\\}  $ we can also equivalent write:\n",
    "$$ P_c(\\hat{Y}= 1 | S = 0 ) =  P_c(\\hat{Y}= 1 | S = 1 ) $$  \n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to measure Demographic parity?**\n",
    "\n",
    "we can measure the Demographic parity using the Demographic parity difference metric.\n",
    "\n",
    "> **Demographic parity difference :** we can measure the demographic parity using the next formula:\n",
    "  $$  \\text{Metric} =  max_s[P_c(\\hat{Y}= 1 | S = s )] -  min_s[P_c(\\hat{Y}= 1 | S = s )] $$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demographic_parity_difference(predictions, sensitive_attribute):\n",
    "    \"\"\"\n",
    "    Implementation of demographic parity difference for different groups of sensitive attribute\n",
    "    1. For each group calculate the propotion of accepted rate.\n",
    "    2. Find the maximum and the minim group.\n",
    "    3. Calcaulte the difference.\n",
    "    \"\"\"\n",
    "    # 1. For each group calculate the proportion of accepted rate.\n",
    "    unique_groups = np.sort(sensitive_attribute.unique())\n",
    "    proportion_of_accepted_rate = []\n",
    "    for group in unique_groups:\n",
    "        pred_group = predictions[sensitive_attribute==group]\n",
    "        accepted_rate = (pred_group==1).sum() / pred_group.shape[0]\n",
    "        proportion_of_accepted_rate += [accepted_rate]\n",
    "\n",
    "    # 2. Find the maximum and the minimum accepted_rate.\n",
    "    maximum_accepted_rate = max(proportion_of_accepted_rate)\n",
    "    minimum_accepted_rate = min(proportion_of_accepted_rate)\n",
    "\n",
    "    # 3. Calculate the different.\n",
    "    difference = maximum_accepted_rate - minimum_accepted_rate\n",
    "    \n",
    "    return difference, proportion_of_accepted_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data[sensitive_feature+non_sensitive_features])\n",
    "sensitive_attribute = test_data[\"sex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_metric, rates = demographic_parity_difference(predictions, sensitive_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Demographic parity difference is: {round(demographic_metric,5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The rate of different groups are: {rates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "**When to use demographic parity?** \n",
    "1. We are aware of **historical biases** that may have affected the quality of our data. E.x not presents of specific minority in a specific work.\n",
    "2. We want to **change the state of our current world to improve it**. For example, we want to have equal admitions of different group in a specific project.\n",
    "\n",
    "\n",
    "**Potential issues?** \n",
    "1. **Laziness**: We can satisfy demographic parity in we accept random people in group S=0 but qualified people in group  b unless we have the same proportion of positive outcome. We avoid lasyness because classfication is usually perfomred by optimizing an perfomance metric.\n",
    "2. **Not optimality compatible**: A classifier that satisfy demographic parity is suboptimal, if the dataset demographic parity is not hold.\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F2. Equalized Opportunities\n",
    "\n",
    "**Equalized Opportunities** , also referred to as **true positive parity**, **sensitivity**.\n",
    "\n",
    "Equalized Opportunities states that each group has equal true positive rates.\n",
    "\n",
    "---------------\n",
    "> **Definition Equalized Opportunities :** A classifier $C$ is said that satisfy **equalized opportunities** if $$ P_c(\\hat{Y} = 1 | S = s, Y = 1 ) =  P_c(\\hat{Y} = 1 | Y = 1), \\quad  \\forall s\\in S $$\n",
    "in case of 2 sensitive classes, $ s \\in \\{0,1\\}  $ we can also equivalent write:\n",
    "$$ P_c(\\hat{Y}= 1 | S = 0, Y = 1 ) =  P_c(\\hat{Y}= 1 | S = 1, Y = 1 ) $$  \n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to measure Equalized Opportunities?**\n",
    "\n",
    "we can measure the equalized Opportunities using the equalized opportunities difference metric.\n",
    "\n",
    "> **equalized opportunities difference :** we can measure the equalized opportunities using the next formula:\n",
    "  $$  \\text{Metric} =  max_s[P_c(\\hat{Y}= 1 | S = s, Y = 1 )] -  min_s[P_c(\\hat{Y}= 1 | S = s, Y = 1 )] $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**\n",
    "\n",
    "> **True Positive Rate** : To calculate the $P_c(\\hat{Y}= 1 | Y = 1 )$ you can do the following:\n",
    "  $$  P_c(\\hat{Y}= 1 | Y = 1 ) = \\frac{P_c(\\hat{Y}= 1, Y = 1 )}{P_c( Y = 1 )} = \\frac{TP}{P} = \\frac{TP}{TP+FN} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalized_opportunities_difference(predictions, actual, sensitive_attribute):\n",
    "    \"\"\"\n",
    "    Implementation of equalized opportunities difference for different groups of sensitive attribute\n",
    "    1. For each group calculate the TPR.\n",
    "    2. Find the maximum and the minim group.\n",
    "    3. Calculate the difference.\n",
    "    \"\"\"\n",
    "    # 1. For each group calculate the TPR.\n",
    "    unique_groups = np.sort(sensitive_attribute.unique())\n",
    "    true_positive_rates = []\n",
    "    for group in unique_groups:\n",
    "        pred_group = predictions[sensitive_attribute==group]\n",
    "        actual_group = actual[sensitive_attribute==group]\n",
    "        conf_matrix = confusion_matrix(y_true = actual_group,\n",
    "                                       y_pred = pred_group)\n",
    "        TN, FP, FN, TP = conf_matrix.ravel()\n",
    "        \n",
    "        true_positive_rate = TP / (TP + FN)\n",
    "        true_positive_rates += [true_positive_rate]\n",
    "\n",
    "    # 2. Find the maximum and the minimum accepted_rate.\n",
    "    maximum_tpr = max(true_positive_rates)\n",
    "    minimum_tpr = min(true_positive_rates)\n",
    "\n",
    "    # 3. Calculate the different.\n",
    "    difference = maximum_tpr - minimum_tpr\n",
    "    \n",
    "    return difference, true_positive_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data[sensitive_feature+non_sensitive_features])\n",
    "sensitive_attribute = test_data[\"sex\"]\n",
    "actual = test_data[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_opportunities_metric, _ = equalized_opportunities_difference(predictions, actual, sensitive_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"equalized opportunities difference is: {round(equalized_opportunities_metric,5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "**When to use equalized opportunities?**\n",
    "1. When is a strong emphasis on predicting the positive outcome correctly. e.x college admissions\n",
    "2. When False Positives are not costly for the stakeholder. e.x spam detection\n",
    "\n",
    "**Potential issues?** \n",
    "1. **Not optimality compatible**: A classifier that satisfy demographic parity is suboptimal, if the dataset demographic parity is not hold.\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F3. Equalized Odds\n",
    "\n",
    "**Equalized Odds** , also referred to as\n",
    "\n",
    "Equalized Odds states that the true positive rates (TPR) and false positive rates (FPR) between sensitive group must be the same.\n",
    "\n",
    "- TPR = TP / total_positives  = TP / (TP+FN)\n",
    "- FPR = FP / total_negatives = FP / (FP+TN)\n",
    "---------------\n",
    "> **Definition Equalized Odds :** A classifier $C$ is said that satisfy **equalized odds** if $$ P_c(\\hat{Y} = 1 | S = s, Y = y ) =  P_c(\\hat{Y} = 1 | Y = y), \\quad  \\forall s\\in S, \\forall y\\in Y $$\n",
    "in case of 2 sensitive classes, $ s \\in \\{0,1\\}  $ we want to satisfy both:\n",
    "$$ P_c(\\hat{Y}= 1 | S = 0, Y = 1 ) =  P_c(\\hat{Y}= 1 | S = 1, Y = 1 ) $$ \n",
    "and \n",
    "$$ P_c(\\hat{Y}= 1 | S = 0, Y = 0 ) =  P_c(\\hat{Y}= 1 | S = 1, Y = 0 ) $$  \n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to measure Equalized Opportunities?**\n",
    "\n",
    "we can measure the equalized Opportunities using the equalized opportunities difference metric.\n",
    "\n",
    "> **equalized opportunities difference :** we can measure the equalized opportunities using the next formula:\n",
    "  $$   \\text{Metric} = \\text{true_positive_rate_difference} + \\text{false_positive_rate_difference}  $$\n",
    "where\n",
    "  $$ \\text{true_positive_rate_difference} =  max_s[P_c(\\hat{Y}= 1 | S = s, Y = 1 )] -  min_s[P_c(\\hat{Y}= 1 | S = s, Y = 1 )] $$ \n",
    "  $$ \\text{false_positive_rate_difference} =  max_s[P_c(\\hat{Y}= 1 | S = s, Y = 0 )] -  min_s[P_c(\\hat{Y}= 1 | S = s, Y = 0 )] $$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalized_odds_difference(predictions, actual, sensitive_attribute):\n",
    "    \"\"\"\n",
    "    Implementation of equalized odds difference for different groups of sensitive attribute\n",
    "    1. For each group calculate the TPR.\n",
    "    2. Find the maximum and the minim group.\n",
    "    3. Calcaulte the difference.\n",
    "    \"\"\"\n",
    "    # 1. For each group calculate the TPR.\n",
    "    unique_groups = np.sort(sensitive_attribute.unique())\n",
    "    true_positive_rates = []\n",
    "    false_positive_rates = []\n",
    "    for group in unique_groups:\n",
    "        pred_group = predictions[sensitive_attribute==group]\n",
    "        actual_group = actual[sensitive_attribute==group]\n",
    "        conf_matrix = confusion_matrix(y_true = actual_group,\n",
    "                                       y_pred = pred_group)\n",
    "        TN, FP, FN, TP = conf_matrix.ravel()\n",
    "        \n",
    "        true_positive_rate = TP / (TP + FN)\n",
    "        false_positive_rate = FP / (FP + TN)\n",
    "        \n",
    "        true_positive_rates += [true_positive_rate]\n",
    "        false_positive_rates += [false_positive_rate]\n",
    "\n",
    "    # 2. Find the maximum and the minimum accepted_rate.\n",
    "    maximum_tpr = max(true_positive_rates)\n",
    "    minimum_tpr = min(true_positive_rates)\n",
    "    \n",
    "    maximum_fpr = max(false_positive_rates)\n",
    "    minimum_fpr = min(false_positive_rates)\n",
    "\n",
    "    # 3. Calculate the different.\n",
    "    difference_tpr = maximum_tpr - minimum_tpr\n",
    "    difference_fpr = maximum_fpr - minimum_fpr\n",
    "    difference = difference_tpr + difference_fpr\n",
    "    \n",
    "    return difference, (true_positive_rates,false_positive_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data[sensitive_feature+non_sensitive_features])\n",
    "sensitive_attribute = test_data[\"sex\"]\n",
    "actual = test_data[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_odds_metrics, _ = equalized_odds_difference(predictions, actual, sensitive_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"equalized odds difference is: {round(equalized_odds_metrics,5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame([acc_score,\n",
    "                        precision_score,\n",
    "                        recall_score,\n",
    "                        f1_score,\n",
    "                        demographic_metric,\n",
    "                        equalized_opportunities_metric,\n",
    "                        equalized_odds_metrics], \n",
    "                       index = [\"accuracy\",\n",
    "                                \"precision\",\n",
    "                                \"recall\",\n",
    "                                \"f1_score\",\n",
    "                                \"demographic_metric\",\n",
    "                                \"equalized_opportunities\",\n",
    "                                \"equalized_odds\"],\n",
    "                       columns = [\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "**When to use equalized odds?** \n",
    "1. When is a strong emphasis on predicting the positive outcome correctly. e.x college admissions\n",
    "2. When we strongly care about minimising costly False Positives\n",
    "\n",
    "**Potential issues?** \n",
    "1. **Not optimality compatible**: A classifier that satisfy equalized odds is suboptimal, if the dataset demographic parity is not hold. Its more constrained than the equal opportunities, so the effect on perforce will be stronger\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your project you have explained why you choose the specific fairness criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(y_pred, y_true, sensitive_feature):\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_true = y_true,\n",
    "                                   y_pred = y_pred)\n",
    "    conf_matrix = pd.DataFrame(conf_matrix)\n",
    "\n",
    "    TN, FP, FN, TP = conf_matrix.values.ravel()\n",
    "\n",
    "    acc_score = ( TN + TP ) / (TN + FP + FN + TP)\n",
    "    precision_score = TP  / (TP + FP)\n",
    "    recall_score = TP  / (TP + FN)\n",
    "    f1_score = 2*(precision_score*recall_score)/(precision_score+recall_score)\n",
    "    \n",
    "    demographic_metric, _ = demographic_parity_difference(y_pred,\n",
    "                                                          sensitive_feature)\n",
    "    \n",
    "    equalized_opportunities_metric, _ = equalized_opportunities_difference(y_pred,\n",
    "                                                                           y_true,\n",
    "                                                                          sensitive_feature)\n",
    "    equalized_odds_metrics, _ = equalized_odds_difference(y_pred,\n",
    "                                                          y_true,\n",
    "                                                          sensitive_feature)\n",
    "    \n",
    "    return {\n",
    "            \"accuracy\": acc_score,\n",
    "            \"precision\": precision_score,\n",
    "            \"recall\": recall_score,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"demographic_metric\": demographic_metric,\n",
    "            \"equalized_opportunities\" : equalized_opportunities_metric,\n",
    "            \"equalized_odds\": equalized_odds_metrics\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Correct the unfair Classifier\n",
    "\n",
    "Numerous recent papers have proposed mechanisms to enhance fairness in machine learning\n",
    "algorithms.\n",
    "\n",
    "In summary there are 3 general methods to correct an unfair classifier.\n",
    "\n",
    "1. **Pre-Processing**: make changes in the data before training the model i.e remove correlated features\n",
    "2. **In-Processing**: make changes in the model to correct fairness i.e add additional loss terms to ensure fairness.\n",
    "3. **Post-Processing**: make changes after the model's output i.e fix classification threshold.\n",
    "\n",
    "\n",
    "For an overview of different methods and fairness criteria we refer you to the following interesting survey https://arxiv.org/pdf/2001.09784.pdf.\n",
    "\n",
    "<div>\n",
    "<img style=\"float: left;\" src=\"./figures/corect_unfairness.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pre-Processing\n",
    "\n",
    "Unfair models can be a result of bias in data. Pre-processing methods try to remove bias in data before it is used to train models. The data is transformed, and an algorithm is trained just as it would be on the original dataset.\n",
    "\n",
    "Preprocessing techniques can be seen as transforming the data into a different representation to remove that bias.\n",
    "Some of the most popular approaches are re-balacing techniques, remove sensitive features and their proxies variables, or transform the data into a fair representation using a machine learning algorithms i.e representation learning.\n",
    "\n",
    "pros:\n",
    "1. We can use it for any downstream task.\n",
    "2. Pre-processing technique is not depended on the final classification model.\n",
    "2. Most of the approaches don't need to have access in a sensitive feature after training.\n",
    "\n",
    "cons:\n",
    "1. Has more influence on performance (decreasing accuracy).\n",
    "2. We can not fix all the different fairness criteria as some of those is depending on the classifier. For example to fix equalized odds we need to have access to the outcome of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. In-Processing\n",
    "\n",
    "In-Processing techniques considers fairness in the optimization process of a classifier.\n",
    "In general In-Processing approach introduce constrains or regularization terms on the objective function when we train a classifier.\n",
    "Different method where proposed either as a general framework or solution based on specific classification algorithms.\n",
    "\n",
    "pros:\n",
    "1. Good performance on accuracy and fairness measures. \n",
    "2. Flexible between trade of between performance and fairness.\n",
    "2. We don't need to have access in sensitive features after training.\n",
    "\n",
    "cons:\n",
    "1. Different contain may be difficult to apply.\n",
    "2. Works with our specific classifier.\n",
    "3. Need to modify a classifier. We have to include contains on retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Post-Processing\n",
    "\n",
    "These methods post-process the output score of classifier to remove bias.\n",
    "The most know techniques use different classification threshold for each group and try to satisfy fairness criteria.\n",
    "\n",
    "pros:\n",
    "1. Can be applied after any classifiers.\n",
    "2. Relatively good performance especially fairness measures.\n",
    "2. No need to modify a classifier.\n",
    "\n",
    "cons:\n",
    "1. Most of the time we require test-time access to the protected attribute\n",
    "2. Not very flexible on the trade of between accuracy and fairness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B1. Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B1.1 Unawareness\n",
    "A very intuitive but **wrong solution** is to assume that ignoring the sensitive feature we can build a fair classifier.\n",
    "This approach is wrong because other feature may be correlated with our sensitive features.\n",
    "\n",
    "We will build a **Unawareness** classifier and the compare the results with our previous classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "unaware_model  = RandomForestClassifier(n_estimators=1000,\n",
    "                                        max_depth=10)\n",
    "\n",
    "unaware_model.fit(X = train_data[non_sensitive_features],\n",
    "                  y = train_data[target_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unaware_model.predict(test_data[non_sensitive_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unaware_metrics = compute_metric(y_pred = unaware_model.predict(test_data[non_sensitive_features]),\n",
    "                                 y_true = test_data[target_column],\n",
    "                                 sensitive_feature = test_data[\"sex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unaware_results = pd.DataFrame(unaware_metrics,\n",
    "                               index = [\"unaware_model\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unaware_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results of 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat([results, unaware_results],axis=1)\n",
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Our conclusion is that we are not able to vanish unfairness by excluding sensitive features.  \n",
    "The reason in that some other feature leak information about our sensitive attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B 1.2 Remove correlated features\n",
    "From the above results we expect that our sensitive feature are correlate with non-sensitive feature in our analysis.\n",
    "We will try to find and exclude also those feature in order to improve our fairness properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = {}\n",
    "for f in non_sensitive_features:\n",
    "    corr[f] =np.corrcoef(test_data[sensitive_feature[0]], test_data[f])[0,1]\n",
    "corr = np.abs(pd.DataFrame(data=corr, index=[\"corr\"]).T).sort_values(by=\"corr\",ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "corr.plot(kind=\"bar\")\n",
    "plt.title(\"Correlation of sensitive attribute\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build classifier excluding k most correlated feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "k_most_correlated = corr.iloc[0:k].index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(k_most_correlated)):\n",
    "    print(f\"===========iteration{i+1}===========\")\n",
    "    training_features = list(set(non_sensitive_features).difference(set(k_most_correlated[0:i+1])))\n",
    "    print(\"We exclude from training: \", k_most_correlated[0:i+1])\n",
    "    \n",
    "    # 1. train\n",
    "    post_processing_model  = RandomForestClassifier(n_estimators=1000,\n",
    "                                                    max_depth=10)\n",
    "\n",
    "    post_processing_model.fit(X = train_data[training_features],\n",
    "                              y = train_data[target_column])\n",
    "    \n",
    "\n",
    "    # 2. compute matrix\n",
    "    post_processing_metrics = compute_metric(y_pred= post_processing_model.predict(test_data[training_features]),\n",
    "                                             y_true= test_data[target_column],\n",
    "                                             sensitive_feature = test_data[\"sex\"])\n",
    "    \n",
    "    # 3. append result\n",
    "    post_processing_results = pd.DataFrame(post_processing_metrics,\n",
    "                                           index = [f\"pre_prossesing_{i+1}\"]).T\n",
    "    \n",
    "    all_results = pd.concat([all_results, post_processing_results],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B2. Post-Processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Threshold\n",
    "\n",
    "A classifier is a function (or rule) that assign samples $X$ to label $\\hat{Y}$  :\n",
    "$${\\hat {y}}=f(x)$$\n",
    "\n",
    "In Probabilistic classifiers generalize this notion of classifiers. \n",
    "\n",
    "\n",
    "The **Probabilistic Classifier** estimates the probability of each class whit respect to the input sample $X$.\n",
    "So the classifier estimates the conditional distributions : $\\Pr(Y=y\\vert X)$\n",
    "\n",
    "And we can get the final class estimate:\n",
    "$$\\hat{y}=\\operatorname {\\arg \\max }_{{y}}\\Pr(Y=y\\vert X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of binary classification we can also define a clasification threshold $t$ such that:\n",
    "\n",
    "$$\n",
    " \\hat{y}= \n",
    "\\begin{cases}\n",
    "    1, & \\text{if} \\Pr(Y=1\\vert X) \\geq t\\\\\n",
    "    0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In the valina binary classification we use a classification theshold $t = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities = model.predict_proba(test_data[sensitive_feature+non_sensitive_features])\n",
    "predicted_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_probabilities = predicted_probabilities[:, 1]\n",
    "predicted_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_threshold(probabilities, threshold):\n",
    "    \"\"\"\n",
    "    preidct classification class based on threshold t.\n",
    "    @propabilities: classification propabilities of positive class.\n",
    "    @threshold: classification threshold.\n",
    "    \"\"\"\n",
    "    # clalculate the prediction based on the propabilities and thresholds.\n",
    "    # hint: use pandas masking to effintiantly calculate predictions.\n",
    "    \n",
    "    # put your code here\n",
    "    prediction = (probabilities > threshold) * 1\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_with_threshold(probabilities = predicted_probabilities,\n",
    "                       threshold = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_with_threshold(probabilities = predicted_probabilities,\n",
    "                       threshold = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_data[sensitive_feature+non_sensitive_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caclulate differenet metrics for different thresholds\n",
    "\n",
    "Bellow we will compute the different metrics using different classification threshold.   \n",
    "As you can see from the bellow figure as we change threshold we have different **TP, FP, TN, FN** and thus we will have different metrics.\n",
    "\n",
    "![thr](./figures/threshold.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0,1,50)\n",
    "\n",
    "metric_ts = []\n",
    "for t in thresholds:\n",
    "    # 1. make predictions for different thresholds using the test set.\n",
    "    prediction = predict_with_threshold(probabilities = predicted_probabilities,\n",
    "                                        threshold = t)\n",
    "    metrics = compute_metric(y_pred = prediction,\n",
    "                             y_true = test_data[target_column],\n",
    "                             sensitive_feature = test_data[sensitive_feature[0]])\n",
    "    \n",
    "    metric_ts += [metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_results = pd.DataFrame(metric_ts,index = thresholds)\n",
    "ts_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "ts_results.plot(figsize=(16,10))\n",
    "plt.title(\"Metrics Vs Threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Metrics\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC curve\n",
    "\n",
    "Roc curve plot TPR vs FPR for different thresholds.  \n",
    "Initially Roc curve was proposed to compare classifier i.e classifier with highter area under curve is a better clasifier.\n",
    "\n",
    "<div>\n",
    "<img style=\"float: left;\" src=\"./figures/roc_curve.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve(y_true, predicted_probabilities, thresholds):\n",
    "    \"\"\"\n",
    "    Custom impementation of roc curve for binary clasification.\n",
    "    \"\"\"\n",
    "    number_of_positives = (y_true == 1).sum()\n",
    "    tpr, fpr, ppr = [], [], []\n",
    "    for thr in thresholds:\n",
    "        predictions = predict_with_threshold(probabilities = predicted_probabilities ,\n",
    "                                             threshold =  thr)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_true = y_true,\n",
    "                                       y_pred = predictions)\n",
    "    \n",
    "        TN, FP, FN, TP = conf_matrix.ravel()    \n",
    "        tpr += [TP / (TP + FN)]\n",
    "        fpr += [FP / (FP + TN)]\n",
    "        ppr += [(TP+ FP) / (TN + FP + FN+ TP)]\n",
    "    \n",
    "    return fpr, tpr, ppr, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities = model.predict_proba(test_data[sensitive_feature+non_sensitive_features])[:, 1]\n",
    "predicted_probabilities = pd.Series(predicted_probabilities,\n",
    "                                    index=test_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0.0, 1.0, 100)\n",
    "fpr, tpr, ppr, thresholds = roc_curve(y_true= test_data[target_column], \n",
    "                                      predicted_probabilities= predicted_probabilities,\n",
    "                                      thresholds= thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr,tpr)\n",
    "plt.xlabel(\"false positive rate\")\n",
    "plt.ylabel(\"true positive rate\")\n",
    "plt.title(\"Roc curve\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate roc curve for different groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_group_0 = test_data[\"sex\"] == 0\n",
    "mask_group_1 = np.logical_not(mask_group_0)\n",
    "\n",
    "# group s=0\n",
    "y_true_group_0 = test_data.loc[mask_group_0, target_column].values\n",
    "scores_group_0 = predicted_probabilities.loc[mask_group_0].values\n",
    "fpr_group_0, tpr_group_0, ppr_group_0, _ = roc_curve(y_true = y_true_group_0,\n",
    "                                                     predicted_probabilities = scores_group_0,\n",
    "                                                     thresholds = thresholds)\n",
    "\n",
    "# group s=1\n",
    "y_true_group_1 = test_data.loc[mask_group_1,target_column].values\n",
    "scores_group_1 = predicted_probabilities.loc[mask_group_1].values\n",
    "fpr_group_1, tpr_group_1, ppr_group_1, _ = roc_curve(y_true = y_true_group_1,\n",
    "                                                     predicted_probabilities = scores_group_1,\n",
    "                                                     thresholds = thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Correct demographic parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(thresholds, ppr_group_0, linestyle='-', marker='.', label = \"group 0\")\n",
    "plt.plot(thresholds, ppr_group_1, linestyle='-', marker=\".\", label = \"group 1\")\n",
    "plt.axhline(y = 0.765, color = 'r', linestyle = '--', linewidth = 0.5, label = \"equal tpr\")\n",
    "plt.axhline(y = 0.53, color = 'r', linestyle = '--', linewidth = 0.5)\n",
    "plt.axhline(y = 0.3, color = 'r', linestyle = '--', linewidth = 0.5)\n",
    "plt.ylabel(\"predictive positive rate\")\n",
    "plt.xlabel(\"thresholds\")\n",
    "plt.title(\"Positive predictive rate for different classification thresholds\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = 0.01\n",
    "\n",
    "accepted_thresholds = []\n",
    "accepted_thresholds_index = []\n",
    "for thr_index_0, ppr_0 in enumerate(ppr_group_0):\n",
    "    abs_diff = np.abs(ppr_0 - ppr_group_1)\n",
    "    arg_min = np.argmin(abs_diff)\n",
    "\n",
    "    if abs_diff[arg_min] < sensitivity:\n",
    "        accepted_thresholds_index.append((thr_index_0,arg_min))\n",
    "        accepted_thresholds.append((thresholds[thr_index_0], thresholds[arg_min]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_thr_demographic_parity = []\n",
    "\n",
    "for (thr0, thr1) in accepted_thresholds:\n",
    "    thr_series_0 = mask_group_0 * thr0\n",
    "    thr_series_1 = mask_group_1 * thr1\n",
    "    thr = thr_series_0 + thr_series_1\n",
    "\n",
    "    predictions = predict_with_threshold(probabilities = predicted_probabilities,\n",
    "                                         threshold = thr)\n",
    "    \n",
    "    metrics = compute_metric(y_pred = predictions,\n",
    "                             y_true = test_data[target_column],\n",
    "                             sensitive_feature = test_data[sensitive_feature[0]])\n",
    "    \n",
    "    metrics_thr_demographic_parity += [metrics]\n",
    "metrics_thr_demographic_parity = pd.DataFrame(metrics_thr_demographic_parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_thr_demographic_parity.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1_score = metrics_thr_demographic_parity[\"f1_score\"].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = metrics_thr_demographic_parity.loc[[best_f1_score]] \n",
    "best_model.rename(index={best_f1_score: 'post-prossesing best demographic parity'},inplace=True)\n",
    "\n",
    "all_results = pd.concat([all_results, best_model.T],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.plot(kind=\"bar\",figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Correct Equalized oportunities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr_group_0, tpr_group_0, linestyle='-', label = \"group_0\")\n",
    "plt.plot(fpr_group_1, tpr_group_1, linestyle='-', label = \"group_1\")\n",
    "\n",
    "plt.axhline(y = 0.85, color = 'r', linestyle = '--', linewidth = 0.5, label = \"equal tpr\")\n",
    "plt.axhline(y = 0.75, color = 'r', linestyle = '--', linewidth = 0.5)\n",
    "plt.axhline(y = 0.6, color = 'r', linestyle = '--', linewidth = 0.5)\n",
    "\n",
    "plt.scatter([0.00,0.65], [0.3,0.999], color=\"r\",s = 50, label = \"equal tpr & fpr\")\n",
    "plt.xlabel(\"false positive rate\")\n",
    "plt.ylabel(\"true positive rate\")\n",
    "plt.title(\"Roc curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = 0.01\n",
    "\n",
    "accepted_thresholds = []\n",
    "accepted_thresholds_index = []\n",
    "for thr_index_0, tpr_0 in enumerate(tpr_group_0):\n",
    "    abs_diff = np.abs(tpr_0 - tpr_group_1)\n",
    "    arg_min = np.argmin(abs_diff)\n",
    "\n",
    "    if abs_diff[arg_min] < sensitivity:\n",
    "        accepted_thresholds_index.append((thr_index_0,arg_min))\n",
    "        accepted_thresholds.append((thresholds[thr_index_0], thresholds[arg_min]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_thr_eq_opportunities = []\n",
    "\n",
    "for (thr0, thr1) in accepted_thresholds:\n",
    "    thr_series_0 = mask_group_0 * thr0\n",
    "    thr_series_1 = mask_group_1 * thr1\n",
    "    thr = thr_series_0 + thr_series_1\n",
    "\n",
    "    predictions = predict_with_threshold(probabilities = predicted_probabilities,\n",
    "                                         threshold = thr)\n",
    "    \n",
    "    metrics = compute_metric(y_pred = predictions,\n",
    "                             y_true = test_data[target_column],\n",
    "                             sensitive_feature = test_data[sensitive_feature[0]])\n",
    "    \n",
    "    metrics_thr_eq_opportunities += [metrics]\n",
    "metrics_thr_eq_opportunities = pd.DataFrame(metrics_thr_eq_opportunities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1_score = metrics_thr_eq_opportunities[\"f1_score\"].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = metrics_thr_eq_opportunities.loc[[best_f1_score]] \n",
    "best_model.rename(index={best_f1_score: 'post-processing best equalize opportunities'},inplace=True)\n",
    "\n",
    "all_results = pd.concat([all_results, best_model.T],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.plot(kind=\"bar\",figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Correct Equalized odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr_group_0, tpr_group_0, linestyle='-', label = \"group_0\")\n",
    "plt.plot(fpr_group_1, tpr_group_1, linestyle='-', label = \"group_1\")\n",
    "\n",
    "plt.axhline(y = 0.85, color = 'r', linestyle = '--', linewidth = 0.5, label = \"equal tpr\")\n",
    "plt.axhline(y = 0.75, color = 'r', linestyle = '--', linewidth = 0.5)\n",
    "plt.axhline(y = 0.6, color = 'r', linestyle = '--', linewidth = 0.5)\n",
    "\n",
    "plt.scatter([0.00,0.65], [0.3,0.999], color=\"r\",s = 50, label = \"equal tpr & fpr\")\n",
    "plt.xlabel(\"false positive rate\")\n",
    "plt.ylabel(\"true positive rate\")\n",
    "plt.title(\"Roc curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity = 0.01\n",
    "\n",
    "accepted_thresholds = []\n",
    "accepted_thresholds_index = []\n",
    "\n",
    "for thr_index_0, (tpr_0, fpr_0) in enumerate(zip(tpr_group_0, fpr_group_0)):    \n",
    "    abs_diff_tpr = np.abs(tpr_0 - tpr_group_1)\n",
    "    abs_diff_fpr = np.abs(fpr_0 - fpr_group_1)\n",
    "    abs_diff = abs_diff_tpr + abs_diff_fpr\n",
    "    arg_min = np.argmin(abs_diff)\n",
    "\n",
    "    if abs_diff[arg_min] < sensitivity:\n",
    "        accepted_thresholds_index.append((thr_index_0,arg_min))\n",
    "        accepted_thresholds.append((thresholds[thr_index_0], thresholds[arg_min]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_ts = []\n",
    "for (thr0, thr1) in accepted_thresholds:\n",
    "    thr_series_0 = mask_group_0 * thr0\n",
    "    thr_series_1 = mask_group_1 * thr1\n",
    "\n",
    "    thr = thr_series_0 + thr_series_1\n",
    "\n",
    "    predictions = predict_with_threshold(probabilities = predicted_probabilities,\n",
    "                                         threshold=thr)\n",
    "    \n",
    "    metrics = compute_metric(y_pred = predictions,\n",
    "                             y_true = test_data[target_column],\n",
    "                             sensitive_feature = test_data[sensitive_feature[0]])\n",
    "    \n",
    "    metric_ts += [metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_for_different_thr = pd.DataFrame(metric_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1_score = results_for_different_thr[\"f1_score\"].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = results_for_different_thr.loc[[best_f1_score]] \n",
    "best_model.rename(index={best_f1_score: 'post-processing best equalized odds'},inplace=True)\n",
    "\n",
    "all_results = pd.concat([all_results, best_model.T],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.plot(kind=\"bar\",figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thing you need to remember\n",
    "1. Ml model are biased due to biases introduced by different sources. The most common are data dependent biases.\n",
    "2. We have to select an appropriate criteria that fits our problem.\n",
    "3. We can calculate the different criteria using confusion matrix of different groups\n",
    "3. Removing sensitive features does not guarantee fairness as there are proxy variables. Most of the time does not influence our algorithm\n",
    "4. We can remove biases using either pre-processing, in-processing or post-processing methods.\n",
    "5. Different methods have different trade off between fairness and performance.\n",
    "\n",
    "For an overview of different methods and fairness critiriaa we refer you to the following survey https://arxiv.org/pdf/2001.09784.pdf.\n",
    "\n",
    "Fairness python libraries:\n",
    "https://aif360.readthedocs.io/en/stable/modules/algorithms.html\n",
    "https://fairlearn.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-society",
   "language": "python",
   "name": "ml-society"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}